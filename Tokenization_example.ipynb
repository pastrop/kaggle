{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tokenization_example.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNMpNMZQUI4f0JIYlORgFFz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pastrop/kaggle/blob/master/Tokenization_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdrLsFoJa6dK"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_ZpDBftJbO8"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import(TreebankWordTokenizer,\n",
        "                          TweetTokenizer,\n",
        "                          MWETokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INJI5NxvJdi0",
        "outputId": "ca32e83e-42f0-4b93-bf05-05a729508f7e"
      },
      "source": [
        "#Create tokenizers:\n",
        "tree = TreebankWordTokenizer()\n",
        "tweet = TweetTokenizer()\n",
        "mwe = MWETokenizer()\n",
        "\n",
        "# Create a string input\n",
        "sent1 = 'There are more things in heaven and earth, Horatio, than are dreamt of in your philosophy'\n",
        "     \n",
        "# Use tokenize method\n",
        "print(f'Treebank -> {tree.tokenize(sent1)}')\n",
        "print(f'Tweettokenizer -> {tweet.tokenize(sent1)}')\n",
        "print(f'MWEtokenizer -> {mwe.tokenize(sent1)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Treebank -> ['There', 'are', 'more', 'things', 'in', 'heaven', 'and', 'earth', ',', 'Horatio', ',', 'than', 'are', 'dreamt', 'of', 'in', 'your', 'philosophy']\n",
            "Tweettokenizer -> ['There', 'are', 'more', 'things', 'in', 'heaven', 'and', 'earth', ',', 'Horatio', ',', 'than', 'are', 'dreamt', 'of', 'in', 'your', 'philosophy']\n",
            "MWEtokenizer -> ['T', 'h', 'e', 'r', 'e', ' ', 'a', 'r', 'e', ' ', 'm', 'o', 'r', 'e', ' ', 't', 'h', 'i', 'n', 'g', 's', ' ', 'i', 'n', ' ', 'h', 'e', 'a', 'v', 'e', 'n', ' ', 'a', 'n', 'd', ' ', 'e', 'a', 'r', 't', 'h', ',', ' ', 'H', 'o', 'r', 'a', 't', 'i', 'o', ',', ' ', 't', 'h', 'a', 'n', ' ', 'a', 'r', 'e', ' ', 'd', 'r', 'e', 'a', 'm', 't', ' ', 'o', 'f', ' ', 'i', 'n', ' ', 'y', 'o', 'u', 'r', ' ', 'p', 'h', 'i', 'l', 'o', 's', 'o', 'p', 'h', 'y']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV_TwjWtz1CI"
      },
      "source": [
        "**Neural Nets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "779ksClsyWyI"
      },
      "source": [
        "#This is a tokenization example while working with neural nets.  Info only,  this is not directly applicable to the current use case:\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Drwsqk7iy8_Z",
        "outputId": "a44d0d43-f497-42cd-80e0-e2f6ab54db7c"
      },
      "source": [
        "sent2 = \"Mary had a little lumb and, according to GPT3, ate it with the mint jelly\"\n",
        "encoded_input = tokenizer(sent2)\n",
        "print(encoded_input.input_ids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[101, 2090, 1125, 170, 1376, 181, 1818, 1830, 1105, 117, 2452, 1106, 15175, 1942, 1495, 117, 8756, 1122, 1114, 1103, 22532, 179, 23083, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t96Gf--MiTYT"
      },
      "source": [
        "pip install spacy-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7oRFezoYICi"
      },
      "source": [
        "import spacy\n",
        "from termcolor import colored"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZAr5pEmTk-J"
      },
      "source": [
        "#spacy custom tokenizer\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.lang.char_classes import ALPHA, ALPHA_LOWER, ALPHA_UPPER, CONCAT_QUOTES, LIST_ELLIPSES, LIST_ICONS\n",
        "from spacy.util import compile_infix_regex\n",
        "\n",
        "def custom_tokenizer(nlp):\n",
        "    infixes = (\n",
        "        LIST_ELLIPSES\n",
        "        + LIST_ICONS\n",
        "        + [\n",
        "            r\"(?<=[0-9])[+\\-\\*^](?=[0-9-])\",\n",
        "            r\"(?<=[{al}{q}])\\.(?=[{au}{q}])\".format(\n",
        "                al=ALPHA_LOWER, au=ALPHA_UPPER, q=CONCAT_QUOTES\n",
        "            ),\n",
        "            r\"(?<=[{a}]),(?=[{a}])\".format(a=ALPHA),\n",
        "            #r\"(?<=[{a}])(?:{h})(?=[{a}])\".format(a=ALPHA, h=HYPHENS),\n",
        "            r\"(?<=[{a}0-9])[:<>=/](?=[{a}])\".format(a=ALPHA),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    infix_re = compile_infix_regex(infixes)\n",
        "\n",
        "    return Tokenizer(nlp.vocab, prefix_search=nlp.tokenizer.prefix_search,\n",
        "                                suffix_search=nlp.tokenizer.suffix_search,\n",
        "                                infix_finditer=infix_re.finditer,\n",
        "                                token_match=nlp.tokenizer.token_match,\n",
        "                                rules=nlp.Defaults.tokenizer_exceptions)\n",
        "\n",
        "\n",
        "nlp_sm.tokenizer = custom_tokenizer(nlp_sm)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RL3CoVbAm1dD"
      },
      "source": [
        "import spacy.cli"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBioocoNXSB1",
        "outputId": "40addd00-e67a-4a69-e697-0a6b6f00b11f"
      },
      "source": [
        "spacy.cli.download(\"en_core_web_lg\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYml9qR2m8aF",
        "outputId": "337e939b-9e87-4a84-c88d-ecaf0a2581a0"
      },
      "source": [
        "spacy.cli.download('en_core_web_sm')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4Y1ppRi7kln"
      },
      "source": [
        "nlp_sm = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRKngjTjYOBR"
      },
      "source": [
        "nlp_lg = spacy.load('en_core_web_lg')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRKOuBoMh2qK"
      },
      "source": [
        "nlp_trf = spacy.load(\"en_core_web_trf\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lQB_vFXUbru"
      },
      "source": [
        "st = ['Make it so we can hide and unhide the carousel',\n",
        "      'Mary had a little lamb and, according to GPT3, ate it with the mint jelly',\n",
        "      '-The well-tested code',\n",
        "      \"I'M GONNA PUKE\",\n",
        "      'Much sleeker. Very attractive!..I would strongly recommend',\n",
        "      'CoughROOTCough',\n",
        "      \"So...I'm very happy\",\n",
        "      'A starling among starlings',\n",
        "      'It was a love-fest',\n",
        "      \"It's great!\",\n",
        "      'Kindle-Fire is on fire'\n",
        "      ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDSRAvRITqoS"
      },
      "source": [
        "doc_sm = []\n",
        "for item in st:\n",
        "  doc_sm.append(nlp_sm(item))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p822MECWYS4W",
        "outputId": "75e1c076-b801-4179-dacd-13c126b2e9f0"
      },
      "source": [
        "res = nlp_sm(\"Mother-in-law loves riding mary-go-around while watching primHD\"); \n",
        "for token in res:\n",
        "  print(token.text, token.lemma_, token.pos_, token.is_stop, token.idx, token.idx+len(token.shape_))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mother-in-law Mother-in-law PROPN False 0 12\n",
            "loves love VERB False 14 18\n",
            "riding ride VERB False 20 24\n",
            "mary-go-around mary-go-around PROPN False 27 39\n",
            "while while SCONJ True 42 46\n",
            "watching watch VERB False 48 52\n",
            "primHD primhd NOUN False 57 63\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_w0a84FeWdD"
      },
      "source": [
        "doc_lg = []\n",
        "for item in st:\n",
        "  doc_lg.append(nlp_lg(item))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-Hwp3FOnjki"
      },
      "source": [
        "doc_trf = []\n",
        "for item in st:\n",
        "  doc_trf.append(nlp_trf(item))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Flfenmq_UL2S"
      },
      "source": [
        "def res_prt(doc,st):\n",
        "  for item, text in zip(doc,st):\n",
        "    print(colored(text,'red'))\n",
        "    for token in item:\n",
        "      print(token.text,token.pos_, token.tag_)\n",
        "    print(' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJpCNwUheTYy",
        "outputId": "38588d36-515f-4f5d-f25c-e1db830904b7"
      },
      "source": [
        "#print(colored('EN_CORE_WEB_LG','blue'))\n",
        "#res_prt(doc_lg,st)\n",
        "print(colored('EN_CORE_WEB_SM','blue'))\n",
        "res_prt(doc_sm,st)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34mEN_CORE_WEB_SM\u001b[0m\n",
            "\u001b[31mMake it so we can hide and unhide the carousel\u001b[0m\n",
            "Make VERB VB\n",
            "it PRON PRP\n",
            "so SCONJ IN\n",
            "we PRON PRP\n",
            "can VERB MD\n",
            "hide VERB VB\n",
            "and CCONJ CC\n",
            "unhide VERB VB\n",
            "the DET DT\n",
            "carousel NOUN NN\n",
            " \n",
            "\u001b[31mMary had a little lamb and, according to GPT3, ate it with the mint jelly\u001b[0m\n",
            "Mary PROPN NNP\n",
            "had AUX VBD\n",
            "a DET DT\n",
            "little ADJ JJ\n",
            "lamb NOUN NN\n",
            "and CCONJ CC\n",
            ", PUNCT ,\n",
            "according VERB VBG\n",
            "to ADP IN\n",
            "GPT3 PROPN NNP\n",
            ", PUNCT ,\n",
            "ate VERB VBD\n",
            "it PRON PRP\n",
            "with ADP IN\n",
            "the DET DT\n",
            "mint NOUN NN\n",
            "jelly ADV RB\n",
            " \n",
            "\u001b[31m-The well-tested code\u001b[0m\n",
            "-The NUM CD\n",
            "well ADV RB\n",
            "- PUNCT HYPH\n",
            "tested VERB VBN\n",
            "code NOUN NN\n",
            " \n",
            "\u001b[31mI'M GONNA PUKE\u001b[0m\n",
            "I'M PROPN NNP\n",
            "GONNA PROPN NNP\n",
            "PUKE PROPN NNP\n",
            " \n",
            "\u001b[31mMuch sleeker. Very attractive!..I would strongly recommend\u001b[0m\n",
            "Much ADJ JJ\n",
            "sleeker NOUN NN\n",
            ". PUNCT .\n",
            "Very ADV RB\n",
            "attractive! PROPN NNP\n",
            ".. PUNCT .\n",
            "I PRON PRP\n",
            "would VERB MD\n",
            "strongly ADV RB\n",
            "recommend VERB VB\n",
            " \n",
            "\u001b[31mCoughROOTCough\u001b[0m\n",
            "CoughROOTCough PROPN NNP\n",
            " \n",
            "\u001b[31mSo...I'm very happy\u001b[0m\n",
            "So ADV RB\n",
            "... PUNCT NFP\n",
            "I'm PRON PRP\n",
            "very ADV RB\n",
            "happy ADJ JJ\n",
            " \n",
            "\u001b[31mA starling among starlings\u001b[0m\n",
            "A DET DT\n",
            "starling NOUN NN\n",
            "among ADP IN\n",
            "starlings NOUN NNS\n",
            " \n",
            "\u001b[31mIt was a love-fest\u001b[0m\n",
            "It PRON PRP\n",
            "was AUX VBD\n",
            "a DET DT\n",
            "love NOUN NN\n",
            "- PUNCT HYPH\n",
            "fest NOUN NN\n",
            " \n",
            "\u001b[31mIt's great!\u001b[0m\n",
            "It PRON PRP\n",
            "'s AUX VBZ\n",
            "great ADJ JJ\n",
            "! PUNCT .\n",
            " \n",
            "\u001b[31mKindle-Fire is on fire\u001b[0m\n",
            "Kindle PROPN NNP\n",
            "- PUNCT HYPH\n",
            "Fire PROPN NNP\n",
            "is AUX VBZ\n",
            "on ADP IN\n",
            "fire NOUN NN\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_vMkySeoHCp"
      },
      "source": [
        "res_prt(doc_trf,st)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvAz40eRZGaL"
      },
      "source": [
        "for token in doc:\n",
        "    print(token.text,token.pos_, token.tag_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iflIiOSj9zET"
      },
      "source": [
        "doc = nlp('I have limited bookshelf space.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2xU3ZxofCaR"
      },
      "source": [
        "-The well-tested code<br> '\\n-The well-tested code'<br>stopwords when spelled out: 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 15, 20, 40, 50, 60, 100<br>\n",
        "not stopwords when spelled out: 7, 13, 14, 16, 17, 18, 19, 30, 70, 80, 90, 1000, 100000<br>Splits \"3G\", though not \"401k\"</br>Splits hyphenated words (including, e.g. \"thirty-six\", \"x-ray\", \"wi-fi\")<br>Doesn't catch multiword tokens like \"in front of\" or \"according to\"<br>I'M GONNA PUKE<br>Much sleeker. Very attractive!..I would strongly recommend<br>sturdy(something<br>Rosette calls \"CoughROOTCough\" a proper noun, which, sure.  Spacy calls it a number, which, what?<br>\"So...I'm very happy.\"<br>\"A starling among starlings.\"<br>\"It was a love-fest\"<br>'Its great!'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSKGAqtE_4WQ"
      },
      "source": [
        "# Text Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yj_xwbaWFrjW"
      },
      "source": [
        "import json\n",
        "import re\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jzjip85QnGn"
      },
      "source": [
        "from scipy.spatial import distance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AUXwZMZHX0o"
      },
      "source": [
        "# file upload while using Google Colab\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRdWI8fqF1LE"
      },
      "source": [
        "data = []\n",
        "with open('kindle.json', 'r') as f:\n",
        "    for line in f:\n",
        "        data.append(json.loads(line))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZUiBvJ4GCQ1"
      },
      "source": [
        "data[50]['text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jc1IWUP4LpLc"
      },
      "source": [
        "txts = [] \n",
        "for item in data:\n",
        "  txts.append(re.sub('\\n+', ' ', item['text']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvkqDyTmj4es"
      },
      "source": [
        "**Google Unviersal Encoder Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9r3x0isJVYx"
      },
      "source": [
        "!pip install -q tensorflow-hub\n",
        "import tensorflow_hub as hub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyCaA0ykJWPK"
      },
      "source": [
        "#using universal sentence encoder to get sentence encodings\n",
        "#Load the Universal Sentence Encoder's TF Hub module\n",
        "#param [\"https://tfhub.dev/google/universal-sentence-encoder/4\", \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"]\n",
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" \n",
        "print (\"module {} loaded\".format(module_url))\n",
        "model = hub.load(module_url)\n",
        "def embed(input):\n",
        "  return model(input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38j5vtKsjn3y"
      },
      "source": [
        "**lumi embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwPNIBDH8OF9"
      },
      "source": [
        "test_data = np.load('vector.npz')\n",
        "#test_data['vect'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaWJUT8CGukA"
      },
      "source": [
        "test_data['vect'].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CkEVgEqH9yw"
      },
      "source": [
        "concept_data = np.load('concept_vector.npz')\n",
        "concept_data['vect'].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znQAWS4FIwoe"
      },
      "source": [
        "lumi = np.concatenate((test_data['vect'],concept_data['vect']), axis=0); lumi.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8ln3PbPHlaF"
      },
      "source": [
        "lumi = []\n",
        "for row in test_data['vect']:\n",
        "  lumi.append(row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoz5TR24IAKY"
      },
      "source": [
        "lumi[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f90zKenojbot"
      },
      "source": [
        "**embedding using universal encoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MgzpTPxse-L"
      },
      "source": [
        "concepts = ['Kindle','Amazon','apps','tablet','Kindle Fire','purchase','Kindle Fire HD','iPad','device','download']\n",
        "concept_vectors = np.array(model(concepts))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuFOLc08JjRL"
      },
      "source": [
        "embedding_tuples = []\n",
        "for item in txts:\n",
        "  tmp = embed([item]).numpy(),item\n",
        "  embedding_tuples.append(tmp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qb8T2fvA7B8-"
      },
      "source": [
        "embedding_tuples[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1WsE_CiaGPA"
      },
      "source": [
        "test = []\n",
        "for item in txts:\n",
        "  test.append(embed([item]).numpy().flatten())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uo-A5AZlIJaV"
      },
      "source": [
        "test = np.array(test); test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkMvWhssE7SZ"
      },
      "source": [
        "test_test = np.concatenate((test,concept_vectors), axis=0); test_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfVxEkmzJkO9"
      },
      "source": [
        "# data visualization:\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCHQ12p_Jpt9"
      },
      "source": [
        "# visualization code\n",
        "def tsne_plot(emb):\n",
        "    \"Creates and TSNE model and plots it\"\n",
        "    labels = []\n",
        "    tokens = []\n",
        "\n",
        "    #for i in range(len(emb)):\n",
        "        #tokens.append(emb[i][0])\n",
        "        #labels.append(emb[i][1])\n",
        "    tkns = np.array(emb)\n",
        "    tkns = tkns.reshape(tkns.shape[0], -1)\n",
        "    tsne_model = TSNE(perplexity=30, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
        "    new_values = tsne_model.fit_transform(tkns)\n",
        "\n",
        "    x = []\n",
        "    y = []\n",
        "    for value in new_values:\n",
        "        x.append(value[0])\n",
        "        y.append(value[1])\n",
        "        \n",
        "    plt.figure(figsize=(16, 16)) \n",
        "    #Outputting all the embeddings\n",
        "    for i in range(len(x)):\n",
        "        plt.scatter(x[i],y[i])\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgg7ERxiJvvV"
      },
      "source": [
        "#tsne_plot(embedding_tuples)\n",
        "tsne_plot(lumi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGBIWD6FgoV6"
      },
      "source": [
        "# some funcier visualization code\n",
        "def tsne_plot_fancy(emb):\n",
        "    \"Creates and TSNE model and plots it\"\n",
        "    labels = []\n",
        "    tokens = []\n",
        "\n",
        "    #for i in range(len(emb)):\n",
        "        #tokens.append(emb[i][0])\n",
        "        #labels.append(emb[i][1])\n",
        "    #tokens = np.array(emb)\n",
        "    #print(tokens[:1])\n",
        "    #tokens = tokens.reshape(tokens.shape[0], -1)\n",
        "    #points = points.reshape(tkns.shape[0], -1)\n",
        "    tsne_model = TSNE(perplexity=30, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
        "    new_values = tsne_model.fit_transform(emb)\n",
        "    #concepts_values = tsne_model.fit_transform(points)\n",
        "\n",
        "    x = []\n",
        "    y = []\n",
        "    for value in new_values:\n",
        "        x.append(value[0])\n",
        "        y.append(value[1])\n",
        "\n",
        "    print(new_values[-10:])\n",
        "    x_p = []\n",
        "    y_p = []\n",
        "    for value in new_values[-10:]:\n",
        "        x_p.append(value[0])\n",
        "        y_p.append(value[1])    \n",
        "\n",
        "        \n",
        "    plt.figure(figsize=(16, 16)) \n",
        "    #Outputting all the embeddings and overlying concepts\n",
        "    lb = ['Kindle','Amazon','apps','tablet','Kindle Fire','purchase','Kindle Fire HD','iPad','device','download']\n",
        "    for i in range(len(x)):\n",
        "        plt.scatter(x[i],y[i])\n",
        "    for i in range(len(x_p)):\n",
        "        plt.scatter(x_p[i],y_p[i],s=500,c='darkblue')\n",
        "        '''plt.annotate(lb[i],\n",
        "              xy=(x_p[i], y_p[i]),\n",
        "              xytext=(15, 15),\n",
        "              textcoords='offset points',\n",
        "              fontsize = 12,\n",
        "              ha='right',\n",
        "              va='bottom')  '''\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrg2Y5Jc1NG3"
      },
      "source": [
        "#tsne_plot with concepts overlay(embedding_tuples)\n",
        "tsne_plot_fancy(test_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8p4bQzmHg4A"
      },
      "source": [
        "tsne_plot_fancy(lumi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSUh2WVkY9Ec"
      },
      "source": [
        " [ -4.180652  -59.0469   ] -- 'Kindle' (1)<br>\n",
        " [ 40.489338  -18.087015 ] -- 'Amazon'(2)<br>\n",
        " [ 45.76149    -6.4174824] -- 'apps'(3)<br>\n",
        " [-28.779793  -39.422756 ] -- 'tablet' (4)<br>\n",
        " [ -4.189738  -59.018326 ] -- 'Kindle Fire (5)'<br>\n",
        " [ 53.536438  -26.654696 ] -- 'purchase'(6)<br>\n",
        " [ -4.195783  -58.99838  ] -- 'Kindle Fire HD' (7)<br>\n",
        " [ -8.17021   -43.90645  ] -- 'iPad' (8)<br>\n",
        " [-28.982622  -39.634308 ] -- 'device' (9)<br>\n",
        " [ 53.530563  -26.637133 ] -- 'download'(10)<br>\n",
        " ['Kindle','Amazon','apps','tablet','Kindle Fire','purchase','Kindle Fire HD','iPad','device','download']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTSdZgHX0Nvx"
      },
      "source": [
        "**Documents to Concepts Measuring**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYc76Yfa0be0"
      },
      "source": [
        "#512 - dimensional embeddings\n",
        "top_20 = []\n",
        "target = concept_vectors[4]\n",
        "for ind, item in enumerate(embedding_tuples[:10]):\n",
        "    tmp = distance.cosine(item[0],target),item[1],ind\n",
        "    top_20.append(tmp)\n",
        "\n",
        "top_20.sort()\n",
        "\n",
        "for ind, item in enumerate(embedding_tuples[10:100]):\n",
        "  tmp = distance.cosine(item[0],target),item[1],ind\n",
        "  if tmp[0]<top_20[-1][0]:\n",
        "    top_20.pop()\n",
        "    top_20.append(tmp)\n",
        "    top_20.sort"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mho4cSSs_b_b"
      },
      "source": [
        "[item[2] for item in top_20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Twjfx2E9_3yw"
      },
      "source": [
        "[2, 6, 1, 4, 8, 7, 0, 5, 9, 51]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8Fx11KC8LtR"
      },
      "source": [
        "import pprint\n",
        "pp = pp = pprint.PrettyPrinter(indent=1, width=100)\n",
        "for text in top_20:\n",
        "  pp.pprint(text[1])\n",
        "  print(' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etmtzaseIKTp"
      },
      "source": [
        "*USEFUL CODE SNIPPETS*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPZmsge_yXoo"
      },
      "source": [
        "concepts = ['Kindle','Amazon','apps','tablet','Kindle Fire','purchase','Kindle Fire HD','iPad','device','download']\n",
        "concept_vectors = np.array(model(concepts))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nj8OPRgyYyt"
      },
      "source": [
        "distance.cosine(concept_vectors[0],concept_vectors[4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16HDSWo62ZlU"
      },
      "source": [
        "concept_data = np.load('concept_vector.npz')\n",
        "concept_data['vect'].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5vt43au2as8"
      },
      "source": [
        "distance.cosine(concept_data['vect'][4],concept_data['vect'][9])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzepDgI_AorF"
      },
      "source": [
        "#concept_vectors.shape\n",
        "a=np.zeros((90,512))\n",
        "concept_vectors_padded = np.concatenate((concept_vectors,a), axis=0); concept_vectors_padded.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OSrG5rbQOre"
      },
      "source": [
        "str1 = 'I\\'m no good'\n",
        "str2 =  \"I'm no good\"\n",
        "str3 = 'I am no good' \n",
        "inp = [str1,str2,str3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlQUQkxDRUYE"
      },
      "source": [
        "out = np.array(model(inp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQF5oeMEROLG"
      },
      "source": [
        "distance.cosine(out[0],out[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCGjkajvIuaP"
      },
      "source": [
        "!unzip file_location"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgdVOuXgGHJr"
      },
      "source": [
        "for item in data:\n",
        "  re.sub('\\n+', ' ' item['text'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}