{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tokenization_example.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMr/pZqIyL28GkhwxGr06/s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pastrop/kaggle/blob/master/Tokenization_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_ZpDBftJbO8"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import(TreebankWordTokenizer,\n",
        "                          TweetTokenizer,\n",
        "                          MWETokenizer)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INJI5NxvJdi0",
        "outputId": "ca32e83e-42f0-4b93-bf05-05a729508f7e"
      },
      "source": [
        "#Create tokenizers:\n",
        "tree = TreebankWordTokenizer()\n",
        "tweet = TweetTokenizer()\n",
        "mwe = MWETokenizer()\n",
        "\n",
        "# Create a string input\n",
        "sent1 = 'There are more things in heaven and earth, Horatio, than are dreamt of in your philosophy'\n",
        "     \n",
        "# Use tokenize method\n",
        "print(f'Treebank -> {tree.tokenize(sent1)}')\n",
        "print(f'Tweettokenizer -> {tweet.tokenize(sent1)}')\n",
        "print(f'MWEtokenizer -> {mwe.tokenize(sent1)}')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Treebank -> ['There', 'are', 'more', 'things', 'in', 'heaven', 'and', 'earth', ',', 'Horatio', ',', 'than', 'are', 'dreamt', 'of', 'in', 'your', 'philosophy']\n",
            "Tweettokenizer -> ['There', 'are', 'more', 'things', 'in', 'heaven', 'and', 'earth', ',', 'Horatio', ',', 'than', 'are', 'dreamt', 'of', 'in', 'your', 'philosophy']\n",
            "MWEtokenizer -> ['T', 'h', 'e', 'r', 'e', ' ', 'a', 'r', 'e', ' ', 'm', 'o', 'r', 'e', ' ', 't', 'h', 'i', 'n', 'g', 's', ' ', 'i', 'n', ' ', 'h', 'e', 'a', 'v', 'e', 'n', ' ', 'a', 'n', 'd', ' ', 'e', 'a', 'r', 't', 'h', ',', ' ', 'H', 'o', 'r', 'a', 't', 'i', 'o', ',', ' ', 't', 'h', 'a', 'n', ' ', 'a', 'r', 'e', ' ', 'd', 'r', 'e', 'a', 'm', 't', ' ', 'o', 'f', ' ', 'i', 'n', ' ', 'y', 'o', 'u', 'r', ' ', 'p', 'h', 'i', 'l', 'o', 's', 'o', 'p', 'h', 'y']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV_TwjWtz1CI"
      },
      "source": [
        "**Neural Nets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ar898zERyyhF"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "779ksClsyWyI"
      },
      "source": [
        "#This is a tokenization example while working with neural nets.  Info only,  this is not directly applicable to the current use case:\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Drwsqk7iy8_Z",
        "outputId": "a44d0d43-f497-42cd-80e0-e2f6ab54db7c"
      },
      "source": [
        "sent2 = \"Mary had a little lumb and, according to GPT3, ate it with the mint jelly\"\n",
        "encoded_input = tokenizer(sent2)\n",
        "print(encoded_input.input_ids)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[101, 2090, 1125, 170, 1376, 181, 1818, 1830, 1105, 117, 2452, 1106, 15175, 1942, 1495, 117, 8756, 1122, 1114, 1103, 22532, 179, 23083, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}