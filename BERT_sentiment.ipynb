{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_sentiment.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOEzhjhuXo9Cs4UUdpijkrF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pastrop/kaggle/blob/master/BERT_sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3J1OOYd-sWaE",
        "colab_type": "text"
      },
      "source": [
        "# BERT Based Sentiment Analysis <br>\n",
        "*original is at: https://www.kaggle.com/pastrop/toxic-data-comp-data*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptTy0j98X1jV",
        "colab_type": "code",
        "outputId": "64e59c05-9d6f-4532-83ae-dd39791cb6e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "!pip install bert-for-tf2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-for-tf2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/5c/6439134ecd17b33fe0396fb0b7d6ce3c5a120c42a4516ba0e9a2d6e43b25/bert-for-tf2-0.14.4.tar.gz (40kB)\n",
            "\r\u001b[K     |████████                        | 10kB 15.7MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 30kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 40kB 2.0MB/s \n",
            "\u001b[?25hCollecting py-params>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/bf/c1c70d5315a8677310ea10a41cfc41c5970d9b37c31f9c90d4ab98021fd1/py-params-0.9.7.tar.gz\n",
            "Collecting params-flow>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a9/95/ff49f5ebd501f142a6f0aaf42bcfd1c192dc54909d1d9eb84ab031d46056/params-flow-0.8.2.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
            "Building wheels for collected packages: bert-for-tf2, py-params, params-flow\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.4-cp36-none-any.whl size=30114 sha256=49e8af10d285b2d5066e336067e0ad752c95b7da643479447ab7b3cb0f3533af\n",
            "  Stored in directory: /root/.cache/pip/wheels/cf/3f/4d/79d7735015a5f523648df90d871ce8e89a7df8185f7703eeab\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.9.7-cp36-none-any.whl size=7302 sha256=34e727439e8e064a1cc083359e88d7c18b73b9a943d58070bd98483098820d12\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/f5/19/b461849a50aefdf4bab47c4756596e82ee2118b8278e5a1980\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.8.2-cp36-none-any.whl size=19473 sha256=d650598f206d2e9079ea0bebd1058d5b381305f12da69f94422a474cae679fa7\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/c8/7f/81c86b9ff2b86e2c477e3914175be03e679e596067dc630c06\n",
            "Successfully built bert-for-tf2 py-params params-flow\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.14.4 params-flow-0.8.2 py-params-0.9.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxMT0hWyWemL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import bert"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMkDApzaieyL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import os, time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqmvrVn7i9S9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"TensorFlow Version:\",tf.__version__)\n",
        "print(\"Hub version: \",hub.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTZCPYpXiqGc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('path to file')\n",
        "df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8vysnDwjGdg",
        "colab_type": "text"
      },
      "source": [
        "# Initial Dataset Processing - Adding word_ids, mask_ids, segment_ids"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIGBoSovjMPM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEQUENCE_LENGTH = 256\n",
        "\n",
        "DATA_PATH =  \"../input/jigsaw-multilingual-toxic-comment-classification\" # data location\n",
        "\n",
        "#BERT_PATH = \"../working/bert_model\"\n",
        "#BERT_PATH_SAVEDMODEL = \"../working/bert_model\"\n",
        "#OUTPUT_PATH = \"../working\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcQW5TkgjR0x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wiki_toxic_comment_data = \"jigsaw-toxic-comment-train.csv\" #training file to be processed\n",
        "wiki_toxic_comment_train = pd.read_csv(os.path.join(DATA_PATH, wiki_toxic_comment_data))\n",
        "wiki_toxic_comment_train.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAJs_511WyEg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#building the tokenizer\n",
        "#bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_cased_L-24_H-1024_A-16/2\",trainable=True) - English only\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2\",trainable=True) # Multi-language\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = bert.bert_tokenization.FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zGhWfxXjtFZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#examples of using the tokenizer (not needed for production code, useful to checking that tokenizer is working correctly)\n",
        "example_sentence = wiki_toxic_comment_train.iloc[37].comment_text[:150]\n",
        "print(example_sentence)\n",
        "\n",
        "example_tokens = tokenizer.tokenize(example_sentence)\n",
        "print(example_tokens[:17])\n",
        "\n",
        "example_input_ids = tokenizer.convert_tokens_to_ids(example_tokens)\n",
        "print(example_input_ids[:17])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kBWZYZjvnw5",
        "colab_type": "text"
      },
      "source": [
        "This is a where the majority of the data prep time is spent. Every record in the input has to be processed. I believe that the process is highly parallezible. The steps are: (1) read file into memory (2) process every record (3) write back to storage. As an example, processing 230,000 records file takes ~600 seconds (Standard Kaggle compute resource). In theory step (3) may be avoided if you have enough memory to hold results in memory for future use yet input files my be arbitrarily large and current impementation of the Tensor Flow dataset class mandates input from the CSV file (not sure why...). This step is required for pretty much all modern NLP models and needs to be performed both in training and production"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpHfTot4rGUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_sentence(sentence, max_seq_length=SEQUENCE_LENGTH, tokenizer=tokenizer):\n",
        "    \"\"\"Helper function to prepare data for BERT. Converts sentence input examples\n",
        "    into the form ['input_word_ids', 'input_mask', 'segment_ids'].\"\"\"\n",
        "    # Tokenize, and truncate to max_seq_length if necessary.\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    if len(tokens) > max_seq_length - 2:\n",
        "        tokens = tokens[:(max_seq_length - 2)]\n",
        "\n",
        "    # Convert the tokens in the sentence to word IDs.\n",
        "    input_ids = tokenizer.convert_tokens_to_ids([\"[CLS]\"] + tokens + [\"[SEP]\"])\n",
        "\n",
        "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "    # tokens are attended to.\n",
        "    input_mask = [1] * len(input_ids)\n",
        "\n",
        "    # Zero-pad up to the sequence length.\n",
        "    pad_length = max_seq_length - len(input_ids)\n",
        "    input_ids.extend([0] * pad_length)\n",
        "    input_mask.extend([0] * pad_length)\n",
        "\n",
        "    # We only have one input segment.\n",
        "    segment_ids = [0] * max_seq_length\n",
        "\n",
        "    return (input_ids, input_mask, segment_ids)\n",
        "\n",
        "def preprocess_and_save_dataset(unprocessed_filename, text_label='comment_text',\n",
        "                                seq_length=SEQUENCE_LENGTH, verbose=True):\n",
        "    \"\"\"Preprocess a CSV to the expected TF Dataset form for multilingual BERT,\n",
        "    and save the result.\"\"\"\n",
        "    dataframe = pandas.read_csv(os.path.join(DATA_PATH, unprocessed_filename),\n",
        "                                index_col='id')\n",
        "    processed_filename = (unprocessed_filename.rstrip('.csv') +\n",
        "                          \"-processed-seqlen{}.csv\".format(SEQUENCE_LENGTH))\n",
        "\n",
        "    pos = 0\n",
        "    start = time.time()\n",
        "\n",
        "    while pos < len(dataframe):\n",
        "        processed_df = dataframe[pos:pos + 10000].copy()\n",
        "\n",
        "        processed_df['input_word_ids'], processed_df['input_mask'], processed_df['segment_ids'] = (\n",
        "            zip(*processed_df[text_label].apply(process_sentence)))\n",
        "\n",
        "        if pos == 0:\n",
        "            processed_df.to_csv(processed_filename, index_label='id', mode='w')\n",
        "        else:\n",
        "            processed_df.to_csv(processed_filename, index_label='id', mode='a',\n",
        "                                header=False)\n",
        "\n",
        "        if verbose:\n",
        "            print('Processed {} examples in {}'.format(\n",
        "                pos + 10000, time.time() - start))\n",
        "        pos += 10000\n",
        "    return\n",
        "  \n",
        "# Process the training dataset.\n",
        "preprocess_and_save_dataset(wiki_toxic_comment_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3Cok2uNkE2q",
        "colab_type": "text"
      },
      "source": [
        "# Dataset transformation into tf.dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnyFSunXvOtC",
        "colab_type": "text"
      },
      "source": [
        "The below data transformations run fast"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J64K-p7okTBW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('path to file',nrows = XXXXX) # Limit nrows for the test run, read the entire  file otherwise\n",
        "df.head(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8FpzeSauywR",
        "colab_type": "text"
      },
      "source": [
        "*tf.dataset creation*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxkV1VFTki4M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = df.filter(['toxic','input_word_ids','input_mask','all_segment_id'])\n",
        "#test = test.rename(columns={\"all_segment_id\": \"segment_ids\"})\n",
        "test.head(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xunERykak0TI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# writing dataframe to CSV\n",
        "test.to_csv('test_processed_256.csv', index = False, mode='w')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2dZTfsLlKHM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#building TF dataset:\n",
        "def get_dataset(file_path = 'path to file'):\n",
        "    dataset = tf.data.experimental.make_csv_dataset(\n",
        "      file_path,\n",
        "      batch_size=12, # Artificially small, the dataset is batched up later.\n",
        "      label_name='toxic', #label for the class column\n",
        "      na_value=\"?\",\n",
        "      num_epochs=1,\n",
        "      shuffle=False)\n",
        "    return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmybP7e8llZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = get_dataset('path to file')\n",
        "train_data = train_data.unbatch()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JOSuKX6l4TO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_string_list_into_ints(strlist):\n",
        "    s = tf.strings.strip(strlist)\n",
        "    s = tf.strings.substr(s, 1, tf.strings.length(s) - 2)  # Remove parentheses around list\n",
        "    #s = tf.strings.split(s, ',', maxsplit=128)\n",
        "    s = tf.strings.split(s, ',', maxsplit=256)\n",
        "    s = tf.strings.to_number(s, tf.int32)\n",
        "    #s = tf.reshape(s, [128])  # Force shape here needed for XLA compilation (TPU)\n",
        "    s = tf.reshape(s, [256])  # Force shape here needed for XLA compilation (TPU)\n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPCw9RdqmC-c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prototype function to process the dataset for the Bert layer\n",
        "def elem_mod(data,label):\n",
        "    for k,v in data.items():\n",
        "        data[k] = parse_string_list_into_ints(v)\n",
        "    return data,label    \n",
        "    \n",
        "result = train_data.map(lambda x,y:elem_mod(x,y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJFt4fOlmbgU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Final step in getting the dataset ready:\n",
        "def make_dataset_pipeline(dataset, repeat_and_shuffle=True):\n",
        "    \"\"\"Set up the pipeline for the given dataset.   \n",
        "    Caches, repeats, shuffles, and sets the pipeline up to prefetch batches.\"\"\"\n",
        "    cached_dataset = dataset.cache()\n",
        "    if repeat_and_shuffle:\n",
        "        cached_dataset = cached_dataset.shuffle(2048)\n",
        "    #cached_dataset = cached_dataset.batch(32 * strategy.num_replicas_in_sync)\n",
        "    cached_dataset = cached_dataset.batch(32)\n",
        "    cached_dataset = cached_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    return cached_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTNgpzMCnAc_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cached_train_data = make_dataset_pipeline(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5fnf22MnMka",
        "colab_type": "text"
      },
      "source": [
        "# Keras Model with BERT Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iclfhy0wnGw0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Building the model (reformat as a function...)\n",
        "max_seq_length = 256  # Your choice here.\n",
        "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
        "                                       name=\"input_word_ids\")\n",
        "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
        "                                   name=\"input_mask\")\n",
        "segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
        "                                    name=\"segment_ids\")\n",
        "# BERT layer from pretrained model\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2\",trainable=True)\n",
        "# Dense Layers\n",
        "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
        "output = tf.keras.layers.Dense(32, activation='relu')(pooled_output)\n",
        "output = tf.keras.layers.Dense(1, activation='sigmoid', name='labels')(output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rf13jg1knUhT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model\n",
        "model = tf.keras.Model(inputs={'input_word_ids': input_word_ids,\n",
        "                                  'input_mask': input_mask,\n",
        "                                  'all_segment_id': segment_ids},\n",
        "                          outputs=output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcQqVQCpnhrS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n",
        "    metrics=[tf.keras.metrics.AUC()])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ_vIwZlnomm",
        "colab_type": "text"
      },
      "source": [
        "*Model Build&Compile if TPU is used*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48URrDF6nwEz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TPU based model DON'T RUN WITHOUT TPU\n",
        "# instantiating the model in the strategy scope creates the model on the TPU\n",
        "with tpu_strategy.scope():\n",
        "    model = tf.keras.Sequential( … ) # define your model normally\n",
        "    model.compile( … )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpnMwxc9oGsE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train on English Wikipedia comment data.\n",
        "history = model.fit(\n",
        "    # Set steps such that the number of examples per epoch is fixed.\n",
        "    # This makes training on different accelerators more comparable.\n",
        "    cached_train_data,steps_per_epoch=4000//128,\n",
        "    epochs=7, verbose=1)\n",
        "#print()\n",
        "#steps_per_epoch=4000//256\n",
        "#validation_data=nonenglish_val_datasets['Combined'],\n",
        "# validation_steps=100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFblSlrZoH9S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = model.evaluate(cached_vaidate_data,\n",
        "                                     steps=100, verbose=0)\n",
        "print('\\nLoss, AUC before training:', results) "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}