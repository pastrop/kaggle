{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_sentiment.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOPOZtC8pgnTAIESpVtooms",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pastrop/kaggle/blob/master/BERT_sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3J1OOYd-sWaE",
        "colab_type": "text"
      },
      "source": [
        "# BERT Based Sentiment Analysis <br>\n",
        "*original is at: https://www.kaggle.com/pastrop/toxic-data-comp-data*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptTy0j98X1jV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "64e59c05-9d6f-4532-83ae-dd39791cb6e2"
      },
      "source": [
        "!pip install bert-for-tf2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-for-tf2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/5c/6439134ecd17b33fe0396fb0b7d6ce3c5a120c42a4516ba0e9a2d6e43b25/bert-for-tf2-0.14.4.tar.gz (40kB)\n",
            "\r\u001b[K     |████████                        | 10kB 15.7MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 30kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 40kB 2.0MB/s \n",
            "\u001b[?25hCollecting py-params>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/bf/c1c70d5315a8677310ea10a41cfc41c5970d9b37c31f9c90d4ab98021fd1/py-params-0.9.7.tar.gz\n",
            "Collecting params-flow>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a9/95/ff49f5ebd501f142a6f0aaf42bcfd1c192dc54909d1d9eb84ab031d46056/params-flow-0.8.2.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
            "Building wheels for collected packages: bert-for-tf2, py-params, params-flow\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.4-cp36-none-any.whl size=30114 sha256=49e8af10d285b2d5066e336067e0ad752c95b7da643479447ab7b3cb0f3533af\n",
            "  Stored in directory: /root/.cache/pip/wheels/cf/3f/4d/79d7735015a5f523648df90d871ce8e89a7df8185f7703eeab\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.9.7-cp36-none-any.whl size=7302 sha256=34e727439e8e064a1cc083359e88d7c18b73b9a943d58070bd98483098820d12\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/f5/19/b461849a50aefdf4bab47c4756596e82ee2118b8278e5a1980\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.8.2-cp36-none-any.whl size=19473 sha256=d650598f206d2e9079ea0bebd1058d5b381305f12da69f94422a474cae679fa7\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/c8/7f/81c86b9ff2b86e2c477e3914175be03e679e596067dc630c06\n",
            "Successfully built bert-for-tf2 py-params params-flow\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.14.4 params-flow-0.8.2 py-params-0.9.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxMT0hWyWemL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import keras-bert\n",
        "#from keras_bert import Tokenizer\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAJs_511WyEg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_cased_L-24_H-1024_A-16/2\",trainable=True)\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIj2iucaYQvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import bert\n",
        "tokenizer = bert.bert_tokenization.FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ue2Jnvrkqs5y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data preproessing (depends on Kaggle Data)\n",
        "SEQUENCE_LENGTH = 128\n",
        "DATA_PATH =  \"../input/jigsaw-multilingual-toxic-comment-classification\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpHfTot4rGUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_sentence(sentence, max_seq_length=SEQUENCE_LENGTH, tokenizer=tokenizer):\n",
        "    \"\"\"Helper function to prepare data for BERT. Converts sentence input examples\n",
        "    into the form ['input_word_ids', 'input_mask', 'segment_ids'].\"\"\"\n",
        "    # Tokenize, and truncate to max_seq_length if necessary.\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    if len(tokens) > max_seq_length - 2:\n",
        "        tokens = tokens[:(max_seq_length - 2)]\n",
        "\n",
        "    # Convert the tokens in the sentence to word IDs.\n",
        "    input_ids = tokenizer.convert_tokens_to_ids([\"[CLS]\"] + tokens + [\"[SEP]\"])\n",
        "\n",
        "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "    # tokens are attended to.\n",
        "    input_mask = [1] * len(input_ids)\n",
        "\n",
        "    # Zero-pad up to the sequence length.\n",
        "    pad_length = max_seq_length - len(input_ids)\n",
        "    input_ids.extend([0] * pad_length)\n",
        "    input_mask.extend([0] * pad_length)\n",
        "\n",
        "    # We only have one input segment.\n",
        "    segment_ids = [0] * max_seq_length\n",
        "\n",
        "    return (input_ids, input_mask, segment_ids)\n",
        "\n",
        "def preprocess_and_save_dataset(unprocessed_filename, text_label='comment_text',\n",
        "                                seq_length=SEQUENCE_LENGTH, verbose=True):\n",
        "    \"\"\"Preprocess a CSV to the expected TF Dataset form for multilingual BERT,\n",
        "    and save the result.\"\"\"\n",
        "    dataframe = pandas.read_csv(os.path.join(DATA_PATH, unprocessed_filename),\n",
        "                                index_col='id')\n",
        "    processed_filename = (unprocessed_filename.rstrip('.csv') +\n",
        "                          \"-processed-seqlen{}.csv\".format(SEQUENCE_LENGTH))\n",
        "\n",
        "    pos = 0\n",
        "    start = time.time()\n",
        "\n",
        "    while pos < len(dataframe):\n",
        "        processed_df = dataframe[pos:pos + 10000].copy()\n",
        "\n",
        "        processed_df['input_word_ids'], processed_df['input_mask'], processed_df['all_segment_id'] = (\n",
        "            zip(*processed_df[text_label].apply(process_sentence)))\n",
        "\n",
        "        if pos == 0:\n",
        "            processed_df.to_csv(processed_filename, index_label='id', mode='w')\n",
        "        else:\n",
        "            processed_df.to_csv(processed_filename, index_label='id', mode='a',\n",
        "                                header=False)\n",
        "\n",
        "        if verbose:\n",
        "            print('Processed {} examples in {}'.format(\n",
        "                pos + 10000, time.time() - start))\n",
        "        pos += 10000\n",
        "    return\n",
        "  \n",
        "# Process the training dataset.\n",
        "preprocess_and_save_dataset(wiki_toxic_comment_data)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}