{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sntence_entailment.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOV3sPsbw8ahcbMU1v0Swjs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pastrop/kaggle/blob/master/Sntence_entailment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8SC_J7jYnto",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGlovJTnY-RO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# file upload while using Google Colab\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dx6OZJOxZEnM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip train.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41yWatMcZxVD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv(\"train.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiafmUd6Z0Zb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LO77AG_iZ0X_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get cell value\n",
        "train.iloc[1]['premise']\n",
        "train.iloc[1]['hypothesis']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHUHwAYHa06_",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo_Ypfngb4r9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!pip install pytorch_lightning\n",
        "!pip install transformers\n",
        "!pip install nlp"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krntiZXScJk6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch as th\n",
        "import pytorch_lightning as pl\n",
        "import nlp\n",
        "import transformers"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlVZI5KQa4mJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "232340a7-dff0-47bf-ed08-2319ee33c0c1"
      },
      "source": [
        "#example of tokenizing 2 sentences into\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "sequence_a = \"HuggingFace is based in NYC\"\n",
        "sequence_b = \"Where is HuggingFace based?\"\n",
        "encoded_dict = tokenizer(sequence_a, sequence_b,                     \n",
        "                         max_length=32, \n",
        "                         truncation = True,\n",
        "                         pad_to_max_length=True); encoded_dict\n",
        "#decoded = tokenizer.decode(encoded_dict[\"input_ids\"])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 20164, 10932, 2271, 7954, 1110, 1359, 1107, 17520, 102, 2777, 1110, 20164, 10932, 2271, 7954, 1359, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mkVX-RR8FYg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nlp import load_dataset\n",
        "datasets = load_dataset('imdb')\n",
        "print(datasets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cLo375_ecAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class IMDBSentimentClassifier(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = transformers.BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "        self.loss = th.nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "    def prepare_data(self):\n",
        "        tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        def _tokenize(x):\n",
        "            return tokenizer(\n",
        "                    x['text'], \n",
        "                    max_length=20, \n",
        "                    truncation = True,\n",
        "                    pad_to_max_length=True)\n",
        "            \n",
        "        def _prepare_ds():\n",
        "            ds_train, ds_test = nlp.load_dataset('imdb', split=['train[:500]', 'test[:20]'])\n",
        "            ds_train = ds_train.map(_tokenize, batched=True)\n",
        "            ds_train.set_format(type='torch',columns = ['input_ids','label','attention_mask'])\n",
        "            ds_test = ds_test.map(_tokenize, batched=True)\n",
        "            ds_test.set_format(type='torch',columns = ['input_ids','label','attention_mask'])\n",
        "            #print('ds_train type - {}'.format(type(ds_train)))\n",
        "            #print('ds_test type - {}'.format(type(ds_test)))\n",
        "\n",
        "            return ds_train, ds_test\n",
        "\n",
        "        self.train_ds, self.test_ds = _prepare_ds()      \n",
        "\n",
        "    def forward(self, input_ids, masks):\n",
        "        #print('forward function')\n",
        "        out = self.model(input_ids, masks)\n",
        "        #print('out type & length -- {}--{}'.format(type(out),len(out)))\n",
        "        #print('out -- {}'.format(out))\n",
        "        logits, = self.model(input_ids, masks)\n",
        "        #print ('logits- {} -- other stuff'.format(logits))\n",
        "        return logits\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        #print('training step function')\n",
        "        logits = self.forward(batch['input_ids'],batch['attention_mask'])\n",
        "        loss = self.loss(logits, batch['label']).mean()\n",
        "\n",
        "        return {'loss': loss, 'log': {'train_loss': loss}}\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        #print('validation step function')\n",
        "        logits = self.forward(batch['input_ids'], batch['attention_mask'])\n",
        "        loss = self.loss(logits, batch['label'])\n",
        "        acc = (logits.argmax(-1) == batch['label']).float()\n",
        "\n",
        "        return {'loss': loss, 'acc': acc}\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        loss = th.cat([o['loss'] for o in outputs], 0).mean()\n",
        "        acc = th.cat([o['acc'] for o in outputs], 0).mean()\n",
        "        out = {'val_loss': loss, 'val_acc': acc}\n",
        "        return {**out, 'log': out}\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        #print('train_dataloader')\n",
        "        return th.utils.data.DataLoader(\n",
        "                self.train_ds,\n",
        "                batch_size=1,\n",
        "                drop_last=True,\n",
        "                shuffle=True,\n",
        "                )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        #print('val_dataloader')\n",
        "        return th.utils.data.DataLoader(\n",
        "                self.test_ds,\n",
        "                batch_size=32,\n",
        "                drop_last=False,\n",
        "                shuffle=True,\n",
        "                )\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return th.optim.SGD(\n",
        "            self.parameters(),\n",
        "            lr=0.01,\n",
        "            momentum=0.9,\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}