{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMX+DsJ8L5Phtmey6PaT4QD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pastrop/kaggle/blob/master/Data_Analyst.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install anthropic\n",
        "!pip install sentence-transformers\n",
        "!pip install pandas\n",
        "!pip install yake"
      ],
      "metadata": {
        "id": "rVMAhiK3kUfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import anthropic\n",
        "import os\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import logging\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "import logging\n",
        "import traceback"
      ],
      "metadata": {
        "id": "yr1qrObCkLnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfRnTARQf6bn"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('Agent_Anth')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "fZIXWRTSxf-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Service Classes\n",
        "class CSVReader:\n",
        "    \"\"\"Tool to read CSV files into pandas dataframes.\"\"\"\n",
        "\n",
        "    def read_csv(self, file_path: str) -> Tuple[pd.DataFrame, str]:\n",
        "        \"\"\"\n",
        "        Read a CSV file into a pandas dataframe.\n",
        "\n",
        "        Args:\n",
        "            file_path: Path to the CSV file\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (dataframe, message)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            df = pd.read_csv(file_path)\n",
        "            columns_info = \", \".join([f\"{col} ({df[col].dtype})\" for col in df.columns])\n",
        "            message = f\"Successfully loaded CSV with {len(df)} rows and {len(df.columns)} columns: {columns_info}\"\n",
        "            logger.info(message)\n",
        "            return df, message\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Error reading CSV file: {str(e)}\"\n",
        "            logger.error(error_msg)\n",
        "            return pd.DataFrame(), error_msg\n",
        "\n",
        "class QueryAnalyzer:\n",
        "    \"\"\"Tool to analyze queries and determine information needs.\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: Optional[str] = None):\n",
        "        \"\"\"\n",
        "        Initialize with Anthropic API key.\n",
        "\n",
        "        Args:\n",
        "            api_key: Anthropic API key (will use environment variable if None)\n",
        "        \"\"\"\n",
        "        self.api_key = api_key or os.environ.get(\"ANTHROPIC_API_KEY\")\n",
        "        if not self.api_key:\n",
        "            logger.warning(\"No Anthropic API key provided. Set ANTHROPIC_API_KEY environment variable.\")\n",
        "\n",
        "        self.client = anthropic.Anthropic(api_key=self.api_key)\n",
        "\n",
        "    def analyze_query(self, query: str, columns: List[str]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analyze the query to determine information needs.\n",
        "\n",
        "        Args:\n",
        "            query: The user's query\n",
        "            columns: List of available columns in the dataframe\n",
        "\n",
        "        Returns:\n",
        "            Dict with analysis results\n",
        "        \"\"\"\n",
        "        try:\n",
        "            prompt = f\"\"\"\n",
        "            <columns>\n",
        "            {', '.join(columns)}\n",
        "            </columns>\n",
        "\n",
        "            You are an AI assistant that analyzes queries about a dataset. Based on the user query, determine:\n",
        "            1. Which columns from the dataset are needed to answer the query\n",
        "            2. What type of analysis is required (filtering, aggregation, etc.)\n",
        "            3. Whether any specific values or conditions are mentioned\n",
        "\n",
        "            User query: {query}\n",
        "\n",
        "            Respond in JSON format like this:\n",
        "            {{\n",
        "                \"needed_columns\": [\"column1\", \"column2\"],\n",
        "                \"analysis_type\": \"one of: filtering, aggregation, sorting, comparison, general_info, semantic_search\",\n",
        "                \"filter_conditions\": {{\"column_name\": \"filter_value\"}},\n",
        "                \"aggregation_function\": \"one of: count, sum, average, min, max, none\",\n",
        "                \"sort_by\": \"column_name or null\",\n",
        "                \"sort_order\": \"ascending or descending or null\",\n",
        "                \"requires_text_search\": true/false,\n",
        "                \"search_term\": \"term to search for in text or null\"\n",
        "                \"query\": \"original query\"\n",
        "            }}\n",
        "\n",
        "            Make sure all column names exactly match the provided list. If a column is not mentioned or needed, don't include it.\n",
        "            \"\"\"\n",
        "\n",
        "            response = self.client.messages.create(\n",
        "                model=\"claude-3-haiku-20240307\",\n",
        "                max_tokens=1000,\n",
        "                messages=[\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            analysis_text = response.content[0].text\n",
        "            print(f'query_analyzer printout: response:{analysis_text}')\n",
        "\n",
        "            # Extract JSON from response\n",
        "            import json\n",
        "            import re\n",
        "\n",
        "            json_match = re.search(r'{[\\s\\S]+}', analysis_text)\n",
        "            if json_match:\n",
        "                analysis = json.loads(json_match.group(0))\n",
        "                logger.info(f\"Query analysis completed: {str(analysis)}\")\n",
        "                return analysis\n",
        "            else:\n",
        "                logger.error(\"Failed to extract JSON from Claude's response\")\n",
        "                return {\n",
        "                    \"needed_columns\": columns,\n",
        "                    \"analysis_type\": \"general_info\",\n",
        "                    \"requires_text_search\": False\n",
        "                }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error analyzing query: {str(e)}\")\n",
        "            return {\n",
        "                \"needed_columns\": columns,\n",
        "                \"analysis_type\": \"general_info\",\n",
        "                \"requires_text_search\": False\n",
        "            }\n",
        "\n",
        "class ColumnSelector:\n",
        "    \"\"\"Tool to determine which columns are needed for a query.\"\"\"\n",
        "\n",
        "    def select_columns(self, df: pd.DataFrame, analysis: Dict[str, Any]) -> List[str]:\n",
        "        \"\"\"\n",
        "        Select columns needed to answer the query.\n",
        "\n",
        "        Args:\n",
        "            df: The dataframe\n",
        "            analysis: Query analysis results\n",
        "\n",
        "        Returns:\n",
        "            List of column names to use\n",
        "        \"\"\"\n",
        "        all_columns = df.columns.tolist()\n",
        "\n",
        "        # Start with columns specified in the analysis\n",
        "        needed_columns = analysis.get(\"needed_columns\", [])\n",
        "\n",
        "        # Always include text column if text search is required\n",
        "        if analysis.get(\"requires_text_search\", False) and \"text\" in all_columns:\n",
        "            if \"text\" not in needed_columns:\n",
        "                needed_columns.append(\"text\")\n",
        "\n",
        "        # Add filter columns if not already included\n",
        "        filter_conditions = analysis.get(\"filter_conditions\", {})\n",
        "        for col in filter_conditions.keys():\n",
        "            if col in all_columns and col not in needed_columns:\n",
        "                needed_columns.append(col)\n",
        "\n",
        "        # Add sort column if not already included\n",
        "        sort_by = analysis.get(\"sort_by\")\n",
        "        if sort_by and sort_by in all_columns and sort_by not in needed_columns:\n",
        "            needed_columns.append(sort_by)\n",
        "\n",
        "        # If no columns were determined, return all columns\n",
        "        if not needed_columns:\n",
        "            logger.warning(\"No specific columns determined, using all columns\")\n",
        "            needed_columns = all_columns\n",
        "\n",
        "        logger.info(f\"Selected columns: {', '.join(needed_columns)}\")\n",
        "        print(f\"Column Selector Printout: Selected columns: {', '.join(needed_columns)}\")\n",
        "        return needed_columns\n",
        "\n",
        "class DataExtractor:\n",
        "    \"\"\"Tool to extract relevant data from the dataframe.\"\"\"\n",
        "\n",
        "    def extract_data(self, df: pd.DataFrame, analysis: Dict[str, Any], selected_columns: List[str]) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Extract relevant data based on query analysis.\n",
        "\n",
        "        Args:\n",
        "            df: The dataframe\n",
        "            analysis: Query analysis results\n",
        "            selected_columns: Columns to include\n",
        "\n",
        "        Returns:\n",
        "            Filtered dataframe\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Start with selected columns\n",
        "            result_df = df[selected_columns].copy()\n",
        "\n",
        "            # Apply filtering if specified\n",
        "            filter_conditions = analysis.get(\"filter_conditions\", {})\n",
        "            if filter_conditions and analysis.get(\"analysis_type\") in [\"filtering\", \"comparison\"]:\n",
        "                for col, value in filter_conditions.items():\n",
        "                    if col in df.columns:\n",
        "                        # Handle different filter types\n",
        "                        if isinstance(value, dict):\n",
        "                            # Range or comparison filter\n",
        "                            if \"min\" in value and \"max\" in value:\n",
        "                                result_df = result_df[(result_df[col] >= value[\"min\"]) &\n",
        "                                                     (result_df[col] <= value[\"max\"])]\n",
        "                            elif \"min\" in value:\n",
        "                                result_df = result_df[result_df[col] >= value[\"min\"]]\n",
        "                            elif \"max\" in value:\n",
        "                                result_df = result_df[result_df[col] <= value[\"max\"]]\n",
        "                            elif \"not_equal\" in value:\n",
        "                                result_df = result_df[result_df[col] != value[\"not_equal\"]]\n",
        "                        elif isinstance(value, list):\n",
        "                            # List of values\n",
        "                            result_df = result_df[result_df[col].isin(value)]\n",
        "                        else:\n",
        "                            # Simple equality\n",
        "                            result_df = result_df[result_df[col] == value]\n",
        "\n",
        "            # Apply sorting if specified\n",
        "            sort_by = analysis.get(\"sort_by\")\n",
        "            sort_order = analysis.get(\"sort_order\", \"ascending\")\n",
        "            if sort_by and sort_by in result_df.columns:\n",
        "                ascending = sort_order.lower() != \"descending\"\n",
        "                result_df = result_df.sort_values(by=sort_by, ascending=ascending)\n",
        "\n",
        "            # Apply aggregation if specified\n",
        "            agg_function = analysis.get(\"aggregation_function\")\n",
        "            if agg_function and agg_function != \"none\" and analysis.get(\"analysis_type\") == \"aggregation\":\n",
        "                # Determine which column to aggregate\n",
        "                agg_col = None\n",
        "                for col in result_df.columns:\n",
        "                    if col != \"text\" and pd.api.types.is_numeric_dtype(result_df[col]):\n",
        "                        agg_col = col\n",
        "                        break\n",
        "\n",
        "                if agg_col:\n",
        "                    if agg_function == \"count\":\n",
        "                        result_df = result_df.groupby(selected_columns[0])[agg_col].count().reset_index()\n",
        "                    elif agg_function == \"sum\":\n",
        "                        result_df = result_df.groupby(selected_columns[0])[agg_col].sum().reset_index()\n",
        "                    elif agg_function == \"average\":\n",
        "                        result_df = result_df.groupby(selected_columns[0])[agg_col].mean().reset_index()\n",
        "                    elif agg_function == \"min\":\n",
        "                        result_df = result_df.groupby(selected_columns[0])[agg_col].min().reset_index()\n",
        "                    elif agg_function == \"max\":\n",
        "                        result_df = result_df.groupby(selected_columns[0])[agg_col].max().reset_index()\n",
        "\n",
        "            logger.info(f\"Extracted {len(result_df)} rows of data\")\n",
        "            print(f\"Data Extractor Printout: {result_df}\")\n",
        "            return result_df\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error extracting data: {str(e)}\")\n",
        "            # Return original data with selected columns\n",
        "            return df[selected_columns].copy()\n",
        "\n",
        "class TextEmbedder:\n",
        "    \"\"\"Tool to generate and search text embeddings.\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
        "        \"\"\"\n",
        "        Initialize with embedding model.\n",
        "\n",
        "        Args:\n",
        "            model_name: Name of the embedding model\n",
        "        \"\"\"\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        logger.info(f\"Initialized embedding model: {model_name}\")\n",
        "\n",
        "    def search_similar_texts(self, df: pd.DataFrame, query: str, text_column: str = \"text\",\n",
        "                            top_k: int = 5) -> pd.DataFrame:\n",
        "       \"\"\"\n",
        "        Find texts most similar to the query.\n",
        "\n",
        "        Args:\n",
        "            df: Dataframe with text column\n",
        "            query: Search query\n",
        "            text_column: Column containing text\n",
        "            top_k: Number of results to return\n",
        "\n",
        "        Returns:\n",
        "            Dataframe with most similar texts\n",
        "        \"\"\"\n",
        "        print('I am inside searching simular texts')\n",
        "        if text_column not in df.columns:\n",
        "            logger.error(f\"Text column '{text_column}' not found in dataframe\")\n",
        "            return df\n",
        "\n",
        "        try:\n",
        "            # Generate embeddings\n",
        "            print('trying to generate embeddings')\n",
        "            texts = df[text_column].fillna(\"\").tolist()\n",
        "            print(f'text_embeddings printout: {texts}')\n",
        "            text_embeddings = self.model.encode(texts)\n",
        "            print('got the text embeddings!')\n",
        "            print(f'input query printout: {query} of type {type(query)}')\n",
        "            query_embedding = self.model.encode(query)\n",
        "            print('got the query embeddings!')\n",
        "\n",
        "            # Calculate similarities\n",
        "            similarities = cosine_similarity(\n",
        "                query_embedding.reshape(1, -1),\n",
        "                text_embeddings\n",
        "            )[0]\n",
        "\n",
        "            # Add similarity scores to dataframe\n",
        "            result_df = df.copy()\n",
        "            result_df[\"similarity_score\"] = similarities\n",
        "\n",
        "            # Sort by similarity and take top_k\n",
        "            result_df = result_df.sort_values(\"similarity_score\", ascending=False).head(top_k)\n",
        "\n",
        "            logger.info(f\"Found {len(result_df)} similar texts\")\n",
        "            print(f\"search_similar_texts printout: {result_df}\")\n",
        "            return result_df\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error searching similar texts: {str(e)}\")\n",
        "            print(traceback.format_exc())  # Print full traceback\n",
        "            return df\n",
        "\n",
        "class AnswerGenerator:\n",
        "    \"\"\"Tool to generate answers using Claude 3.7.\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: Optional[str] = None):\n",
        "        \"\"\"\n",
        "        Initialize with Anthropic API key.\n",
        "\n",
        "        Args:\n",
        "            api_key: Anthropic API key (will use environment variable if None)\n",
        "        \"\"\"\n",
        "        self.api_key = api_key or os.environ.get(\"ANTHROPIC_API_KEY\")\n",
        "        if not self.api_key:\n",
        "            logger.warning(\"No Anthropic API key provided. Set ANTHROPIC_API_KEY environment variable.\")\n",
        "\n",
        "        self.client = anthropic.Anthropic(api_key=self.api_key)\n",
        "\n",
        "    def generate_answer(self, query: str, data_df: pd.DataFrame, analysis: Dict[str, Any]) -> str:\n",
        "        \"\"\"\n",
        "        Generate an answer using Claude 3.7.\n",
        "\n",
        "        Args:\n",
        "            query: User query\n",
        "            data_df: Dataframe with relevant data\n",
        "            analysis: Query analysis results\n",
        "\n",
        "        Returns:\n",
        "            Generated answer\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Convert dataframe to string representation\n",
        "            data_str = data_df.to_string(index=False) if not data_df.empty else \"No data found\"\n",
        "\n",
        "            # Create prompt for Claude\n",
        "            prompt = f\"\"\"\n",
        "            <data>\n",
        "            {data_str}\n",
        "            </data>\n",
        "\n",
        "            <query_analysis>\n",
        "            {str(analysis)}\n",
        "            </query_analysis>\n",
        "\n",
        "            User query: {query}\n",
        "\n",
        "            Based on the provided data and analysis of the query, please provide a comprehensive answer to the user's question.\n",
        "            Include specific details from the data where appropriate. If the data doesn't contain information needed to answer the query,\n",
        "            state that clearly.\n",
        "\n",
        "            Answer the query directly and concisely. If appropriate, include any relevant statistics from the data.\n",
        "            \"\"\"\n",
        "\n",
        "            response = self.client.messages.create(\n",
        "                model=\"claude-3-haiku-20240307\",\n",
        "                max_tokens=2000,\n",
        "                messages=[\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            answer = response.content[0].text\n",
        "            logger.info(f\"Generated answer of length {len(answer)}\")\n",
        "            return answer\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generating answer: {str(e)}\")\n",
        "            return f\"I encountered an error while generating the answer: {str(e)}\""
      ],
      "metadata": {
        "id": "VPfobfxxj4x5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#logger = logging.getLogger(__name__)\n",
        "\n",
        "class CSVAgent:\n",
        "    \"\"\"Agent that analyzes CSV data to answer queries.\"\"\"\n",
        "\n",
        "    def __init__(self, csv_path, api_key=api_key):\n",
        "        \"\"\"\n",
        "        Initialize the agent.\n",
        "\n",
        "        Args:\n",
        "            csv_path: Path to the CSV file\n",
        "            api_key: Anthropic API key (optional)\n",
        "        \"\"\"\n",
        "        self.csv_path = csv_path\n",
        "        self.api_key = api_key\n",
        "\n",
        "        #Refactoring Prep:\n",
        "        #self.agent_client = anthropic.Anthropic(api_key=self.api_key)\n",
        "        #self.embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "        # Initialize tools\n",
        "        self.csv_reader = CSVReader()\n",
        "        self.query_analyzer = QueryAnalyzer(api_key=self.api_key)\n",
        "        self.column_selector = ColumnSelector()\n",
        "        self.data_extractor = DataExtractor()\n",
        "        self.text_embedder = TextEmbedder()\n",
        "        self.answer_generator = AnswerGenerator(api_key=self.api_key)\n",
        "\n",
        "        # Load CSV data\n",
        "        self.df, load_message = self.csv_reader.read_csv(csv_path)\n",
        "        logger.info(load_message)\n",
        "\n",
        "        # Store column information\n",
        "        self.columns = list(self.df.columns) if not self.df.empty else []\n",
        "\n",
        "    def process_query(self, query):\n",
        "        \"\"\"\n",
        "        Process a user query and generate an answer.\n",
        "\n",
        "        Args:\n",
        "            query: User query string\n",
        "\n",
        "        Returns:\n",
        "            dict: Response containing answer and processing details\n",
        "        \"\"\"\n",
        "        logger.info(f\"Processing query: {query}\")\n",
        "\n",
        "        if self.df.empty:\n",
        "            return {\n",
        "                \"answer\": \"Unable to analyze the CSV file. Please check the file path and format.\",\n",
        "                \"success\": False\n",
        "            }\n",
        "\n",
        "        try:\n",
        "            # Step 1: Analyze the query\n",
        "            analysis = self.query_analyzer.analyze_query(query, self.columns)\n",
        "\n",
        "            # Step 2: Select relevant columns\n",
        "            selected_columns = self.column_selector.select_columns(self.df, analysis)\n",
        "\n",
        "            # Step 3: Extract relevant data\n",
        "            extracted_data = self.data_extractor.extract_data(self.df, analysis, selected_columns)\n",
        "\n",
        "            # Step 4: Apply text search if needed\n",
        "            if analysis.get(\"requires_text_search\", False) and \"text\" in self.columns:\n",
        "                if analysis.get(\"search_term\") != None:\n",
        "                  search_term = analysis.get(\"search_term\")\n",
        "                else:\n",
        "                  search_term = analysis.get(\"query\")\n",
        "                extracted_data = self.text_embedder.search_similar_texts(\n",
        "                    extracted_data,\n",
        "                    search_term,\n",
        "                    text_column=\"text\",\n",
        "                    top_k=10\n",
        "                )\n",
        "\n",
        "            # Step 5: Generate answer\n",
        "            answer = self.answer_generator.generate_answer(query, extracted_data, analysis)\n",
        "\n",
        "            return {\n",
        "                \"answer\": answer,\n",
        "                \"columns_analyzed\": selected_columns,\n",
        "                \"rows_analyzed\": len(extracted_data),\n",
        "                \"analysis_type\": analysis.get(\"analysis_type\", \"unknown\"),\n",
        "                \"success\": True\n",
        "            }\n",
        "            '''\n",
        "            return{\n",
        "                \"success\":True\n",
        "            }\n",
        "            '''\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing query: {str(e)}\")\n",
        "            return {\n",
        "                \"answer\": f\"An error occurred while processing your query: {str(e)}\",\n",
        "                \"success\": False\n",
        "            }"
      ],
      "metadata": {
        "id": "K95z0uZElINi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_logging():\n",
        "    \"\"\"Set up logging configuration.\"\"\"\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "        handlers=[\n",
        "            logging.FileHandler(\"csv_agent.log\"),\n",
        "            logging.StreamHandler()\n",
        "        ]\n",
        "    )\n",
        "\n",
        "setup_logging()"
      ],
      "metadata": {
        "id": "9gb5jqXeljnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Initialize the agent\n",
        "agent = CSVAgent('alpha_test.csv')"
      ],
      "metadata": {
        "id": "1yUJkufXRekW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "queries examples:\n",
        "\n",
        "\" Please summarise texts about the Caldera Ginger Beer with the number_apperance 4\""
      ],
      "metadata": {
        "id": "K4fAjxz9JXjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \" Please find all beers that have the number_apperance 2.5 and return their names and the summary of texts for these beers\"\n",
        "\n",
        "result = agent.process_query(query)\n",
        "\n",
        "print(f\"\\nAnswer: {result['answer']}\\n\")\n",
        "if result['success']:\n",
        "    print(f\"Analysis type: {result['analysis_type']}\")\n",
        "    print(f\"Columns analyzed: {', '.join(result['columns_analyzed'])}\")\n",
        "    print(f\"Rows analyzed: {result['rows_analyzed']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPdMw_10RW5d",
        "outputId": "752a553c-8a16-4b73-c0ed-dfdd3a3cead1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query_analyzer printout: response:{\n",
            "    \"needed_columns\": [\"string_name\", \"text\"],\n",
            "    \"analysis_type\": \"filtering\",\n",
            "    \"filter_conditions\": {\"number_appearance\": 2.5},\n",
            "    \"aggregation_function\": \"none\",\n",
            "    \"sort_by\": null,\n",
            "    \"sort_order\": null,\n",
            "    \"requires_text_search\": false,\n",
            "    \"search_term\": null,\n",
            "    \"query\": \"Please find all beers that have the number_apperance 2.5 and return their names and the summary of texts for these beers\"\n",
            "}\n",
            "Column Selector Printout: Selected columns: string_name, text, number_appearance\n",
            "Data Extractor Printout:                    string_name  \\\n",
            "0                 Sausa Weizen   \n",
            "14       Caldera Oatmeal Stout   \n",
            "15       Caldera Oatmeal Stout   \n",
            "76            Caldera Pale Ale   \n",
            "190           Caldera Pale Ale   \n",
            "200           Caldera Pale Ale   \n",
            "257           Vas Deferens Ale   \n",
            "272  Old Growth Imperial Stout   \n",
            "\n",
            "                                                  text  number_appearance  \n",
            "0    A lot of foam. But a lot.\\tIn the smell some b...                2.5  \n",
            "14   Brown in color, somewhere between a porter and...                2.5  \n",
            "15   Caldera presents yet another circumstance wher...                2.5  \n",
            "76   Pours a crisp clear pale gold color with a sma...                2.5  \n",
            "190  Golden amber with a big foamy head that just f...                2.5  \n",
            "200  Poured into 12oz straight glass. Poured a clou...                2.5  \n",
            "257  Chilled bottle into a glass. A generous gift f...                2.5  \n",
            "272  Got this one from the Nashvillian, cheers John...                2.5  \n",
            "\n",
            "Answer: Based on the provided data and query analysis, the following beers have a number_appearance of 2.5:\n",
            "\n",
            "1. Sausa Weizen\n",
            "2. Caldera Oatmeal Stout\n",
            "3. Caldera Pale Ale (multiple entries)\n",
            "4. Vas Deferens Ale\n",
            "5. Old Growth Imperial Stout\n",
            "\n",
            "The summary of the text for these beers is as follows:\n",
            "\n",
            "Sausa Weizen:\n",
            "\"A lot of foam. But a lot.\\tIn the smell some banana, and then lactic and tart. Not a good start.\\tQuite dark orange in color, with a lively carbonation (now visible, under the foam).\\tAgain tending to lactic sourness.\\tSame for the taste. With some yeast and banana.\"\n",
            "\n",
            "Caldera Oatmeal Stout:\n",
            "\"Brown in color, somewhere between a porter and a brown ale. Lacking in aroma, but no off stuff. \\t\\tSame with the taste, lacking flavor, complexity, just went with smoothness. No off flavors though, so I can't say this is bad, just unadventurous, especially for Caldera, whom I think is generally underrated. You really have to search to pull anything out of this in terms of the usual chocolate/coffee flavors, really, the only thing I can tell is that the oats did their job, because this is smooth and unoffensive.\\t\\tOther than that, extremely pedestrian.\"\n",
            "\n",
            "Caldera Pale Ale (multiple entries):\n",
            "\"Pours a crisp clear pale gold color with a small head that dissipates rather quickly.\\t\\tHas a bitter hoppy pine and citrus smell. Very pungent for a APA.\\t\\tBitter bite hits right away and fades to a smooth caramel malty biscuity flavor. Not very complex, but good tastes none the less.\\t\\tA very light smooth beer. Very sessionable could def hammer a few of these back.\"\n",
            "\"Golden amber with a big foamy head that just floats on top. Crisp pale malt and floral hop aromas. Flavors of light floral hops with a good malty reflection that combine to make a honey-like finish with a light hop linger. Medium carbonation with a non descript mouthfeel.\\t\\tIt might not look like much but it tastes great although there's also not much to talk about in the mouthfeel. I really liked the honey I tasted in the finish.\"\n",
            "\"Poured into 12oz straight glass. Poured a cloudy yellow, almost the color of pineapple juice, with barely a thin cap of white head that had no retention and minimal lacing.\\t\\tThe aroma was mostly malt forward, a pleasant change from Pales that are basically \"IPA light\", with just enough hops to keep the scent balanced. The flavor was balanced as well, although I thought a little on the bland side. None of the flavors really jumped out at me.\\t\\tThe body was typical for the style but lost some points for being overly sticky. Drinkability was OK, I'll drink it again, but this beer is not on the level of Mirror Pond or Manny's.\\t\\tOverall, a decent enough brew, but nothing special.\"\n",
            "\n",
            "Vas Deferens Ale:\n",
            "\"Chilled bottle into a glass. A generous gift from ramnuts. Thanks, Frank! \\t\\tShared with alfrantzell and chswimmer. \\t\\tA: Pours a clear maroon body with a light tan head. The bubbles fill the tulip but quickly collapse into nothing. \\t\\tS: Not much aroma at all. Some nuts and orange, perhaps. \\t\\tT: This beer initially tasted like sushi. Ted narrowed it down to the seaweed wrap used in rolls, but I am convinced I picked up the soy / cardboard signs of oxidation, as well. As it warmed, those flavors receded a bit, producing flavors of orange, nuts, coffee, and some kind of spice. Somewhat disjointed. \\t\\tM: Oddly thin, with a watery flavor. \\t\\tD: Not enjoyable...\"\n",
            "\n",
            "Old Growth Imperial Stout:\n",
            "\"Got this one from the Nashvillian, cheers John!\\t\\tPours ebony with less than a pinky of mocha colored head.Near zero head retention & lacing\\t\\tS: Chocolate, dark fruit, vanilla, & a touch of Bourbon once warm\\t\\tT: Follows the nose, plus some herbal & woody hop notes, charred grain & dryness up front. More charred grain & Baker's chocolate as this warms. Finishes with a nice chocolate sweetness, herbal hops, a bit vanilla & more charred grain\\t\\tMF: Chewy, oily, subtle carbonation\\t\\tA nice Imp Stout, didn't quite come together for perfection, but a solid offering\"\n",
            "\n",
            "Analysis type: filtering\n",
            "Columns analyzed: string_name, text, number_appearance\n",
            "Rows analyzed: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Refactoring"
      ],
      "metadata": {
        "id": "uCBo5T8lynvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Service Classes\n",
        "class CSVReader:\n",
        "    \"\"\"Tool to read CSV files into pandas dataframes.\"\"\"\n",
        "\n",
        "    def read_csv(self, file_path: str) -> Tuple[pd.DataFrame, str]:\n",
        "        \"\"\"\n",
        "        Read a CSV file into a pandas dataframe.\n",
        "\n",
        "        Args:\n",
        "            file_path: Path to the CSV file\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (dataframe, message)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            df = pd.read_csv(file_path)\n",
        "            columns_info = \", \".join([f\"{col} ({df[col].dtype})\" for col in df.columns])\n",
        "            message = f\"Successfully loaded CSV with {len(df)} rows and {len(df.columns)} columns: {columns_info}\"\n",
        "            logger.info(message)\n",
        "            return df, message\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Error reading CSV file: {str(e)}\"\n",
        "            logger.error(error_msg)\n",
        "            return pd.DataFrame(), error_msg\n",
        "\n",
        "class Workflow:\n",
        "    \"\"\"\n",
        "    Tools to analyze queries and determine information needs;\n",
        "          to determine which columns are needed for a query;\n",
        "          to extract relevant data from the dataframe;\n",
        "          to generate and search text embeddings;\n",
        "          to generate answers using Claude 3.7.\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: Optional[str] = None, model_name: str = \"all-MiniLM-L6-v2\"):\n",
        "        \"\"\"\n",
        "        Initialize with Anthropic API key.\n",
        "\n",
        "        Args:\n",
        "            api_key: Anthropic API key (will use environment variable if None)\n",
        "\n",
        "        Initialize the embedding model.\n",
        "\n",
        "        Args:\n",
        "            model_name: Name of the embedding model\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        self.api_key = api_key or os.environ.get(\"ANTHROPIC_API_KEY\")\n",
        "\n",
        "        #if not self.api_key:\n",
        "            #logger.warning(\"No Anthropic API key provided. Set ANTHROPIC_API_KEY environment variable.\")\n",
        "\n",
        "        self.client = anthropic.Anthropic(api_key=self.api_key)\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "\n",
        "\n",
        "    def analyze_query(query: str, columns: List[str], api_key = api_key) -> Dict[str, Any]:\n",
        "            \"\"\"\n",
        "            Analyze the query to determine information needs.\n",
        "\n",
        "            Args:\n",
        "                query: The user's query\n",
        "                columns: List of available columns in the dataframe\n",
        "\n",
        "            Returns:\n",
        "                Dict with analysis results\n",
        "            \"\"\"\n",
        "            try:\n",
        "                prompt = f\"\"\"\n",
        "                <columns>\n",
        "                {', '.join(columns)}\n",
        "                </columns>\n",
        "\n",
        "                You are an AI assistant that analyzes queries about a dataset. Based on the user query, determine:\n",
        "                1. Which columns from the dataset are needed to answer the query\n",
        "                2. What type of analysis is required (filtering, aggregation, etc.)\n",
        "                3. Whether any specific values or conditions are mentioned\n",
        "\n",
        "                User query: {query}\n",
        "\n",
        "                Respond in JSON format like this:\n",
        "                {{\n",
        "                    \"needed_columns\": [\"column1\", \"column2\"],\n",
        "                    \"analysis_type\": \"one of: filtering, aggregation, sorting, comparison, general_info, semantic_search\",\n",
        "                    \"filter_conditions\": {{\"column_name\": \"filter_value\"}},\n",
        "                    \"aggregation_function\": \"one of: count, sum, average, min, max, none\",\n",
        "                    \"sort_by\": \"column_name or null\",\n",
        "                    \"sort_order\": \"ascending or descending or null\",\n",
        "                    \"requires_text_search\": true/false,\n",
        "                    \"search_term\": \"term to search for in text or null\"\n",
        "                    \"query\": \"original query\"\n",
        "                }}\n",
        "\n",
        "                Make sure all column names exactly match the provided list. If a column is not mentioned or needed, don't include it.\n",
        "                \"\"\"\n",
        "                client = anthropic.Anthropic(api_key)\n",
        "                response = self.client.messages.create(\n",
        "                    model=\"claude-3-haiku-20240307\",\n",
        "                    max_tokens=1000,\n",
        "                    messages=[\n",
        "                        {\"role\": \"user\", \"content\": prompt}\n",
        "                    ]\n",
        "                )\n",
        "\n",
        "                analysis_text = response.content[0].text\n",
        "                print(f'query_analyzer printout: response:{analysis_text}')\n",
        "\n",
        "                # Extract JSON from response\n",
        "                import json\n",
        "                import re\n",
        "\n",
        "                json_match = re.search(r'{[\\s\\S]+}', analysis_text)\n",
        "                if json_match:\n",
        "                    analysis = json.loads(json_match.group(0))\n",
        "                    logger.info(f\"Query analysis completed: {str(analysis)}\")\n",
        "                    return analysis\n",
        "                else:\n",
        "                    logger.error(\"Failed to extract JSON from Claude's response\")\n",
        "                    return {\n",
        "                        \"needed_columns\": columns,\n",
        "                        \"analysis_type\": \"general_info\",\n",
        "                        \"requires_text_search\": False\n",
        "                    }\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error analyzing query: {str(e)}\")\n",
        "                return {\n",
        "                    \"needed_columns\": columns,\n",
        "                    \"analysis_type\": \"general_info\",\n",
        "                    \"requires_text_search\": False\n",
        "                }\n",
        "\n",
        "    def select_columns(df: pd.DataFrame, analysis: Dict[str, Any]) -> List[str]:\n",
        "            \"\"\"\n",
        "            Select columns needed to answer the query.\n",
        "\n",
        "            Args:\n",
        "                df: The dataframe\n",
        "                analysis: Query analysis results\n",
        "\n",
        "            Returns:\n",
        "                List of column names to use\n",
        "            \"\"\"\n",
        "            all_columns = df.columns.tolist()\n",
        "\n",
        "            # Start with columns specified in the analysis\n",
        "            needed_columns = analysis.get(\"needed_columns\", [])\n",
        "\n",
        "            # Always include text column if text search is required\n",
        "            if analysis.get(\"requires_text_search\", False) and \"text\" in all_columns:\n",
        "                if \"text\" not in needed_columns:\n",
        "                    needed_columns.append(\"text\")\n",
        "\n",
        "            # Add filter columns if not already included\n",
        "            filter_conditions = analysis.get(\"filter_conditions\", {})\n",
        "            for col in filter_conditions.keys():\n",
        "                if col in all_columns and col not in needed_columns:\n",
        "                    needed_columns.append(col)\n",
        "\n",
        "            # Add sort column if not already included\n",
        "            sort_by = analysis.get(\"sort_by\")\n",
        "            if sort_by and sort_by in all_columns and sort_by not in needed_columns:\n",
        "                needed_columns.append(sort_by)\n",
        "\n",
        "            # If no columns were determined, return all columns\n",
        "            if not needed_columns:\n",
        "                logger.warning(\"No specific columns determined, using all columns\")\n",
        "                needed_columns = all_columns\n",
        "\n",
        "            logger.info(f\"Selected columns: {', '.join(needed_columns)}\")\n",
        "            print(f\"Column Selector Printout: Selected columns: {', '.join(needed_columns)}\")\n",
        "            return needed_columns\n",
        "\n",
        "    def extract_data(df: pd.DataFrame, analysis: Dict[str, Any], selected_columns: List[str]) -> pd.DataFrame:\n",
        "            \"\"\"\n",
        "            Extract relevant data based on query analysis.\n",
        "\n",
        "            Args:\n",
        "                df: The dataframe\n",
        "                analysis: Query analysis results\n",
        "                selected_columns: Columns to include\n",
        "\n",
        "            Returns:\n",
        "                Filtered dataframe\n",
        "            \"\"\"\n",
        "            try:\n",
        "                # Start with selected columns\n",
        "                result_df = df[selected_columns].copy()\n",
        "\n",
        "                # Apply filtering if specified\n",
        "                filter_conditions = analysis.get(\"filter_conditions\", {})\n",
        "                if filter_conditions and analysis.get(\"analysis_type\") in [\"filtering\", \"comparison\"]:\n",
        "                    for col, value in filter_conditions.items():\n",
        "                        if col in df.columns:\n",
        "                            # Handle different filter types\n",
        "                            if isinstance(value, dict):\n",
        "                                # Range or comparison filter\n",
        "                                if \"min\" in value and \"max\" in value:\n",
        "                                    result_df = result_df[(result_df[col] >= value[\"min\"]) &\n",
        "                                                        (result_df[col] <= value[\"max\"])]\n",
        "                                elif \"min\" in value:\n",
        "                                    result_df = result_df[result_df[col] >= value[\"min\"]]\n",
        "                                elif \"max\" in value:\n",
        "                                    result_df = result_df[result_df[col] <= value[\"max\"]]\n",
        "                                elif \"not_equal\" in value:\n",
        "                                    result_df = result_df[result_df[col] != value[\"not_equal\"]]\n",
        "                            elif isinstance(value, list):\n",
        "                                # List of values\n",
        "                                result_df = result_df[result_df[col].isin(value)]\n",
        "                            else:\n",
        "                                # Simple equality\n",
        "                                result_df = result_df[result_df[col] == value]\n",
        "\n",
        "                # Apply sorting if specified\n",
        "                sort_by = analysis.get(\"sort_by\")\n",
        "                sort_order = analysis.get(\"sort_order\", \"ascending\")\n",
        "                if sort_by and sort_by in result_df.columns:\n",
        "                    ascending = sort_order.lower() != \"descending\"\n",
        "                    result_df = result_df.sort_values(by=sort_by, ascending=ascending)\n",
        "\n",
        "                # Apply aggregation if specified\n",
        "                agg_function = analysis.get(\"aggregation_function\")\n",
        "                if agg_function and agg_function != \"none\" and analysis.get(\"analysis_type\") == \"aggregation\":\n",
        "                    # Determine which column to aggregate\n",
        "                    agg_col = None\n",
        "                    for col in result_df.columns:\n",
        "                        if col != \"text\" and pd.api.types.is_numeric_dtype(result_df[col]):\n",
        "                            agg_col = col\n",
        "                            break\n",
        "\n",
        "                    if agg_col:\n",
        "                        if agg_function == \"count\":\n",
        "                            result_df = result_df.groupby(selected_columns[0])[agg_col].count().reset_index()\n",
        "                        elif agg_function == \"sum\":\n",
        "                            result_df = result_df.groupby(selected_columns[0])[agg_col].sum().reset_index()\n",
        "                        elif agg_function == \"average\":\n",
        "                            result_df = result_df.groupby(selected_columns[0])[agg_col].mean().reset_index()\n",
        "                        elif agg_function == \"min\":\n",
        "                            result_df = result_df.groupby(selected_columns[0])[agg_col].min().reset_index()\n",
        "                        elif agg_function == \"max\":\n",
        "                            result_df = result_df.groupby(selected_columns[0])[agg_col].max().reset_index()\n",
        "\n",
        "                logger.info(f\"Extracted {len(result_df)} rows of data\")\n",
        "                print(f\"Data Extractor Printout: {result_df}\")\n",
        "                return result_df\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error extracting data: {str(e)}\")\n",
        "                # Return original data with selected columns\n",
        "                return df[selected_columns].copy()\n",
        "\n",
        "\n",
        "\n",
        "    def search_similar_texts(df: pd.DataFrame, query: str, model: SentenceTransformer,\n",
        "                            text_column: str = \"text\", top_k: int = 5) -> pd.DataFrame:\n",
        "          \"\"\"\n",
        "            Find texts most similar to the query.\n",
        "\n",
        "            Args:\n",
        "                df: Dataframe with text column\n",
        "                query: Search query\n",
        "                text_column: Column containing text\n",
        "                top_k: Number of results to return\n",
        "\n",
        "            Returns:\n",
        "                Dataframe with most similar texts\n",
        "            \"\"\"\n",
        "            print('I am inside searching simular texts')\n",
        "            if text_column not in df.columns:\n",
        "                logger.error(f\"Text column '{text_column}' not found in dataframe\")\n",
        "                return df\n",
        "\n",
        "            try:\n",
        "                # Generate embeddings\n",
        "                print('trying to generate embeddings')\n",
        "                texts = df[text_column].fillna(\"\").tolist()\n",
        "                print(f'text_embeddings printout: {texts}')\n",
        "                text_embeddings = self.model.encode(texts)\n",
        "                print('got the text embeddings!')\n",
        "                print(f'input query printout: {query} of type {type(query)}')\n",
        "                query_embedding = self.model.encode(query)\n",
        "                print('got the query embeddings!')\n",
        "\n",
        "                # Calculate similarities\n",
        "                similarities = cosine_similarity(\n",
        "                    query_embedding.reshape(1, -1),\n",
        "                    text_embeddings\n",
        "                )[0]\n",
        "\n",
        "                # Add similarity scores to dataframe\n",
        "                result_df = df.copy()\n",
        "                result_df[\"similarity_score\"] = similarities\n",
        "\n",
        "                # Sort by similarity and take top_k\n",
        "                result_df = result_df.sort_values(\"similarity_score\", ascending=False).head(top_k)\n",
        "\n",
        "                logger.info(f\"Found {len(result_df)} similar texts\")\n",
        "                print(f\"search_similar_texts printout: {result_df}\")\n",
        "                return result_df\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error searching similar texts: {str(e)}\")\n",
        "                print(traceback.format_exc())  # Print full traceback\n",
        "                return df\n",
        "\n",
        "    def generate_answer(query: str, data_df: pd.DataFrame, analysis: Dict[str, Any],\n",
        "                      client: anthropic.Anthropic) -> str:\n",
        "            \"\"\"\n",
        "            Generate an answer using Claude 3.7.\n",
        "\n",
        "            Args:\n",
        "                query: User query\n",
        "                data_df: Dataframe with relevant data\n",
        "                analysis: Query analysis results\n",
        "\n",
        "            Returns:\n",
        "                Generated answer\n",
        "            \"\"\"\n",
        "            try:\n",
        "                # Convert dataframe to string representation\n",
        "                data_str = data_df.to_string(index=False) if not data_df.empty else \"No data found\"\n",
        "\n",
        "                # Create prompt for Claude\n",
        "                prompt = f\"\"\"\n",
        "                <data>\n",
        "                {data_str}\n",
        "                </data>\n",
        "\n",
        "                <query_analysis>\n",
        "                {str(analysis)}\n",
        "                </query_analysis>\n",
        "\n",
        "                User query: {query}\n",
        "\n",
        "                Based on the provided data and analysis of the query, please provide a comprehensive answer to the user's question.\n",
        "                Include specific details from the data where appropriate. If the data doesn't contain information needed to answer the query,\n",
        "                state that clearly.\n",
        "\n",
        "                Answer the query directly and concisely. If appropriate, include any relevant statistics from the data.\n",
        "                \"\"\"\n",
        "\n",
        "                response = self.client.messages.create(\n",
        "                    model=\"claude-3-haiku-20240307\",\n",
        "                    max_tokens=2000,\n",
        "                    messages=[\n",
        "                        {\"role\": \"user\", \"content\": prompt}\n",
        "                    ]\n",
        "                )\n",
        "\n",
        "                answer = response.content[0].text\n",
        "                logger.info(f\"Generated answer of length {len(answer)}\")\n",
        "                return answer\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error generating answer: {str(e)}\")\n",
        "                return f\"I encountered an error while generating the answer: {str(e)}\""
      ],
      "metadata": {
        "id": "Yj59PNl219vJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Potential new tools:\n",
        "# 1. Concept Extraction\n",
        "import yake\n",
        "# YAKE Config\n",
        "kw_extractor = yake.KeywordExtractor()\n",
        "language = 'en'\n",
        "max_ngram_size = 2\n",
        "deduplication_threshold = 0.9\n",
        "numOfKeywords = 50\n",
        "#get the document corpus (assumes that the text is in the \"text\" column):\n",
        "\n",
        "def text_input(file = 'alpha_test.csv'):\n",
        "  df = pd.read_csv(file)\n",
        "  df_clean = df[df['text'].apply(lambda x: isinstance(x, str))]\n",
        "  texts = [item.replace(\"\\t\", \" \") for item in df_clean['text']]\n",
        "  return texts\n",
        "\n",
        "\n",
        "#Keyword for the corpus a.k.a Global Concepts\n",
        "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
        "keywords = custom_kw_extractor.extract_keywords(corpus)\n",
        "#select a number of keywords to work with\n",
        "def keywords_number(n = len(keywords), input=keywords):\n",
        "  return [item[0] for item in input[:n]]"
      ],
      "metadata": {
        "id": "iYGTizz7SWZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#logger = logging.getLogger(__name__)\n",
        "\n",
        "class CSVAgent_Rfct:\n",
        "    \"\"\"Agent that analyzes CSV data to answer queries.\"\"\"\n",
        "\n",
        "    def __init__(self, csv_path, api_key=api_key):\n",
        "        \"\"\"\n",
        "        Initialize the agent.\n",
        "\n",
        "        Args:\n",
        "            csv_path: Path to the CSV file\n",
        "            api_key: Anthropic API key (optional)\n",
        "        \"\"\"\n",
        "        self.csv_path = csv_path\n",
        "        self.api_key = api_key\n",
        "\n",
        "        #Refactoring Prep:\n",
        "        self.workflow = Workflow(api_key=self.api_key)\n",
        "\n",
        "        # Initialize tools\n",
        "        self.csv_reader = CSVReader()\n",
        "        self.query_analyzer = QueryAnalyzer(api_key=self.api_key)\n",
        "        self.column_selector = ColumnSelector()\n",
        "        self.data_extractor = DataExtractor()\n",
        "        self.text_embedder = TextEmbedder()\n",
        "        self.answer_generator = AnswerGenerator(api_key=self.api_key)\n",
        "\n",
        "\n",
        "        #Recatoring Prep (new class functions)\n",
        "\n",
        "\n",
        "\n",
        "        # Load CSV data\n",
        "        self.df, load_message = self.csv_reader.read_csv(csv_path)\n",
        "        logger.info(load_message)\n",
        "\n",
        "        # Store column information\n",
        "        self.columns = list(self.df.columns) if not self.df.empty else []\n",
        "\n",
        "    def process_query(self, query):\n",
        "        \"\"\"\n",
        "        Process a user query and generate an answer.\n",
        "\n",
        "        Args:\n",
        "            query: User query string\n",
        "\n",
        "        Returns:\n",
        "            dict: Response containing answer and processing details\n",
        "        \"\"\"\n",
        "        logger.info(f\"Processing query: {query}\")\n",
        "\n",
        "        if self.df.empty:\n",
        "            return {\n",
        "                \"answer\": \"Unable to analyze the CSV file. Please check the file path and format.\",\n",
        "                \"success\": False\n",
        "            }\n",
        "\n",
        "        try:\n",
        "            # Step 1: Analyze the query\n",
        "            analysis = self.workflow.analyze_query(query, self.columns)\n",
        "\n",
        "            # Step 2: Select relevant columns\n",
        "            selected_columns = self.workflow.select_columns(self.df, analysis)\n",
        "\n",
        "            # Step 3: Extract relevant data\n",
        "            extracted_data = self.workflow.extract_data(self.df, analysis, selected_columns)\n",
        "\n",
        "            # Step 4: Apply text search if needed\n",
        "            if analysis.get(\"requires_text_search\", False) and \"text\" in self.columns:\n",
        "                if analysis.get(\"search_term\") != None:\n",
        "                  search_term = analysis.get(\"search_term\")\n",
        "                else:\n",
        "                  search_term = analysis.get(\"query\")\n",
        "                extracted_data = self.workflow.search_similar_texts(\n",
        "                    extracted_data,\n",
        "                    search_term,\n",
        "                    text_column=\"text\",\n",
        "                    top_k=10\n",
        "                )\n",
        "\n",
        "            # Step 5: Generate answer\n",
        "            answer = self.workflow.generate_answer(query, extracted_data, analysis)\n",
        "\n",
        "            return {\n",
        "                \"answer\": answer,\n",
        "                \"columns_analyzed\": selected_columns,\n",
        "                \"rows_analyzed\": len(extracted_data),\n",
        "                \"analysis_type\": analysis.get(\"analysis_type\", \"unknown\"),\n",
        "                \"success\": True\n",
        "            }\n",
        "            '''\n",
        "            return{\n",
        "                \"success\":True\n",
        "            }\n",
        "            '''\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing query: {str(e)}\")\n",
        "            return {\n",
        "                \"answer\": f\"An error occurred while processing your query: {str(e)}\",\n",
        "                \"success\": False\n",
        "            }"
      ],
      "metadata": {
        "id": "Pa6Y2yrbfaWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Initialize the agent\n",
        "agent = CSVAgent_Rfct('alpha_test.csv')"
      ],
      "metadata": {
        "id": "_cTcOBusfyot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models Connectivity Tests"
      ],
      "metadata": {
        "id": "FtcJ-rPoOFG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Anthropic client\n",
        "client = anthropic.Anthropic(api_key=api_key)\n",
        "\n",
        "try:\n",
        "    # Make a simple API request\n",
        "    response = client.messages.create(\n",
        "        model=\"claude-3-haiku-20240307\",\n",
        "        max_tokens=50,\n",
        "        messages=[{\"role\": \"user\", \"content\": \"Hello! Can you confirm you're working?\"}]\n",
        "    )\n",
        "\n",
        "    # Print the response\n",
        "    print(\"Claude's Response:\", response.content)\n",
        "\n",
        "except anthropic.APIStatusError as e:\n",
        "    print(f\"API returned an error: {e}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "e0NCjdsuOMKJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}