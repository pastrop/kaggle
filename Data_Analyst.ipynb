{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZLa1lrevKs8NfDh8T7LbT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pastrop/kaggle/blob/master/Data_Analyst.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install anthropic\n",
        "!pip install sentence-transformers\n",
        "!pip install pandas"
      ],
      "metadata": {
        "id": "rVMAhiK3kUfJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import anthropic\n",
        "import os\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import logging\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "import logging"
      ],
      "metadata": {
        "id": "yr1qrObCkLnt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "YfRnTARQf6bn"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('Agent_Anth')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class CSVReader:\n",
        "    \"\"\"Tool to read CSV files into pandas dataframes.\"\"\"\n",
        "\n",
        "    def read_csv(self, file_path: str) -> Tuple[pd.DataFrame, str]:\n",
        "        \"\"\"\n",
        "        Read a CSV file into a pandas dataframe.\n",
        "\n",
        "        Args:\n",
        "            file_path: Path to the CSV file\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (dataframe, message)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            df = pd.read_csv(file_path)\n",
        "            columns_info = \", \".join([f\"{col} ({df[col].dtype})\" for col in df.columns])\n",
        "            message = f\"Successfully loaded CSV with {len(df)} rows and {len(df.columns)} columns: {columns_info}\"\n",
        "            logger.info(message)\n",
        "            return df, message\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Error reading CSV file: {str(e)}\"\n",
        "            logger.error(error_msg)\n",
        "            return pd.DataFrame(), error_msg\n",
        "\n",
        "class QueryAnalyzer:\n",
        "    \"\"\"Tool to analyze queries and determine information needs.\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: Optional[str] = None):\n",
        "        \"\"\"\n",
        "        Initialize with Anthropic API key.\n",
        "\n",
        "        Args:\n",
        "            api_key: Anthropic API key (will use environment variable if None)\n",
        "        \"\"\"\n",
        "        self.api_key = api_key or os.environ.get(\"ANTHROPIC_API_KEY\")\n",
        "        if not self.api_key:\n",
        "            logger.warning(\"No Anthropic API key provided. Set ANTHROPIC_API_KEY environment variable.\")\n",
        "\n",
        "        self.client = anthropic.Anthropic(api_key=self.api_key)\n",
        "\n",
        "    def analyze_query(self, query: str, columns: List[str]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analyze the query to determine information needs.\n",
        "\n",
        "        Args:\n",
        "            query: The user's query\n",
        "            columns: List of available columns in the dataframe\n",
        "\n",
        "        Returns:\n",
        "            Dict with analysis results\n",
        "        \"\"\"\n",
        "        try:\n",
        "            prompt = f\"\"\"\n",
        "            <columns>\n",
        "            {', '.join(columns)}\n",
        "            </columns>\n",
        "\n",
        "            You are an AI assistant that analyzes queries about a dataset. Based on the user query, determine:\n",
        "            1. Which columns from the dataset are needed to answer the query\n",
        "            2. What type of analysis is required (filtering, aggregation, etc.)\n",
        "            3. Whether any specific values or conditions are mentioned\n",
        "\n",
        "            User query: {query}\n",
        "\n",
        "            Respond in JSON format like this:\n",
        "            {{\n",
        "                \"needed_columns\": [\"column1\", \"column2\"],\n",
        "                \"analysis_type\": \"one of: filtering, aggregation, sorting, comparison, general_info, semantic_search\",\n",
        "                \"filter_conditions\": {{\"column_name\": \"filter_value\"}},\n",
        "                \"aggregation_function\": \"one of: count, sum, average, min, max, none\",\n",
        "                \"sort_by\": \"column_name or null\",\n",
        "                \"sort_order\": \"ascending or descending or null\",\n",
        "                \"requires_text_search\": true/false,\n",
        "                \"search_term\": \"term to search for in text or null\"\n",
        "            }}\n",
        "\n",
        "            Make sure all column names exactly match the provided list. If a column is not mentioned or needed, don't include it.\n",
        "            \"\"\"\n",
        "\n",
        "            response = self.client.messages.create(\n",
        "                model=\"claude-3-haiku-20240307\",\n",
        "                max_tokens=1000,\n",
        "                messages=[\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            analysis_text = response.content[0].text\n",
        "            print(f'query_analyzer response:{analysis_text}')\n",
        "\n",
        "            # Extract JSON from response\n",
        "            import json\n",
        "            import re\n",
        "\n",
        "            json_match = re.search(r'{[\\s\\S]+}', analysis_text)\n",
        "            if json_match:\n",
        "                analysis = json.loads(json_match.group(0))\n",
        "                logger.info(f\"Query analysis completed: {str(analysis)}\")\n",
        "                return analysis\n",
        "            else:\n",
        "                logger.error(\"Failed to extract JSON from Claude's response\")\n",
        "                return {\n",
        "                    \"needed_columns\": columns,\n",
        "                    \"analysis_type\": \"general_info\",\n",
        "                    \"requires_text_search\": False\n",
        "                }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error analyzing query: {str(e)}\")\n",
        "            return {\n",
        "                \"needed_columns\": columns,\n",
        "                \"analysis_type\": \"general_info\",\n",
        "                \"requires_text_search\": False\n",
        "            }\n",
        "\n",
        "class ColumnSelector:\n",
        "    \"\"\"Tool to determine which columns are needed for a query.\"\"\"\n",
        "\n",
        "    def select_columns(self, df: pd.DataFrame, analysis: Dict[str, Any]) -> List[str]:\n",
        "        \"\"\"\n",
        "        Select columns needed to answer the query.\n",
        "\n",
        "        Args:\n",
        "            df: The dataframe\n",
        "            analysis: Query analysis results\n",
        "\n",
        "        Returns:\n",
        "            List of column names to use\n",
        "        \"\"\"\n",
        "        all_columns = df.columns.tolist()\n",
        "\n",
        "        # Start with columns specified in the analysis\n",
        "        needed_columns = analysis.get(\"needed_columns\", [])\n",
        "\n",
        "        # Always include text column if text search is required\n",
        "        if analysis.get(\"requires_text_search\", False) and \"text\" in all_columns:\n",
        "            if \"text\" not in needed_columns:\n",
        "                needed_columns.append(\"text\")\n",
        "\n",
        "        # Add filter columns if not already included\n",
        "        filter_conditions = analysis.get(\"filter_conditions\", {})\n",
        "        for col in filter_conditions.keys():\n",
        "            if col in all_columns and col not in needed_columns:\n",
        "                needed_columns.append(col)\n",
        "\n",
        "        # Add sort column if not already included\n",
        "        sort_by = analysis.get(\"sort_by\")\n",
        "        if sort_by and sort_by in all_columns and sort_by not in needed_columns:\n",
        "            needed_columns.append(sort_by)\n",
        "\n",
        "        # If no columns were determined, return all columns\n",
        "        if not needed_columns:\n",
        "            logger.warning(\"No specific columns determined, using all columns\")\n",
        "            needed_columns = all_columns\n",
        "\n",
        "        logger.info(f\"Selected columns: {', '.join(needed_columns)}\")\n",
        "        return needed_columns\n",
        "\n",
        "class DataExtractor:\n",
        "    \"\"\"Tool to extract relevant data from the dataframe.\"\"\"\n",
        "\n",
        "    def extract_data(self, df: pd.DataFrame, analysis: Dict[str, Any], selected_columns: List[str]) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Extract relevant data based on query analysis.\n",
        "\n",
        "        Args:\n",
        "            df: The dataframe\n",
        "            analysis: Query analysis results\n",
        "            selected_columns: Columns to include\n",
        "\n",
        "        Returns:\n",
        "            Filtered dataframe\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Start with selected columns\n",
        "            result_df = df[selected_columns].copy()\n",
        "\n",
        "            # Apply filtering if specified\n",
        "            filter_conditions = analysis.get(\"filter_conditions\", {})\n",
        "            if filter_conditions and analysis.get(\"analysis_type\") in [\"filtering\", \"comparison\"]:\n",
        "                for col, value in filter_conditions.items():\n",
        "                    if col in df.columns:\n",
        "                        # Handle different filter types\n",
        "                        if isinstance(value, dict):\n",
        "                            # Range or comparison filter\n",
        "                            if \"min\" in value and \"max\" in value:\n",
        "                                result_df = result_df[(result_df[col] >= value[\"min\"]) &\n",
        "                                                     (result_df[col] <= value[\"max\"])]\n",
        "                            elif \"min\" in value:\n",
        "                                result_df = result_df[result_df[col] >= value[\"min\"]]\n",
        "                            elif \"max\" in value:\n",
        "                                result_df = result_df[result_df[col] <= value[\"max\"]]\n",
        "                            elif \"not_equal\" in value:\n",
        "                                result_df = result_df[result_df[col] != value[\"not_equal\"]]\n",
        "                        elif isinstance(value, list):\n",
        "                            # List of values\n",
        "                            result_df = result_df[result_df[col].isin(value)]\n",
        "                        else:\n",
        "                            # Simple equality\n",
        "                            result_df = result_df[result_df[col] == value]\n",
        "\n",
        "            # Apply sorting if specified\n",
        "            sort_by = analysis.get(\"sort_by\")\n",
        "            sort_order = analysis.get(\"sort_order\", \"ascending\")\n",
        "            if sort_by and sort_by in result_df.columns:\n",
        "                ascending = sort_order.lower() != \"descending\"\n",
        "                result_df = result_df.sort_values(by=sort_by, ascending=ascending)\n",
        "\n",
        "            # Apply aggregation if specified\n",
        "            agg_function = analysis.get(\"aggregation_function\")\n",
        "            if agg_function and agg_function != \"none\" and analysis.get(\"analysis_type\") == \"aggregation\":\n",
        "                # Determine which column to aggregate\n",
        "                agg_col = None\n",
        "                for col in result_df.columns:\n",
        "                    if col != \"text\" and pd.api.types.is_numeric_dtype(result_df[col]):\n",
        "                        agg_col = col\n",
        "                        break\n",
        "\n",
        "                if agg_col:\n",
        "                    if agg_function == \"count\":\n",
        "                        result_df = result_df.groupby(selected_columns[0])[agg_col].count().reset_index()\n",
        "                    elif agg_function == \"sum\":\n",
        "                        result_df = result_df.groupby(selected_columns[0])[agg_col].sum().reset_index()\n",
        "                    elif agg_function == \"average\":\n",
        "                        result_df = result_df.groupby(selected_columns[0])[agg_col].mean().reset_index()\n",
        "                    elif agg_function == \"min\":\n",
        "                        result_df = result_df.groupby(selected_columns[0])[agg_col].min().reset_index()\n",
        "                    elif agg_function == \"max\":\n",
        "                        result_df = result_df.groupby(selected_columns[0])[agg_col].max().reset_index()\n",
        "\n",
        "            logger.info(f\"Extracted {len(result_df)} rows of data\")\n",
        "            return result_df\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error extracting data: {str(e)}\")\n",
        "            # Return original data with selected columns\n",
        "            return df[selected_columns].copy()\n",
        "\n",
        "class TextEmbedder:\n",
        "    \"\"\"Tool to generate and search text embeddings.\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
        "        \"\"\"\n",
        "        Initialize with embedding model.\n",
        "\n",
        "        Args:\n",
        "            model_name: Name of the embedding model\n",
        "        \"\"\"\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        logger.info(f\"Initialized embedding model: {model_name}\")\n",
        "\n",
        "    def search_similar_texts(self, df: pd.DataFrame, query: str, text_column: str = \"text\",\n",
        "                            top_k: int = 5) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Find texts most similar to the query.\n",
        "\n",
        "        Args:\n",
        "            df: Dataframe with text column\n",
        "            query: Search query\n",
        "            text_column: Column containing text\n",
        "            top_k: Number of results to return\n",
        "\n",
        "        Returns:\n",
        "            Dataframe with most similar texts\n",
        "        \"\"\"\n",
        "        if text_column not in df.columns:\n",
        "            logger.error(f\"Text column '{text_column}' not found in dataframe\")\n",
        "            return df\n",
        "\n",
        "        try:\n",
        "            # Generate embeddings\n",
        "            texts = df[text_column].fillna(\"\").tolist()\n",
        "            text_embeddings = self.model.encode(texts)\n",
        "            query_embedding = self.model.encode(query)\n",
        "\n",
        "            # Calculate similarities\n",
        "            similarities = cosine_similarity(\n",
        "                query_embedding.reshape(1, -1),\n",
        "                text_embeddings\n",
        "            )[0]\n",
        "\n",
        "            # Add similarity scores to dataframe\n",
        "            result_df = df.copy()\n",
        "            result_df[\"similarity_score\"] = similarities\n",
        "\n",
        "            # Sort by similarity and take top_k\n",
        "            result_df = result_df.sort_values(\"similarity_score\", ascending=False).head(top_k)\n",
        "\n",
        "            logger.info(f\"Found {len(result_df)} similar texts\")\n",
        "            return result_df\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error searching similar texts: {str(e)}\")\n",
        "            return df\n",
        "\n",
        "class AnswerGenerator:\n",
        "    \"\"\"Tool to generate answers using Claude 3.7.\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: Optional[str] = None):\n",
        "        \"\"\"\n",
        "        Initialize with Anthropic API key.\n",
        "\n",
        "        Args:\n",
        "            api_key: Anthropic API key (will use environment variable if None)\n",
        "        \"\"\"\n",
        "        self.api_key = api_key or os.environ.get(\"ANTHROPIC_API_KEY\")\n",
        "        if not self.api_key:\n",
        "            logger.warning(\"No Anthropic API key provided. Set ANTHROPIC_API_KEY environment variable.\")\n",
        "\n",
        "        self.client = anthropic.Anthropic(api_key=self.api_key)\n",
        "\n",
        "    def generate_answer(self, query: str, data_df: pd.DataFrame, analysis: Dict[str, Any]) -> str:\n",
        "        \"\"\"\n",
        "        Generate an answer using Claude 3.7.\n",
        "\n",
        "        Args:\n",
        "            query: User query\n",
        "            data_df: Dataframe with relevant data\n",
        "            analysis: Query analysis results\n",
        "\n",
        "        Returns:\n",
        "            Generated answer\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Convert dataframe to string representation\n",
        "            data_str = data_df.to_string(index=False) if not data_df.empty else \"No data found\"\n",
        "\n",
        "            # Create prompt for Claude\n",
        "            prompt = f\"\"\"\n",
        "            <data>\n",
        "            {data_str}\n",
        "            </data>\n",
        "\n",
        "            <query_analysis>\n",
        "            {str(analysis)}\n",
        "            </query_analysis>\n",
        "\n",
        "            User query: {query}\n",
        "\n",
        "            Based on the provided data and analysis of the query, please provide a comprehensive answer to the user's question.\n",
        "            Include specific details from the data where appropriate. If the data doesn't contain information needed to answer the query,\n",
        "            state that clearly.\n",
        "\n",
        "            Answer the query directly and concisely. If appropriate, include any relevant statistics from the data.\n",
        "            \"\"\"\n",
        "\n",
        "            response = self.client.messages.create(\n",
        "                model=\"claude-3-7-sonnet-20240620\",\n",
        "                max_tokens=2000,\n",
        "                messages=[\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            answer = response.content[0].text\n",
        "            logger.info(f\"Generated answer of length {len(answer)}\")\n",
        "            return answer\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generating answer: {str(e)}\")\n",
        "            return f\"I encountered an error while generating the answer: {str(e)}\""
      ],
      "metadata": {
        "id": "VPfobfxxj4x5"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class CSVAgent:\n",
        "    \"\"\"Agent that analyzes CSV data to answer queries.\"\"\"\n",
        "\n",
        "    def __init__(self, csv_path, api_key=api_key):\n",
        "        \"\"\"\n",
        "        Initialize the agent.\n",
        "\n",
        "        Args:\n",
        "            csv_path: Path to the CSV file\n",
        "            api_key: Anthropic API key (optional)\n",
        "        \"\"\"\n",
        "        self.csv_path = csv_path\n",
        "        self.api_key = api_key\n",
        "\n",
        "        # Initialize tools\n",
        "        self.csv_reader = CSVReader()\n",
        "        self.query_analyzer = QueryAnalyzer(api_key=self.api_key)\n",
        "        self.column_selector = ColumnSelector()\n",
        "        self.data_extractor = DataExtractor()\n",
        "        self.text_embedder = TextEmbedder()\n",
        "        self.answer_generator = AnswerGenerator(api_key=self.api_key)\n",
        "\n",
        "        # Load CSV data\n",
        "        self.df, load_message = self.csv_reader.read_csv(csv_path)\n",
        "        logger.info(load_message)\n",
        "\n",
        "        # Store column information\n",
        "        self.columns = list(self.df.columns) if not self.df.empty else []\n",
        "\n",
        "    def process_query(self, query):\n",
        "        \"\"\"\n",
        "        Process a user query and generate an answer.\n",
        "\n",
        "        Args:\n",
        "            query: User query string\n",
        "\n",
        "        Returns:\n",
        "            dict: Response containing answer and processing details\n",
        "        \"\"\"\n",
        "        logger.info(f\"Processing query: {query}\")\n",
        "\n",
        "        if self.df.empty:\n",
        "            return {\n",
        "                \"answer\": \"Unable to analyze the CSV file. Please check the file path and format.\",\n",
        "                \"success\": False\n",
        "            }\n",
        "\n",
        "        try:\n",
        "            # Step 1: Analyze the query\n",
        "            analysis = self.query_analyzer.analyze_query(query, self.columns)\n",
        "            '''\n",
        "\n",
        "            # Step 2: Select relevant columns\n",
        "            selected_columns = self.column_selector.select_columns(self.df, analysis)\n",
        "\n",
        "            # Step 3: Extract relevant data\n",
        "            extracted_data = self.data_extractor.extract_data(self.df, analysis, selected_columns)\n",
        "\n",
        "            # Step 4: Apply text search if needed\n",
        "            if analysis.get(\"requires_text_search\", False) and \"text\" in self.columns:\n",
        "                search_term = analysis.get(\"search_term\", query)\n",
        "                extracted_data = self.text_embedder.search_similar_texts(\n",
        "                    extracted_data,\n",
        "                    search_term,\n",
        "                    text_column=\"text\",\n",
        "                    top_k=10\n",
        "                )\n",
        "\n",
        "            # Step 5: Generate answer\n",
        "            answer = self.answer_generator.generate_answer(query, extracted_data, analysis)\n",
        "\n",
        "            return {\n",
        "                \"answer\": answer,\n",
        "                \"columns_analyzed\": selected_columns,\n",
        "                \"rows_analyzed\": len(extracted_data),\n",
        "                \"analysis_type\": analysis.get(\"analysis_type\", \"unknown\"),\n",
        "                \"success\": True\n",
        "            }\n",
        "            '''\n",
        "            return{\n",
        "                \"success\":True\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing query: {str(e)}\")\n",
        "            return {\n",
        "                \"answer\": f\"An error occurred while processing your query: {str(e)}\",\n",
        "                \"success\": False\n",
        "            }"
      ],
      "metadata": {
        "id": "K95z0uZElINi"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_logging():\n",
        "    \"\"\"Set up logging configuration.\"\"\"\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "        handlers=[\n",
        "            logging.FileHandler(\"csv_agent.log\"),\n",
        "            logging.StreamHandler()\n",
        "        ]\n",
        "    )\n",
        "\n",
        "setup_logging()"
      ],
      "metadata": {
        "id": "9gb5jqXeljnI"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Initialize the agent\n",
        "agent = CSVAgent('alpha_test.csv')"
      ],
      "metadata": {
        "id": "1yUJkufXRekW"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \" Please summurise texts about the Caldera Ginger Beer with the number_apperance 4\"\n",
        "\n",
        "result = agent.process_query(query)\n",
        "'''\n",
        "print(f\"\\nAnswer: {result['answer']}\\n\")\n",
        "if result['success']:\n",
        "    print(f\"Analysis type: {result['analysis_type']}\")\n",
        "    print(f\"Columns analyzed: {', '.join(result['columns_analyzed'])}\")\n",
        "    print(f\"Rows analyzed: {result['rows_analyzed']}\")\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "lPdMw_10RW5d",
        "outputId": "49e0e06d-5bab-4011-a560-d12a9d6ef73d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query_analyzer response:{\n",
            "    \"needed_columns\": [\"string_name\", \"number_appearance\", \"text\"],\n",
            "    \"analysis_type\": \"filtering\",\n",
            "    \"filter_conditions\": {\n",
            "        \"number_appearance\": 4,\n",
            "        \"string_name\": \"Caldera Ginger Beer\"\n",
            "    },\n",
            "    \"aggregation_function\": \"none\",\n",
            "    \"sort_by\": null,\n",
            "    \"sort_order\": null,\n",
            "    \"requires_text_search\": true,\n",
            "    \"search_term\": null\n",
            "}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nprint(f\"\\nAnswer: {result[\\'answer\\']}\\n\")\\nif result[\\'success\\']:\\n    print(f\"Analysis type: {result[\\'analysis_type\\']}\")\\n    print(f\"Columns analyzed: {\\', \\'.join(result[\\'columns_analyzed\\'])}\")\\n    print(f\"Rows analyzed: {result[\\'rows_analyzed\\']}\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models Connectivity Tests"
      ],
      "metadata": {
        "id": "FtcJ-rPoOFG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Anthropic client\n",
        "client = anthropic.Anthropic(api_key=api_key)\n",
        "\n",
        "try:\n",
        "    # Make a simple API request\n",
        "    response = client.messages.create(\n",
        "        model=\"claude-3-haiku-20240307\",\n",
        "        max_tokens=50,\n",
        "        messages=[{\"role\": \"user\", \"content\": \"Hello! Can you confirm you're working?\"}]\n",
        "    )\n",
        "\n",
        "    # Print the response\n",
        "    print(\"Claude's Response:\", response.content)\n",
        "\n",
        "except anthropic.APIStatusError as e:\n",
        "    print(f\"API returned an error: {e}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "e0NCjdsuOMKJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}