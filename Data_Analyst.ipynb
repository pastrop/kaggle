{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpvE21iZjeIlaZAiw5pFYA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pastrop/kaggle/blob/master/Data_Analyst.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install anthropic\n",
        "!pip install sentence-transformers\n",
        "!pip install pandas\n",
        "!pip install yake"
      ],
      "metadata": {
        "id": "rVMAhiK3kUfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import anthropic\n",
        "import os\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import logging\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "import logging\n",
        "import traceback"
      ],
      "metadata": {
        "id": "yr1qrObCkLnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfRnTARQf6bn"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('Agent_Anth')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "fZIXWRTSxf-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Service Classes\n",
        "class CSVReader:\n",
        "    \"\"\"Tool to read CSV files into pandas dataframes.\"\"\"\n",
        "\n",
        "    def read_csv(self, file_path: str) -> Tuple[pd.DataFrame, str]:\n",
        "        \"\"\"\n",
        "        Read a CSV file into a pandas dataframe.\n",
        "\n",
        "        Args:\n",
        "            file_path: Path to the CSV file\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (dataframe, message)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            df = pd.read_csv(file_path)\n",
        "            columns_info = \", \".join([f\"{col} ({df[col].dtype})\" for col in df.columns])\n",
        "            message = f\"Successfully loaded CSV with {len(df)} rows and {len(df.columns)} columns: {columns_info}\"\n",
        "            logger.info(message)\n",
        "            return df, message\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Error reading CSV file: {str(e)}\"\n",
        "            logger.error(error_msg)\n",
        "            return pd.DataFrame(), error_msg\n",
        "\n",
        "class QueryAnalyzer:\n",
        "    \"\"\"Tool to analyze queries and determine information needs.\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: Optional[str] = None):\n",
        "        \"\"\"\n",
        "        Initialize with Anthropic API key.\n",
        "\n",
        "        Args:\n",
        "            api_key: Anthropic API key (will use environment variable if None)\n",
        "        \"\"\"\n",
        "        self.api_key = api_key or os.environ.get(\"ANTHROPIC_API_KEY\")\n",
        "        if not self.api_key:\n",
        "            logger.warning(\"No Anthropic API key provided. Set ANTHROPIC_API_KEY environment variable.\")\n",
        "\n",
        "        self.client = anthropic.Anthropic(api_key=self.api_key)\n",
        "\n",
        "    def analyze_query(self, query: str, columns: List[str]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analyze the query to determine information needs.\n",
        "\n",
        "        Args:\n",
        "            query: The user's query\n",
        "            columns: List of available columns in the dataframe\n",
        "\n",
        "        Returns:\n",
        "            Dict with analysis results\n",
        "        \"\"\"\n",
        "        try:\n",
        "            prompt = f\"\"\"\n",
        "            <columns>\n",
        "            {', '.join(columns)}\n",
        "            </columns>\n",
        "\n",
        "            You are an AI assistant that analyzes queries about a dataset. Based on the user query, determine:\n",
        "            1. Which columns from the dataset are needed to answer the query\n",
        "            2. What type of analysis is required (filtering, aggregation, etc.)\n",
        "            3. Whether any specific values or conditions are mentioned\n",
        "\n",
        "            User query: {query}\n",
        "\n",
        "            Respond in JSON format like this:\n",
        "            {{\n",
        "                \"needed_columns\": [\"column1\", \"column2\"],\n",
        "                \"analysis_type\": \"one of: filtering, aggregation, sorting, comparison, general_info, semantic_search\",\n",
        "                \"filter_conditions\": {{\"column_name\": \"filter_value\"}},\n",
        "                \"aggregation_function\": \"one of: count, sum, average, min, max, none\",\n",
        "                \"sort_by\": \"column_name or null\",\n",
        "                \"sort_order\": \"ascending or descending or null\",\n",
        "                \"requires_text_search\": true/false,\n",
        "                \"search_term\": \"term to search for in text or null\"\n",
        "                \"query\": \"original query\"\n",
        "            }}\n",
        "\n",
        "            Make sure all column names exactly match the provided list. If a column is not mentioned or needed, don't include it.\n",
        "            \"\"\"\n",
        "\n",
        "            response = self.client.messages.create(\n",
        "                model=\"claude-3-haiku-20240307\",\n",
        "                max_tokens=1000,\n",
        "                messages=[\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            analysis_text = response.content[0].text\n",
        "            print(f'query_analyzer printout: response:{analysis_text}')\n",
        "\n",
        "            # Extract JSON from response\n",
        "            import json\n",
        "            import re\n",
        "\n",
        "            json_match = re.search(r'{[\\s\\S]+}', analysis_text)\n",
        "            if json_match:\n",
        "                analysis = json.loads(json_match.group(0))\n",
        "                logger.info(f\"Query analysis completed: {str(analysis)}\")\n",
        "                return analysis\n",
        "            else:\n",
        "                logger.error(\"Failed to extract JSON from Claude's response\")\n",
        "                return {\n",
        "                    \"needed_columns\": columns,\n",
        "                    \"analysis_type\": \"general_info\",\n",
        "                    \"requires_text_search\": False\n",
        "                }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error analyzing query: {str(e)}\")\n",
        "            return {\n",
        "                \"needed_columns\": columns,\n",
        "                \"analysis_type\": \"general_info\",\n",
        "                \"requires_text_search\": False\n",
        "            }\n",
        "\n",
        "class ColumnSelector:\n",
        "    \"\"\"Tool to determine which columns are needed for a query.\"\"\"\n",
        "\n",
        "    def select_columns(self, df: pd.DataFrame, analysis: Dict[str, Any]) -> List[str]:\n",
        "        \"\"\"\n",
        "        Select columns needed to answer the query.\n",
        "\n",
        "        Args:\n",
        "            df: The dataframe\n",
        "            analysis: Query analysis results\n",
        "\n",
        "        Returns:\n",
        "            List of column names to use\n",
        "        \"\"\"\n",
        "        all_columns = df.columns.tolist()\n",
        "\n",
        "        # Start with columns specified in the analysis\n",
        "        needed_columns = analysis.get(\"needed_columns\", [])\n",
        "\n",
        "        # Always include text column if text search is required\n",
        "        if analysis.get(\"requires_text_search\", False) and \"text\" in all_columns:\n",
        "            if \"text\" not in needed_columns:\n",
        "                needed_columns.append(\"text\")\n",
        "\n",
        "        # Add filter columns if not already included\n",
        "        filter_conditions = analysis.get(\"filter_conditions\", {})\n",
        "        for col in filter_conditions.keys():\n",
        "            if col in all_columns and col not in needed_columns:\n",
        "                needed_columns.append(col)\n",
        "\n",
        "        # Add sort column if not already included\n",
        "        sort_by = analysis.get(\"sort_by\")\n",
        "        if sort_by and sort_by in all_columns and sort_by not in needed_columns:\n",
        "            needed_columns.append(sort_by)\n",
        "\n",
        "        # If no columns were determined, return all columns\n",
        "        if not needed_columns:\n",
        "            logger.warning(\"No specific columns determined, using all columns\")\n",
        "            needed_columns = all_columns\n",
        "\n",
        "        logger.info(f\"Selected columns: {', '.join(needed_columns)}\")\n",
        "        print(f\"Column Selector Printout: Selected columns: {', '.join(needed_columns)}\")\n",
        "        return needed_columns\n",
        "\n",
        "class DataExtractor:\n",
        "    \"\"\"Tool to extract relevant data from the dataframe.\"\"\"\n",
        "\n",
        "    def extract_data(self, df: pd.DataFrame, analysis: Dict[str, Any], selected_columns: List[str]) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Extract relevant data based on query analysis.\n",
        "\n",
        "        Args:\n",
        "            df: The dataframe\n",
        "            analysis: Query analysis results\n",
        "            selected_columns: Columns to include\n",
        "\n",
        "        Returns:\n",
        "            Filtered dataframe\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Start with selected columns\n",
        "            result_df = df[selected_columns].copy()\n",
        "\n",
        "            # Apply filtering if specified\n",
        "            filter_conditions = analysis.get(\"filter_conditions\", {})\n",
        "            if filter_conditions and analysis.get(\"analysis_type\") in [\"filtering\", \"comparison\"]:\n",
        "                for col, value in filter_conditions.items():\n",
        "                    if col in df.columns:\n",
        "                        # Handle different filter types\n",
        "                        if isinstance(value, dict):\n",
        "                            # Range or comparison filter\n",
        "                            if \"min\" in value and \"max\" in value:\n",
        "                                result_df = result_df[(result_df[col] >= value[\"min\"]) &\n",
        "                                                     (result_df[col] <= value[\"max\"])]\n",
        "                            elif \"min\" in value:\n",
        "                                result_df = result_df[result_df[col] >= value[\"min\"]]\n",
        "                            elif \"max\" in value:\n",
        "                                result_df = result_df[result_df[col] <= value[\"max\"]]\n",
        "                            elif \"not_equal\" in value:\n",
        "                                result_df = result_df[result_df[col] != value[\"not_equal\"]]\n",
        "                        elif isinstance(value, list):\n",
        "                            # List of values\n",
        "                            result_df = result_df[result_df[col].isin(value)]\n",
        "                        else:\n",
        "                            # Simple equality\n",
        "                            result_df = result_df[result_df[col] == value]\n",
        "\n",
        "            # Apply sorting if specified\n",
        "            sort_by = analysis.get(\"sort_by\")\n",
        "            sort_order = analysis.get(\"sort_order\", \"ascending\")\n",
        "            if sort_by and sort_by in result_df.columns:\n",
        "                ascending = sort_order.lower() != \"descending\"\n",
        "                result_df = result_df.sort_values(by=sort_by, ascending=ascending)\n",
        "\n",
        "            # Apply aggregation if specified\n",
        "            agg_function = analysis.get(\"aggregation_function\")\n",
        "            if agg_function and agg_function != \"none\" and analysis.get(\"analysis_type\") == \"aggregation\":\n",
        "                # Determine which column to aggregate\n",
        "                agg_col = None\n",
        "                for col in result_df.columns:\n",
        "                    if col != \"text\" and pd.api.types.is_numeric_dtype(result_df[col]):\n",
        "                        agg_col = col\n",
        "                        break\n",
        "\n",
        "                if agg_col:\n",
        "                    if agg_function == \"count\":\n",
        "                        result_df = result_df.groupby(selected_columns[0])[agg_col].count().reset_index()\n",
        "                    elif agg_function == \"sum\":\n",
        "                        result_df = result_df.groupby(selected_columns[0])[agg_col].sum().reset_index()\n",
        "                    elif agg_function == \"average\":\n",
        "                        result_df = result_df.groupby(selected_columns[0])[agg_col].mean().reset_index()\n",
        "                    elif agg_function == \"min\":\n",
        "                        result_df = result_df.groupby(selected_columns[0])[agg_col].min().reset_index()\n",
        "                    elif agg_function == \"max\":\n",
        "                        result_df = result_df.groupby(selected_columns[0])[agg_col].max().reset_index()\n",
        "\n",
        "            logger.info(f\"Extracted {len(result_df)} rows of data\")\n",
        "            print(f\"Data Extractor Printout: {result_df}\")\n",
        "            return result_df\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error extracting data: {str(e)}\")\n",
        "            # Return original data with selected columns\n",
        "            return df[selected_columns].copy()\n",
        "\n",
        "class TextEmbedder:\n",
        "    \"\"\"Tool to generate and search text embeddings.\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
        "        \"\"\"\n",
        "        Initialize with embedding model.\n",
        "\n",
        "        Args:\n",
        "            model_name: Name of the embedding model\n",
        "        \"\"\"\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        logger.info(f\"Initialized embedding model: {model_name}\")\n",
        "\n",
        "    def search_similar_texts(self, df: pd.DataFrame, query: str, text_column: str = \"text\",\n",
        "                            top_k: int = 5) -> pd.DataFrame:\n",
        "       \"\"\"\n",
        "        Find texts most similar to the query.\n",
        "\n",
        "        Args:\n",
        "            df: Dataframe with text column\n",
        "            query: Search query\n",
        "            text_column: Column containing text\n",
        "            top_k: Number of results to return\n",
        "\n",
        "        Returns:\n",
        "            Dataframe with most similar texts\n",
        "        \"\"\"\n",
        "        print('I am inside searching simular texts')\n",
        "        if text_column not in df.columns:\n",
        "            logger.error(f\"Text column '{text_column}' not found in dataframe\")\n",
        "            return df\n",
        "\n",
        "        try:\n",
        "            # Generate embeddings\n",
        "            print('trying to generate embeddings')\n",
        "            texts = df[text_column].fillna(\"\").tolist()\n",
        "            print(f'text_embeddings printout: {texts}')\n",
        "            text_embeddings = self.model.encode(texts)\n",
        "            print('got the text embeddings!')\n",
        "            print(f'input query printout: {query} of type {type(query)}')\n",
        "            query_embedding = self.model.encode(query)\n",
        "            print('got the query embeddings!')\n",
        "\n",
        "            # Calculate similarities\n",
        "            similarities = cosine_similarity(\n",
        "                query_embedding.reshape(1, -1),\n",
        "                text_embeddings\n",
        "            )[0]\n",
        "\n",
        "            # Add similarity scores to dataframe\n",
        "            result_df = df.copy()\n",
        "            result_df[\"similarity_score\"] = similarities\n",
        "\n",
        "            # Sort by similarity and take top_k\n",
        "            result_df = result_df.sort_values(\"similarity_score\", ascending=False).head(top_k)\n",
        "\n",
        "            logger.info(f\"Found {len(result_df)} similar texts\")\n",
        "            print(f\"search_similar_texts printout: {result_df}\")\n",
        "            return result_df\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error searching similar texts: {str(e)}\")\n",
        "            print(traceback.format_exc())  # Print full traceback\n",
        "            return df\n",
        "\n",
        "class AnswerGenerator:\n",
        "    \"\"\"Tool to generate answers using Claude 3.7.\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: Optional[str] = None):\n",
        "        \"\"\"\n",
        "        Initialize with Anthropic API key.\n",
        "\n",
        "        Args:\n",
        "            api_key: Anthropic API key (will use environment variable if None)\n",
        "        \"\"\"\n",
        "        self.api_key = api_key or os.environ.get(\"ANTHROPIC_API_KEY\")\n",
        "        if not self.api_key:\n",
        "            logger.warning(\"No Anthropic API key provided. Set ANTHROPIC_API_KEY environment variable.\")\n",
        "\n",
        "        self.client = anthropic.Anthropic(api_key=self.api_key)\n",
        "\n",
        "    def generate_answer(self, query: str, data_df: pd.DataFrame, analysis: Dict[str, Any]) -> str:\n",
        "        \"\"\"\n",
        "        Generate an answer using Claude 3.7.\n",
        "\n",
        "        Args:\n",
        "            query: User query\n",
        "            data_df: Dataframe with relevant data\n",
        "            analysis: Query analysis results\n",
        "\n",
        "        Returns:\n",
        "            Generated answer\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Convert dataframe to string representation\n",
        "            data_str = data_df.to_string(index=False) if not data_df.empty else \"No data found\"\n",
        "\n",
        "            # Create prompt for Claude\n",
        "            prompt = f\"\"\"\n",
        "            <data>\n",
        "            {data_str}\n",
        "            </data>\n",
        "\n",
        "            <query_analysis>\n",
        "            {str(analysis)}\n",
        "            </query_analysis>\n",
        "\n",
        "            User query: {query}\n",
        "\n",
        "            Based on the provided data and analysis of the query, please provide a comprehensive answer to the user's question.\n",
        "            Include specific details from the data where appropriate. If the data doesn't contain information needed to answer the query,\n",
        "            state that clearly.\n",
        "\n",
        "            Answer the query directly and concisely. If appropriate, include any relevant statistics from the data.\n",
        "            \"\"\"\n",
        "\n",
        "            response = self.client.messages.create(\n",
        "                model=\"claude-3-haiku-20240307\",\n",
        "                max_tokens=2000,\n",
        "                messages=[\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            answer = response.content[0].text\n",
        "            logger.info(f\"Generated answer of length {len(answer)}\")\n",
        "            return answer\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generating answer: {str(e)}\")\n",
        "            return f\"I encountered an error while generating the answer: {str(e)}\""
      ],
      "metadata": {
        "id": "VPfobfxxj4x5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#logger = logging.getLogger(__name__)\n",
        "\n",
        "class CSVAgent:\n",
        "    \"\"\"Agent that analyzes CSV data to answer queries.\"\"\"\n",
        "\n",
        "    def __init__(self, csv_path, api_key=api_key):\n",
        "        \"\"\"\n",
        "        Initialize the agent.\n",
        "\n",
        "        Args:\n",
        "            csv_path: Path to the CSV file\n",
        "            api_key: Anthropic API key (optional)\n",
        "        \"\"\"\n",
        "        self.csv_path = csv_path\n",
        "        self.api_key = api_key\n",
        "\n",
        "        #Refactoring Prep:\n",
        "        #self.agent_client = anthropic.Anthropic(api_key=self.api_key)\n",
        "        #self.embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "        # Initialize tools\n",
        "        self.csv_reader = CSVReader()\n",
        "        self.query_analyzer = QueryAnalyzer(api_key=self.api_key)\n",
        "        self.column_selector = ColumnSelector()\n",
        "        self.data_extractor = DataExtractor()\n",
        "        self.text_embedder = TextEmbedder()\n",
        "        self.answer_generator = AnswerGenerator(api_key=self.api_key)\n",
        "\n",
        "        # Load CSV data\n",
        "        self.df, load_message = self.csv_reader.read_csv(csv_path)\n",
        "        logger.info(load_message)\n",
        "\n",
        "        # Store column information\n",
        "        self.columns = list(self.df.columns) if not self.df.empty else []\n",
        "\n",
        "    def process_query(self, query):\n",
        "        \"\"\"\n",
        "        Process a user query and generate an answer.\n",
        "\n",
        "        Args:\n",
        "            query: User query string\n",
        "\n",
        "        Returns:\n",
        "            dict: Response containing answer and processing details\n",
        "        \"\"\"\n",
        "        logger.info(f\"Processing query: {query}\")\n",
        "\n",
        "        if self.df.empty:\n",
        "            return {\n",
        "                \"answer\": \"Unable to analyze the CSV file. Please check the file path and format.\",\n",
        "                \"success\": False\n",
        "            }\n",
        "\n",
        "        try:\n",
        "            # Step 1: Analyze the query\n",
        "            analysis = self.query_analyzer.analyze_query(query, self.columns)\n",
        "\n",
        "            # Step 2: Select relevant columns\n",
        "            selected_columns = self.column_selector.select_columns(self.df, analysis)\n",
        "\n",
        "            # Step 3: Extract relevant data\n",
        "            extracted_data = self.data_extractor.extract_data(self.df, analysis, selected_columns)\n",
        "\n",
        "            # Step 4: Apply text search if needed\n",
        "            if analysis.get(\"requires_text_search\", False) and \"text\" in self.columns:\n",
        "                if analysis.get(\"search_term\") != None:\n",
        "                  search_term = analysis.get(\"search_term\")\n",
        "                else:\n",
        "                  search_term = analysis.get(\"query\")\n",
        "                extracted_data = self.text_embedder.search_similar_texts(\n",
        "                    extracted_data,\n",
        "                    search_term,\n",
        "                    text_column=\"text\",\n",
        "                    top_k=10\n",
        "                )\n",
        "\n",
        "            # Step 5: Generate answer\n",
        "            answer = self.answer_generator.generate_answer(query, extracted_data, analysis)\n",
        "\n",
        "            return {\n",
        "                \"answer\": answer,\n",
        "                \"columns_analyzed\": selected_columns,\n",
        "                \"rows_analyzed\": len(extracted_data),\n",
        "                \"analysis_type\": analysis.get(\"analysis_type\", \"unknown\"),\n",
        "                \"success\": True\n",
        "            }\n",
        "            '''\n",
        "            return{\n",
        "                \"success\":True\n",
        "            }\n",
        "            '''\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing query: {str(e)}\")\n",
        "            return {\n",
        "                \"answer\": f\"An error occurred while processing your query: {str(e)}\",\n",
        "                \"success\": False\n",
        "            }"
      ],
      "metadata": {
        "id": "K95z0uZElINi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_logging():\n",
        "    \"\"\"Set up logging configuration.\"\"\"\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "        handlers=[\n",
        "            logging.FileHandler(\"csv_agent.log\"),\n",
        "            logging.StreamHandler()\n",
        "        ]\n",
        "    )\n",
        "\n",
        "setup_logging()"
      ],
      "metadata": {
        "id": "9gb5jqXeljnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Initialize the agent\n",
        "agent = CSVAgent('alpha_test.csv')"
      ],
      "metadata": {
        "id": "1yUJkufXRekW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "queries examples:\n",
        "\n",
        "\" Please summarise texts about the Caldera Ginger Beer with the number_apperance 4\""
      ],
      "metadata": {
        "id": "K4fAjxz9JXjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \" Please find all beers that have the number_apperance 2.5 and return their names and the summary of texts for these beers\"\n",
        "\n",
        "result = agent.process_query(query)\n",
        "\n",
        "print(f\"\\nAnswer: {result['answer']}\\n\")\n",
        "if result['success']:\n",
        "    print(f\"Analysis type: {result['analysis_type']}\")\n",
        "    print(f\"Columns analyzed: {', '.join(result['columns_analyzed'])}\")\n",
        "    print(f\"Rows analyzed: {result['rows_analyzed']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPdMw_10RW5d",
        "outputId": "752a553c-8a16-4b73-c0ed-dfdd3a3cead1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query_analyzer printout: response:{\n",
            "    \"needed_columns\": [\"string_name\", \"text\"],\n",
            "    \"analysis_type\": \"filtering\",\n",
            "    \"filter_conditions\": {\"number_appearance\": 2.5},\n",
            "    \"aggregation_function\": \"none\",\n",
            "    \"sort_by\": null,\n",
            "    \"sort_order\": null,\n",
            "    \"requires_text_search\": false,\n",
            "    \"search_term\": null,\n",
            "    \"query\": \"Please find all beers that have the number_apperance 2.5 and return their names and the summary of texts for these beers\"\n",
            "}\n",
            "Column Selector Printout: Selected columns: string_name, text, number_appearance\n",
            "Data Extractor Printout:                    string_name  \\\n",
            "0                 Sausa Weizen   \n",
            "14       Caldera Oatmeal Stout   \n",
            "15       Caldera Oatmeal Stout   \n",
            "76            Caldera Pale Ale   \n",
            "190           Caldera Pale Ale   \n",
            "200           Caldera Pale Ale   \n",
            "257           Vas Deferens Ale   \n",
            "272  Old Growth Imperial Stout   \n",
            "\n",
            "                                                  text  number_appearance  \n",
            "0    A lot of foam. But a lot.\\tIn the smell some b...                2.5  \n",
            "14   Brown in color, somewhere between a porter and...                2.5  \n",
            "15   Caldera presents yet another circumstance wher...                2.5  \n",
            "76   Pours a crisp clear pale gold color with a sma...                2.5  \n",
            "190  Golden amber with a big foamy head that just f...                2.5  \n",
            "200  Poured into 12oz straight glass. Poured a clou...                2.5  \n",
            "257  Chilled bottle into a glass. A generous gift f...                2.5  \n",
            "272  Got this one from the Nashvillian, cheers John...                2.5  \n",
            "\n",
            "Answer: Based on the provided data and query analysis, the following beers have a number_appearance of 2.5:\n",
            "\n",
            "1. Sausa Weizen\n",
            "2. Caldera Oatmeal Stout\n",
            "3. Caldera Pale Ale (multiple entries)\n",
            "4. Vas Deferens Ale\n",
            "5. Old Growth Imperial Stout\n",
            "\n",
            "The summary of the text for these beers is as follows:\n",
            "\n",
            "Sausa Weizen:\n",
            "\"A lot of foam. But a lot.\\tIn the smell some banana, and then lactic and tart. Not a good start.\\tQuite dark orange in color, with a lively carbonation (now visible, under the foam).\\tAgain tending to lactic sourness.\\tSame for the taste. With some yeast and banana.\"\n",
            "\n",
            "Caldera Oatmeal Stout:\n",
            "\"Brown in color, somewhere between a porter and a brown ale. Lacking in aroma, but no off stuff. \\t\\tSame with the taste, lacking flavor, complexity, just went with smoothness. No off flavors though, so I can't say this is bad, just unadventurous, especially for Caldera, whom I think is generally underrated. You really have to search to pull anything out of this in terms of the usual chocolate/coffee flavors, really, the only thing I can tell is that the oats did their job, because this is smooth and unoffensive.\\t\\tOther than that, extremely pedestrian.\"\n",
            "\n",
            "Caldera Pale Ale (multiple entries):\n",
            "\"Pours a crisp clear pale gold color with a small head that dissipates rather quickly.\\t\\tHas a bitter hoppy pine and citrus smell. Very pungent for a APA.\\t\\tBitter bite hits right away and fades to a smooth caramel malty biscuity flavor. Not very complex, but good tastes none the less.\\t\\tA very light smooth beer. Very sessionable could def hammer a few of these back.\"\n",
            "\"Golden amber with a big foamy head that just floats on top. Crisp pale malt and floral hop aromas. Flavors of light floral hops with a good malty reflection that combine to make a honey-like finish with a light hop linger. Medium carbonation with a non descript mouthfeel.\\t\\tIt might not look like much but it tastes great although there's also not much to talk about in the mouthfeel. I really liked the honey I tasted in the finish.\"\n",
            "\"Poured into 12oz straight glass. Poured a cloudy yellow, almost the color of pineapple juice, with barely a thin cap of white head that had no retention and minimal lacing.\\t\\tThe aroma was mostly malt forward, a pleasant change from Pales that are basically \"IPA light\", with just enough hops to keep the scent balanced. The flavor was balanced as well, although I thought a little on the bland side. None of the flavors really jumped out at me.\\t\\tThe body was typical for the style but lost some points for being overly sticky. Drinkability was OK, I'll drink it again, but this beer is not on the level of Mirror Pond or Manny's.\\t\\tOverall, a decent enough brew, but nothing special.\"\n",
            "\n",
            "Vas Deferens Ale:\n",
            "\"Chilled bottle into a glass. A generous gift from ramnuts. Thanks, Frank! \\t\\tShared with alfrantzell and chswimmer. \\t\\tA: Pours a clear maroon body with a light tan head. The bubbles fill the tulip but quickly collapse into nothing. \\t\\tS: Not much aroma at all. Some nuts and orange, perhaps. \\t\\tT: This beer initially tasted like sushi. Ted narrowed it down to the seaweed wrap used in rolls, but I am convinced I picked up the soy / cardboard signs of oxidation, as well. As it warmed, those flavors receded a bit, producing flavors of orange, nuts, coffee, and some kind of spice. Somewhat disjointed. \\t\\tM: Oddly thin, with a watery flavor. \\t\\tD: Not enjoyable...\"\n",
            "\n",
            "Old Growth Imperial Stout:\n",
            "\"Got this one from the Nashvillian, cheers John!\\t\\tPours ebony with less than a pinky of mocha colored head.Near zero head retention & lacing\\t\\tS: Chocolate, dark fruit, vanilla, & a touch of Bourbon once warm\\t\\tT: Follows the nose, plus some herbal & woody hop notes, charred grain & dryness up front. More charred grain & Baker's chocolate as this warms. Finishes with a nice chocolate sweetness, herbal hops, a bit vanilla & more charred grain\\t\\tMF: Chewy, oily, subtle carbonation\\t\\tA nice Imp Stout, didn't quite come together for perfection, but a solid offering\"\n",
            "\n",
            "Analysis type: filtering\n",
            "Columns analyzed: string_name, text, number_appearance\n",
            "Rows analyzed: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Refactoring"
      ],
      "metadata": {
        "id": "uCBo5T8lynvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Service Classes\n",
        "class CSVReader:\n",
        "    \"\"\"Tool to read CSV files into pandas dataframes.\"\"\"\n",
        "\n",
        "    def read_csv(self, file_path: str) -> Tuple[pd.DataFrame, str]:\n",
        "        \"\"\"\n",
        "        Read a CSV file into a pandas dataframe.\n",
        "\n",
        "        Args:\n",
        "            file_path: Path to the CSV file\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (dataframe, message)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            df = pd.read_csv(file_path)\n",
        "            columns_info = \", \".join([f\"{col} ({df[col].dtype})\" for col in df.columns])\n",
        "            message = f\"Successfully loaded CSV with {len(df)} rows and {len(df.columns)} columns: {columns_info}\"\n",
        "            logger.info(message)\n",
        "            return df, message\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Error reading CSV file: {str(e)}\"\n",
        "            logger.error(error_msg)\n",
        "            return pd.DataFrame(), error_msg\n",
        "\n",
        "class Workflow:\n",
        "    \"\"\"\n",
        "    Tools to analyze queries and determine information needs;\n",
        "          to determine which columns are needed for a query;\n",
        "          to extract relevant data from the dataframe;\n",
        "          to generate and search text embeddings;\n",
        "          to generate answers using Claude 3.7.\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: Optional[str] = None, model_name: str = \"all-MiniLM-L6-v2\"):\n",
        "        \"\"\"\n",
        "        Initialize with Anthropic API key.\n",
        "\n",
        "        Args:\n",
        "            api_key: Anthropic API key (will use environment variable if None)\n",
        "\n",
        "        Initialize the embedding model.\n",
        "\n",
        "        Args:\n",
        "            model_name: Name of the embedding model\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        self.api_key = api_key or os.environ.get(\"ANTHROPIC_API_KEY\")\n",
        "\n",
        "        #if not self.api_key:\n",
        "            #logger.warning(\"No Anthropic API key provided. Set ANTHROPIC_API_KEY environment variable.\")\n",
        "\n",
        "        self.client = anthropic.Anthropic(api_key=self.api_key)\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "\n",
        "\n",
        "    def analyze_query(query: str, columns: List[str], api_key = api_key) -> Dict[str, Any]:\n",
        "            \"\"\"\n",
        "            Analyze the query to determine information needs.\n",
        "\n",
        "            Args:\n",
        "                query: The user's query\n",
        "                columns: List of available columns in the dataframe\n",
        "\n",
        "            Returns:\n",
        "                Dict with analysis results\n",
        "            \"\"\"\n",
        "            try:\n",
        "                prompt = f\"\"\"\n",
        "                <columns>\n",
        "                {', '.join(columns)}\n",
        "                </columns>\n",
        "\n",
        "                You are an AI assistant that analyzes queries about a dataset. Based on the user query, determine:\n",
        "                1. Which columns from the dataset are needed to answer the query\n",
        "                2. What type of analysis is required (filtering, aggregation, etc.)\n",
        "                3. Whether any specific values or conditions are mentioned\n",
        "\n",
        "                User query: {query}\n",
        "\n",
        "                Respond in JSON format like this:\n",
        "                {{\n",
        "                    \"needed_columns\": [\"column1\", \"column2\"],\n",
        "                    \"analysis_type\": \"one of: filtering, aggregation, sorting, comparison, general_info, semantic_search\",\n",
        "                    \"filter_conditions\": {{\"column_name\": \"filter_value\"}},\n",
        "                    \"aggregation_function\": \"one of: count, sum, average, min, max, none\",\n",
        "                    \"sort_by\": \"column_name or null\",\n",
        "                    \"sort_order\": \"ascending or descending or null\",\n",
        "                    \"requires_text_search\": true/false,\n",
        "                    \"search_term\": \"term to search for in text or null\"\n",
        "                    \"query\": \"original query\"\n",
        "                }}\n",
        "\n",
        "                Make sure all column names exactly match the provided list. If a column is not mentioned or needed, don't include it.\n",
        "                \"\"\"\n",
        "                client = anthropic.Anthropic(api_key)\n",
        "                response = self.client.messages.create(\n",
        "                    model=\"claude-3-haiku-20240307\",\n",
        "                    max_tokens=1000,\n",
        "                    messages=[\n",
        "                        {\"role\": \"user\", \"content\": prompt}\n",
        "                    ]\n",
        "                )\n",
        "\n",
        "                analysis_text = response.content[0].text\n",
        "                print(f'query_analyzer printout: response:{analysis_text}')\n",
        "\n",
        "                # Extract JSON from response\n",
        "                import json\n",
        "                import re\n",
        "\n",
        "                json_match = re.search(r'{[\\s\\S]+}', analysis_text)\n",
        "                if json_match:\n",
        "                    analysis = json.loads(json_match.group(0))\n",
        "                    logger.info(f\"Query analysis completed: {str(analysis)}\")\n",
        "                    return analysis\n",
        "                else:\n",
        "                    logger.error(\"Failed to extract JSON from Claude's response\")\n",
        "                    return {\n",
        "                        \"needed_columns\": columns,\n",
        "                        \"analysis_type\": \"general_info\",\n",
        "                        \"requires_text_search\": False\n",
        "                    }\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error analyzing query: {str(e)}\")\n",
        "                return {\n",
        "                    \"needed_columns\": columns,\n",
        "                    \"analysis_type\": \"general_info\",\n",
        "                    \"requires_text_search\": False\n",
        "                }\n",
        "\n",
        "    def select_columns(df: pd.DataFrame, analysis: Dict[str, Any]) -> List[str]:\n",
        "            \"\"\"\n",
        "            Select columns needed to answer the query.\n",
        "\n",
        "            Args:\n",
        "                df: The dataframe\n",
        "                analysis: Query analysis results\n",
        "\n",
        "            Returns:\n",
        "                List of column names to use\n",
        "            \"\"\"\n",
        "            all_columns = df.columns.tolist()\n",
        "\n",
        "            # Start with columns specified in the analysis\n",
        "            needed_columns = analysis.get(\"needed_columns\", [])\n",
        "\n",
        "            # Always include text column if text search is required\n",
        "            if analysis.get(\"requires_text_search\", False) and \"text\" in all_columns:\n",
        "                if \"text\" not in needed_columns:\n",
        "                    needed_columns.append(\"text\")\n",
        "\n",
        "            # Add filter columns if not already included\n",
        "            filter_conditions = analysis.get(\"filter_conditions\", {})\n",
        "            for col in filter_conditions.keys():\n",
        "                if col in all_columns and col not in needed_columns:\n",
        "                    needed_columns.append(col)\n",
        "\n",
        "            # Add sort column if not already included\n",
        "            sort_by = analysis.get(\"sort_by\")\n",
        "            if sort_by and sort_by in all_columns and sort_by not in needed_columns:\n",
        "                needed_columns.append(sort_by)\n",
        "\n",
        "            # If no columns were determined, return all columns\n",
        "            if not needed_columns:\n",
        "                logger.warning(\"No specific columns determined, using all columns\")\n",
        "                needed_columns = all_columns\n",
        "\n",
        "            logger.info(f\"Selected columns: {', '.join(needed_columns)}\")\n",
        "            print(f\"Column Selector Printout: Selected columns: {', '.join(needed_columns)}\")\n",
        "            return needed_columns\n",
        "\n",
        "    def extract_data(df: pd.DataFrame, analysis: Dict[str, Any], selected_columns: List[str]) -> pd.DataFrame:\n",
        "            \"\"\"\n",
        "            Extract relevant data based on query analysis.\n",
        "\n",
        "            Args:\n",
        "                df: The dataframe\n",
        "                analysis: Query analysis results\n",
        "                selected_columns: Columns to include\n",
        "\n",
        "            Returns:\n",
        "                Filtered dataframe\n",
        "            \"\"\"\n",
        "            try:\n",
        "                # Start with selected columns\n",
        "                result_df = df[selected_columns].copy()\n",
        "\n",
        "                # Apply filtering if specified\n",
        "                filter_conditions = analysis.get(\"filter_conditions\", {})\n",
        "                if filter_conditions and analysis.get(\"analysis_type\") in [\"filtering\", \"comparison\"]:\n",
        "                    for col, value in filter_conditions.items():\n",
        "                        if col in df.columns:\n",
        "                            # Handle different filter types\n",
        "                            if isinstance(value, dict):\n",
        "                                # Range or comparison filter\n",
        "                                if \"min\" in value and \"max\" in value:\n",
        "                                    result_df = result_df[(result_df[col] >= value[\"min\"]) &\n",
        "                                                        (result_df[col] <= value[\"max\"])]\n",
        "                                elif \"min\" in value:\n",
        "                                    result_df = result_df[result_df[col] >= value[\"min\"]]\n",
        "                                elif \"max\" in value:\n",
        "                                    result_df = result_df[result_df[col] <= value[\"max\"]]\n",
        "                                elif \"not_equal\" in value:\n",
        "                                    result_df = result_df[result_df[col] != value[\"not_equal\"]]\n",
        "                            elif isinstance(value, list):\n",
        "                                # List of values\n",
        "                                result_df = result_df[result_df[col].isin(value)]\n",
        "                            else:\n",
        "                                # Simple equality\n",
        "                                result_df = result_df[result_df[col] == value]\n",
        "\n",
        "                # Apply sorting if specified\n",
        "                sort_by = analysis.get(\"sort_by\")\n",
        "                sort_order = analysis.get(\"sort_order\", \"ascending\")\n",
        "                if sort_by and sort_by in result_df.columns:\n",
        "                    ascending = sort_order.lower() != \"descending\"\n",
        "                    result_df = result_df.sort_values(by=sort_by, ascending=ascending)\n",
        "\n",
        "                # Apply aggregation if specified\n",
        "                agg_function = analysis.get(\"aggregation_function\")\n",
        "                if agg_function and agg_function != \"none\" and analysis.get(\"analysis_type\") == \"aggregation\":\n",
        "                    # Determine which column to aggregate\n",
        "                    agg_col = None\n",
        "                    for col in result_df.columns:\n",
        "                        if col != \"text\" and pd.api.types.is_numeric_dtype(result_df[col]):\n",
        "                            agg_col = col\n",
        "                            break\n",
        "\n",
        "                    if agg_col:\n",
        "                        if agg_function == \"count\":\n",
        "                            result_df = result_df.groupby(selected_columns[0])[agg_col].count().reset_index()\n",
        "                        elif agg_function == \"sum\":\n",
        "                            result_df = result_df.groupby(selected_columns[0])[agg_col].sum().reset_index()\n",
        "                        elif agg_function == \"average\":\n",
        "                            result_df = result_df.groupby(selected_columns[0])[agg_col].mean().reset_index()\n",
        "                        elif agg_function == \"min\":\n",
        "                            result_df = result_df.groupby(selected_columns[0])[agg_col].min().reset_index()\n",
        "                        elif agg_function == \"max\":\n",
        "                            result_df = result_df.groupby(selected_columns[0])[agg_col].max().reset_index()\n",
        "\n",
        "                logger.info(f\"Extracted {len(result_df)} rows of data\")\n",
        "                print(f\"Data Extractor Printout: {result_df}\")\n",
        "                return result_df\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error extracting data: {str(e)}\")\n",
        "                # Return original data with selected columns\n",
        "                return df[selected_columns].copy()\n",
        "\n",
        "\n",
        "\n",
        "    def search_similar_texts(df: pd.DataFrame, query: str, model: SentenceTransformer,\n",
        "                            text_column: str = \"text\", top_k: int = 5) -> pd.DataFrame:\n",
        "          \"\"\"\n",
        "            Find texts most similar to the query.\n",
        "\n",
        "            Args:\n",
        "                df: Dataframe with text column\n",
        "                query: Search query\n",
        "                text_column: Column containing text\n",
        "                top_k: Number of results to return\n",
        "\n",
        "            Returns:\n",
        "                Dataframe with most similar texts\n",
        "            \"\"\"\n",
        "            print('I am inside searching simular texts')\n",
        "            if text_column not in df.columns:\n",
        "                logger.error(f\"Text column '{text_column}' not found in dataframe\")\n",
        "                return df\n",
        "\n",
        "            try:\n",
        "                # Generate embeddings\n",
        "                print('trying to generate embeddings')\n",
        "                texts = df[text_column].fillna(\"\").tolist()\n",
        "                print(f'text_embeddings printout: {texts}')\n",
        "                text_embeddings = self.model.encode(texts)\n",
        "                print('got the text embeddings!')\n",
        "                print(f'input query printout: {query} of type {type(query)}')\n",
        "                query_embedding = self.model.encode(query)\n",
        "                print('got the query embeddings!')\n",
        "\n",
        "                # Calculate similarities\n",
        "                similarities = cosine_similarity(\n",
        "                    query_embedding.reshape(1, -1),\n",
        "                    text_embeddings\n",
        "                )[0]\n",
        "\n",
        "                # Add similarity scores to dataframe\n",
        "                result_df = df.copy()\n",
        "                result_df[\"similarity_score\"] = similarities\n",
        "\n",
        "                # Sort by similarity and take top_k\n",
        "                result_df = result_df.sort_values(\"similarity_score\", ascending=False).head(top_k)\n",
        "\n",
        "                logger.info(f\"Found {len(result_df)} similar texts\")\n",
        "                print(f\"search_similar_texts printout: {result_df}\")\n",
        "                return result_df\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error searching similar texts: {str(e)}\")\n",
        "                print(traceback.format_exc())  # Print full traceback\n",
        "                return df\n",
        "\n",
        "    def generate_answer(query: str, data_df: pd.DataFrame, analysis: Dict[str, Any],\n",
        "                      client: anthropic.Anthropic) -> str:\n",
        "            \"\"\"\n",
        "            Generate an answer using Claude 3.7.\n",
        "\n",
        "            Args:\n",
        "                query: User query\n",
        "                data_df: Dataframe with relevant data\n",
        "                analysis: Query analysis results\n",
        "\n",
        "            Returns:\n",
        "                Generated answer\n",
        "            \"\"\"\n",
        "            try:\n",
        "                # Convert dataframe to string representation\n",
        "                data_str = data_df.to_string(index=False) if not data_df.empty else \"No data found\"\n",
        "\n",
        "                # Create prompt for Claude\n",
        "                prompt = f\"\"\"\n",
        "                <data>\n",
        "                {data_str}\n",
        "                </data>\n",
        "\n",
        "                <query_analysis>\n",
        "                {str(analysis)}\n",
        "                </query_analysis>\n",
        "\n",
        "                User query: {query}\n",
        "\n",
        "                Based on the provided data and analysis of the query, please provide a comprehensive answer to the user's question.\n",
        "                Include specific details from the data where appropriate. If the data doesn't contain information needed to answer the query,\n",
        "                state that clearly.\n",
        "\n",
        "                Answer the query directly and concisely. If appropriate, include any relevant statistics from the data.\n",
        "                \"\"\"\n",
        "\n",
        "                response = self.client.messages.create(\n",
        "                    model=\"claude-3-haiku-20240307\",\n",
        "                    max_tokens=2000,\n",
        "                    messages=[\n",
        "                        {\"role\": \"user\", \"content\": prompt}\n",
        "                    ]\n",
        "                )\n",
        "\n",
        "                answer = response.content[0].text\n",
        "                logger.info(f\"Generated answer of length {len(answer)}\")\n",
        "                return answer\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error generating answer: {str(e)}\")\n",
        "                return f\"I encountered an error while generating the answer: {str(e)}\""
      ],
      "metadata": {
        "id": "Yj59PNl219vJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Potential new tools:\n",
        "# 1. Concept Extraction\n",
        "import yake\n",
        "# YAKE Config\n",
        "kw_extractor = yake.KeywordExtractor()\n",
        "language = 'en'\n",
        "max_ngram_size = 2\n",
        "deduplication_threshold = 0.9\n",
        "numOfKeywords = 50\n",
        "#get the document corpus (assumes that the text is in the \"text\" column):\n",
        "\n",
        "def text_input(file = 'alpha_test.csv'):\n",
        "  df = pd.read_csv(file)\n",
        "  df_clean = df[df['text'].apply(lambda x: isinstance(x, str))]\n",
        "  texts = [item.replace(\"\\t\", \" \") for item in df_clean['text']]\n",
        "  return texts\n",
        "\n",
        "\n",
        "#Keyword for the corpus a.k.a Global Concepts\n",
        "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
        "keywords = custom_kw_extractor.extract_keywords(corpus)\n",
        "#select a number of keywords to work with\n",
        "def keywords_number(n = len(keywords), input=keywords):\n",
        "  return [item[0] for item in input[:n]]"
      ],
      "metadata": {
        "id": "iYGTizz7SWZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#logger = logging.getLogger(__name__)\n",
        "\n",
        "class CSVAgent_Rfct:\n",
        "    \"\"\"Agent that analyzes CSV data to answer queries.\"\"\"\n",
        "\n",
        "    def __init__(self, csv_path, api_key=api_key):\n",
        "        \"\"\"\n",
        "        Initialize the agent.\n",
        "\n",
        "        Args:\n",
        "            csv_path: Path to the CSV file\n",
        "            api_key: Anthropic API key (optional)\n",
        "        \"\"\"\n",
        "        self.csv_path = csv_path\n",
        "        self.api_key = api_key\n",
        "\n",
        "        #Refactoring Prep:\n",
        "        self.workflow = Workflow(api_key=self.api_key)\n",
        "\n",
        "        # Initialize tools\n",
        "        self.csv_reader = CSVReader()\n",
        "        self.query_analyzer = QueryAnalyzer(api_key=self.api_key)\n",
        "        self.column_selector = ColumnSelector()\n",
        "        self.data_extractor = DataExtractor()\n",
        "        self.text_embedder = TextEmbedder()\n",
        "        self.answer_generator = AnswerGenerator(api_key=self.api_key)\n",
        "\n",
        "\n",
        "        #Recatoring Prep (new class functions)\n",
        "\n",
        "\n",
        "\n",
        "        # Load CSV data\n",
        "        self.df, load_message = self.csv_reader.read_csv(csv_path)\n",
        "        logger.info(load_message)\n",
        "\n",
        "        # Store column information\n",
        "        self.columns = list(self.df.columns) if not self.df.empty else []\n",
        "\n",
        "    def process_query(self, query):\n",
        "        \"\"\"\n",
        "        Process a user query and generate an answer.\n",
        "\n",
        "        Args:\n",
        "            query: User query string\n",
        "\n",
        "        Returns:\n",
        "            dict: Response containing answer and processing details\n",
        "        \"\"\"\n",
        "        logger.info(f\"Processing query: {query}\")\n",
        "\n",
        "        if self.df.empty:\n",
        "            return {\n",
        "                \"answer\": \"Unable to analyze the CSV file. Please check the file path and format.\",\n",
        "                \"success\": False\n",
        "            }\n",
        "\n",
        "        try:\n",
        "            # Step 1: Analyze the query\n",
        "            analysis = self.workflow.analyze_query(query, self.columns)\n",
        "\n",
        "            # Step 2: Select relevant columns\n",
        "            selected_columns = self.workflow.select_columns(self.df, analysis)\n",
        "\n",
        "            # Step 3: Extract relevant data\n",
        "            extracted_data = self.workflow.extract_data(self.df, analysis, selected_columns)\n",
        "\n",
        "            # Step 4: Apply text search if needed\n",
        "            if analysis.get(\"requires_text_search\", False) and \"text\" in self.columns:\n",
        "                if analysis.get(\"search_term\") != None:\n",
        "                  search_term = analysis.get(\"search_term\")\n",
        "                else:\n",
        "                  search_term = analysis.get(\"query\")\n",
        "                extracted_data = self.workflow.search_similar_texts(\n",
        "                    extracted_data,\n",
        "                    search_term,\n",
        "                    text_column=\"text\",\n",
        "                    top_k=10\n",
        "                )\n",
        "\n",
        "            # Step 5: Generate answer\n",
        "            answer = self.workflow.generate_answer(query, extracted_data, analysis)\n",
        "\n",
        "            return {\n",
        "                \"answer\": answer,\n",
        "                \"columns_analyzed\": selected_columns,\n",
        "                \"rows_analyzed\": len(extracted_data),\n",
        "                \"analysis_type\": analysis.get(\"analysis_type\", \"unknown\"),\n",
        "                \"success\": True\n",
        "            }\n",
        "            '''\n",
        "            return{\n",
        "                \"success\":True\n",
        "            }\n",
        "            '''\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing query: {str(e)}\")\n",
        "            return {\n",
        "                \"answer\": f\"An error occurred while processing your query: {str(e)}\",\n",
        "                \"success\": False\n",
        "            }"
      ],
      "metadata": {
        "id": "Pa6Y2yrbfaWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Initialize the agent\n",
        "agent = CSVAgent_Rfct('alpha_test.csv')"
      ],
      "metadata": {
        "id": "_cTcOBusfyot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Expermienting with \"Thinking\" Tools"
      ],
      "metadata": {
        "id": "klyY4aUH8_Uq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install anthropic"
      ],
      "metadata": {
        "id": "3kC7J8FxqLL6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "D142_FN9vx4q"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "import anthropic"
      ],
      "metadata": {
        "id": "GsasZb5mkN0z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "api_key_openAI = userdata.get('OpenAI')\n",
        "api_key_anthropic = userdata.get('Antropic')\n",
        "api_key_gemini = userdata.get('google')"
      ],
      "metadata": {
        "id": "utn8DLiHF8TC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset to be used:\n",
        "df = pd.read_csv('Mejuri_texts.csv')\n",
        "#Text cleanup\n",
        "def text_input(file = 'alpha_test.csv'):\n",
        "  df = pd.read_csv(file)\n",
        "  df_clean = df[df['Text'].apply(lambda x: isinstance(x, str))]\n",
        "  texts = [item.replace(\"\\t\", \" \") for item in df_clean['Text']]\n",
        "\n",
        "  return texts"
      ],
      "metadata": {
        "id": "KFjMlgPP9HmW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts_cleaned = text_input('Mejuri_texts.csv')\n",
        "corpus = ' '.join(texts_cleaned)"
      ],
      "metadata": {
        "id": "KtoF7Nk09Vh5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test1 = ' '.join(corpus.split()[:20000])"
      ],
      "metadata": {
        "id": "xVGWKyY1kVk9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Summarization Using Gemini (1 million tokens)\n",
        "question = \"could you provide me a summary of the following text: \" + corpus\n",
        "summary_response = model_gemini.generate_content(\n",
        "    contents=question,\n",
        "    generation_config=generation_config\n",
        ")\n",
        "\n",
        "summary = summary_response.text\n"
      ],
      "metadata": {
        "id": "j_E1zxffx-Bd"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "aSPRCElTykmg",
        "outputId": "f1300a52-2ba5-4679-8da0-9f32da385ff4"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Okay, here's a summary of the provided text focusing on key themes and points:\\n\\n**Overall Sentiment:**\\n\\nThe reviews are largely positive, praising the quality of Mejuri jewelry, the store's aesthetic, and the friendliness and helpfulness of the staff. However, there are also recurring criticisms regarding inventory issues (items out of stock), inconsistent customer service (some stylists are great, others are aloof or rude), and inefficiencies in the in-store checkout process. There are also some concerns about the limited selection of certain metals (white gold, silver), and the lack of clear pricing on displayed items.\\n\\n**Key Positives:**\\n\\n*   **High-Quality Jewelry:**  Many reviewers consistently praise the quality, beauty, and unique style of Mejuri's jewelry, especially the solid gold pieces and minimalist designs.\\n*   **Friendly and Knowledgeable Staff:** A significant number of reviewers highlight positive interactions with specific stylists who were helpful, patient, and provided excellent styling advice. Employees who do piercings are also highly commended. There are reoccurring names that are mentioned as being great to work with.\\n*   **Beautiful Store Aesthetic:** The store's design and ambiance are frequently described as clean, modern, chic, and welcoming.\\n*   **\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CODE MOVED FURTHER DOWN, NEXT CELL IS LEFT FOR TESTING**"
      ],
      "metadata": {
        "id": "RSFtJdkXLbxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_answwer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "cmosF6Ty1WQs",
        "outputId": "d6dd58e4-a79f-4958-85c1-c0d07b26cf34"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Certainly! Here’s a concise summary of the provided text, which appears to be a collection of customer reviews and feedback about their experiences shopping at Mejuri stores:\\n\\n---\\n\\n**Summary:**\\n\\nCustomers generally report positive experiences with Mejuri stores, highlighting friendly, helpful, and knowledgeable staff, beautiful store design, and high-quality jewelry. Many appreciate the efficient service, personalized assistance, and pleasant atmosphere. Some specific staff members received praise for their exceptional service.\\n\\nHowever, several recurring issues are mentioned:\\n- **Limited in-store inventory:** Many customers note that the in-store selection is smaller than what’s available online, making it hard to find or try certain items.\\n- **Product availability and sizing:** Some items were out of stock or not available in preferred sizes, and there were occasional discrepancies between online and in-store inventory information.\\n- **Store policies and checkout process:** Customers expressed frustration about refund/store credit policies, lack of cash payment options, inability to use multiple gift cards, and discomfort with entering card details manually at checkout.\\n- **Pricing transparency:** Some found it difficult to see prices clearly on items, which hindered their shopping experience.\\n- **Crowding and atmosphere:** During busy times, stores could feel overcrowded, and a few customers felt unwelcome or observed by security.\\n- **Suggestions for improvement:** Customers would like to see a wider variety of products in-store, clearer pricing, improved inventory accuracy, and more locations.\\n\\nDespite occasional negative experiences—such as unhelpful staff, product issues, or policy frustrations—the majority of feedback is positive, with many customers eager to return or recommend Mejuri to others.\\n\\n---\\n\\nLet me know if you’d like a more detailed or focused summary!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Raptor Implementation"
      ],
      "metadata": {
        "id": "jU63_qreQs_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install llama-index-packs-raptor\n",
        "!pip install llama-index"
      ],
      "metadata": {
        "id": "RN1lDeympfFf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from llama_index.packs.raptor import RaptorPack\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core import Document"
      ],
      "metadata": {
        "id": "vbmfUIUo9L8w"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "IJLQq-dHCZ1j"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_list = texts_cleaned[:400]\n",
        "documents = [Document(text=t) for t in text_list]"
      ],
      "metadata": {
        "id": "u4tLGrUMFOXB"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key_openAI  # Replace with your actual key"
      ],
      "metadata": {
        "id": "7yvrjG3mGl3v"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raptor_pack = RaptorPack(\n",
        "    documents,\n",
        "    embed_model=OpenAIEmbedding(\n",
        "        model=\"text-embedding-3-small\"\n",
        "    ),  # used for embedding clusters\n",
        "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1),  # used for generating summaries\n",
        "    #vector_store=vector_store,  # used for storage\n",
        "    similarity_top_k=2,  # top k for each layer, or overall top-k for collapsed\n",
        "    mode=\"collapsed\",  # sets default mode\n",
        "    transformations=[\n",
        "        SentenceSplitter(chunk_size=400, chunk_overlap=50)\n",
        "    ],  # transformations applied for ingestion\n",
        ")"
      ],
      "metadata": {
        "id": "Q_7NkUi6ulzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nodes = raptor_pack.run(\"What customers think about Mejuri?\", mode=\"collapsed\")\n",
        "print(len(nodes))\n",
        "nodes[0].text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "QAufXLqYK8ab",
        "outputId": "5cef693e-46c0-4be6-eb96-79655ca96983"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Customers have shared their experiences with Mejuri, a jewelry brand, expressing satisfaction with the products and customer service. Many customers appreciate the helpful and kind staff, with some even sharing personal stories and connections with the stylists. However, there have been instances where loyal customers felt uncomfortable due to perceived profiling by security. While some customers praise Mejuri for its inclusivity and accessibility, others have expressed frustration over being asked to wait outside the store, feeling it goes against the brand's image of being relatable and unpretentious. Despite these issues, overall, customers seem to value their interactions with Mejuri and the quality of the jewelry offered.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models Connectivity Tests"
      ],
      "metadata": {
        "id": "FtcJ-rPoOFG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Anthropic client\n",
        "client = anthropic.Anthropic(api_key=api_key_anthropic)\n",
        "\n",
        "try:\n",
        "    # Make a simple API request\n",
        "    response = client.messages.create(\n",
        "        model=\"claude-3-haiku-20240307\",\n",
        "        max_tokens=50,\n",
        "        messages=[{\"role\": \"user\", \"content\": \"Hello! Can you confirm you're working?\"}]\n",
        "    )\n",
        "\n",
        "    # Print the response\n",
        "    print(\"Claude's Response:\", response.content)\n",
        "\n",
        "except anthropic.APIStatusError as e:\n",
        "    print(f\"API returned an error: {e}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "e0NCjdsuOMKJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38ea94a9-e0e4-4bcc-f969-16ade17ec78d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Claude's Response: [TextBlock(citations=None, text=\"Yes, I'm working and ready to assist you! How can I help you today?\", type='text')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gemini Model**"
      ],
      "metadata": {
        "id": "LnL7wf6aZhej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "# Replace with your actual Gemini API key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_API_KEY\"\n",
        "\n",
        "# Initialize the Gemini client\n",
        "client = genai.Client()\n",
        "\n",
        "# Specify the Gemini 2.0 Flash model\n",
        "model = client.models.get(\"gemini-2.0-flash\")\n",
        "\n",
        "# Create a prompt\n",
        "prompt = \"How does AI work?\"\n",
        "\n",
        "# Generate content using the model\n",
        "response = model.generate_content(prompt)\n",
        "\n",
        "# Print the response\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "8eHjLrt4Znpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install --upgrade google-generativeai"
      ],
      "metadata": {
        "id": "ANiG3DOMo8d-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "\n",
        "# Configure the API key\n",
        "genai.configure(api_key=api_key_gemini) # Replace \"YOUR_API_KEY\" with your actual API key\n",
        "\n",
        "# Initialize the Gemini 2.0 Flash model\n",
        "model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "\n",
        "generation_config = {\n",
        "    \"temperature\": 0.7,  # Controls the randomness of the output (0.0 - 1.0)\n",
        "    \"max_output_tokens\": 256,  # Limits the maximum number of tokens in the generated response\n",
        "    # You can also include other parameters here like top_p, top_k, stop_sequences, etc.\n",
        "}\n",
        "\n",
        "\n",
        "# Prepare the prompt for the model\n",
        "prompt = \"Write a short poem about the ocean.\"\n",
        "\n",
        "# Generate content using the model\n",
        "response = model.generate_content(\n",
        "    contents=prompt,\n",
        "    #safety_settings=safety_settings,\n",
        "    generation_config=generation_config\n",
        ")\n",
        "\n",
        "# Print the generated text\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "1FOvyJH3Z_BW",
        "outputId": "8c259e3b-84fb-46e4-ec38-604c6fcb783a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blue giant, breathing deep,\n",
            "Secrets held in currents sleep.\n",
            "Salty kiss upon the shore,\n",
            "Waves that crash and ever roar.\n",
            "\n",
            "Sunlight dances on the foam,\n",
            "A wild and watery home.\n",
            "Mysteries within its heart,\n",
            "A world forever set apart.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Thinking Tools"
      ],
      "metadata": {
        "id": "guZX5knw7CkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#openAI thinking tools\n",
        "import os\n",
        "import openai\n",
        "import json\n",
        "\n",
        "client = openai.OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "\n",
        "# Define the think tool as a function spec for OpenAI\n",
        "think_tool = {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"think\",\n",
        "        \"description\": \"Reflect on a problem or idea and return your thoughts.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"thought\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The reasoning or idea to reflect on\"\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"thought\"]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Function to simulate running the tool (like the executor did)\n",
        "def think(thought: str) -> str:\n",
        "    return thought  # Just reflect back\n",
        "\n",
        "# Main interaction function\n",
        "def run_think_tool(prompt: str, model=\"gpt-4-turbo\"):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        tools=[think_tool],\n",
        "        tool_choice=\"auto\"\n",
        "    )\n",
        "\n",
        "    message = response.choices[0].message\n",
        "\n",
        "    if message.tool_calls:\n",
        "        tool_call = message.tool_calls[0]\n",
        "        tool_name = tool_call.function.name\n",
        "        args = json.loads(tool_call.function.arguments)\n",
        "\n",
        "        # Execute the tool manually\n",
        "        if tool_name == \"think\":\n",
        "            tool_result = think(**args)\n",
        "        else:\n",
        "            tool_result = f\"Tool '{tool_name}' not implemented.\"\n",
        "\n",
        "        return {\n",
        "            \"tool_call\": args,\n",
        "            \"tool_result\": tool_result\n",
        "        }\n",
        "    else:\n",
        "        return message.content"
      ],
      "metadata": {
        "id": "ZyTUKknq7QCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Anthropic implementation\n",
        "from anthropic import Anthropic\n",
        "\n",
        "class ThinkingTool:\n",
        "    '''\n",
        "     A tool that enables Claude to stop and think during complex problem-solving situations.\n",
        "    This implementation is based on Anthropic's \"think\" tool concept.\n",
        "    '''\n",
        "\n",
        "\n",
        "\n",
        "    def __init__(self, api_key = None):\n",
        "        '''\n",
        "        Initialize the thinking tool with Claude API credentials.\n",
        "\n",
        "        Args:\n",
        "            api_key: Optional Anthropic API key. If not provided, will look for ANTHROPIC_API_KEY in environment.\n",
        "        '''\n",
        "\n",
        "        self.api_key = api_key\n",
        "        if not self.api_key:\n",
        "            raise ValueError(\"Anthropic API key is required. Set ANTHROPIC_API_KEY environment variable or pass api_key parameter.\")\n",
        "\n",
        "        self.client = Anthropic(api_key=self.api_key)\n",
        "        self.thinking_log = []\n",
        "\n",
        "    def think(self, thought: str, model: str = \"claude-3-sonnet-20240229\") -> str:\n",
        "        '''\n",
        "        Use Claude to think about something. This will not make any external changes\n",
        "        but will log the thought process.\n",
        "\n",
        "        Args:\n",
        "            thought: The thought or reasoning to process\n",
        "            model: The Claude model to use for thinking\n",
        "\n",
        "        Returns:\n",
        "            str: Claude's response to the thought\n",
        "        '''\n",
        "        try:\n",
        "            response = self.client.messages.create(\n",
        "                model='claude-3-7-sonnet-latest',\n",
        "                max_tokens=1000,\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": f'''Please analyze and think about the following:\n",
        "                        {thought}\n",
        "\n",
        "Provide your reasoning, considerations, and any potential implications. Focus on:\n",
        "1. Breaking down complex information\n",
        "2. Identifying key points and relationships\n",
        "3. Considering different perspectives\n",
        "4. Noting any assumptions or uncertainties\n",
        "'''\n",
        "                    }\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            # Log the thought and response\n",
        "            self.thinking_log.append({\n",
        "                \"thought\": thought,\n",
        "                \"response\": response.content[0].text\n",
        "            })\n",
        "\n",
        "            return response.content[0].text\n",
        "\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"Error during thinking process: {str(e)}\")\n",
        "\n",
        "    def get_thinking_log(self) -> list:\n",
        "        '''\n",
        "        Get the history of thoughts and responses.\n",
        "\n",
        "        Returns:\n",
        "            list: List of dictionaries containing thought and response pairs\n",
        "        '''\n",
        "        return self.thinking_log\n"
      ],
      "metadata": {
        "id": "noTYDTWxYqiM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "\n",
        "# Initialize the thinking tool\n",
        "thinker = ThinkingTool(api_key_anthropic)\n",
        "\n",
        "# Example thought process\n",
        "thought = f'''\n",
        "Assume you a business analyst tasked to extract insight from the following {test1}\n",
        "While producing the insight, assume that they will be use to develop a business development strategy.\n",
        "'''\n",
        "\n",
        "# Get Claude's thinking on this topic\n",
        "response = thinker.think(thought)\n",
        "print(\"Claude's thinking response:\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qi0caiMuZKgc",
        "outputId": "38715652-bc6d-4a7b-f620-cde0301211c9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Claude's thinking response:\n",
            "# Business Analysis of Mejuri Customer Feedback\n",
            "\n",
            "## Executive Summary\n",
            "\n",
            "Based on extensive customer feedback, Mejuri has created a strong brand foundation with quality products and generally positive customer experiences, but there are several key areas for improvement that could drive business growth. The feedback reveals both operational challenges and strategic opportunities that should inform business development strategy.\n",
            "\n",
            "## Key Strengths\n",
            "\n",
            "1. **Product Quality and Design**: Customers consistently praise Mejuri's jewelry quality, aesthetic, and price point. The minimalist, clean design language resonates with customers.\n",
            "\n",
            "2. **In-Store Experience**: Many customers report positive experiences with staff described as helpful, knowledgeable, and friendly. The store atmosphere is generally well-received.\n",
            "\n",
            "3. **Piercing Services**: The piercing studios are highly rated, with specialists receiving particular praise for their professionalism and care.\n",
            "\n",
            "4. **Brand Perception**: Mejuri has successfully positioned itself as offering attainable luxury with high-quality materials at reasonable prices.\n",
            "\n",
            "## Key Areas for Improvement\n",
            "\n",
            "1. **Inventory Management**: \n",
            "   - Frequent mentions of items being out of stock in stores\n",
            "   - Disconnect between online availability and in-store inventory\n",
            "   - Long wait times for ordered items\n",
            "\n",
            "2. **In-Store Experience Inconsistencies**:\n",
            "   - Varying quality of customer service between locations and staff members\n",
            "   - Issues with store layout and crowding during busy periods\n",
            "   - Long wait times to enter stores\n",
            "\n",
            "3. **Payment and Checkout Process**:\n",
            "   - Customers expressed confusion about the digital checkout process\n",
            "   - Concerns about security when entering credit card information into tablets\n",
            "   - Preference for traditional POS systems over web-based checkout\n",
            "\n",
            "4. **Price Transparency**:\n",
            "   - Consistent feedback about lack of visible pricing on merchandise\n",
            "   - Customers dislike having to ask staff about prices\n",
            "\n",
            "5. **Product Range and Availability**:\n",
            "   - Desire for more variety, particularly in men's jewelry\n",
            "   - Requests for more silver/white gold options\n",
            "   - Need for half sizes in rings\n",
            "   - Limited selection compared to online offerings\n",
            "\n",
            "## Business Development Opportunities\n",
            "\n",
            "1. **Inventory and Supply Chain Optimization**\n",
            "   - Implement improved inventory management system to better sync online and in-store availability\n",
            "   - Develop more accurate forecasting models to reduce stockouts of popular items\n",
            "   - Consider regional inventory distribution based on local preferences\n",
            "\n",
            "2. **Store Experience Enhancement**\n",
            "   - Standardize training across locations to ensure consistent customer service\n",
            "   - Develop a more efficient system for high-traffic periods\n",
            "   - Create dedicated zones for piercing consultations, browsing, and checkout\n",
            "   - Implement a digital queue system for busy locations\n",
            "\n",
            "3. **Product Extension**\n",
            "   - Expand men's jewelry collection based on repeated customer requests\n",
            "   - Introduce half sizes for rings to address fit issues\n",
            "   - Increase silver and white gold options to complement existing gold lines\n",
            "   - Consider seasonal or limited collections to drive repeat visits\n",
            "\n",
            "4. **Digital Integration**\n",
            "   - Develop a hybrid checkout system that maintains security while offering convenience\n",
            "   - Create in-store digital displays showing product information and pricing\n",
            "   - Implement QR codes for self-service price checking\n",
            "   - Develop a mobile app for customers to browse full catalog while in-store\n",
            "\n",
            "5. **Customer Journey Improvements**\n",
            "   - Redesign the exchange and return process to be more seamless\n",
            "   - Establish clear communication protocols for repairs and special orders\n",
            "   - Create an improved gifting experience with more packaging options\n",
            "\n",
            "6. **Loyalty Program Development**\n",
            "   - Implement a customer loyalty program to encourage repeat purchases\n",
            "   - Offer exclusive benefits for returning customers (early access to new collections, etc.)\n",
            "   - Create special events for loyal customers\n",
            "\n",
            "## Regional Considerations\n",
            "\n",
            "The feedback suggests varying experiences across different store locations. A regionalized approach to business development might be beneficial:\n",
            "\n",
            "- **High-Traffic Urban Locations**: Focus on queue management, efficient checkout, and staff training\n",
            "- **Suburban Locations**: Emphasize product range and availability\n",
            "- **New Markets**: Ensure comprehensive staff training and consistent brand experience\n",
            "\n",
            "## Implementation Priorities\n",
            "\n",
            "Based on impact versus effort analysis:\n",
            "\n",
            "1. **Immediate Fixes** (High Impact, Low Effort):\n",
            "   - Improve price visibility in stores\n",
            "   - Standardize customer service training\n",
            "   - Fix inventory discrepancies between online and in-store\n",
            "\n",
            "2. **Medium-Term Projects** (High Impact, Medium Effort):\n",
            "   - Expand product range in key categories\n",
            "   - Improve checkout experience\n",
            "   - Enhance gifting options\n",
            "\n",
            "3. **Strategic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Thinking Tools - implementation ideas** *& Rafactoring*"
      ],
      "metadata": {
        "id": "hqw5I8qGAksR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class LLMProvider(ABC):\n",
        "    def __init__(self, model_name: str, api_key: str, client):\n",
        "        self.client = client\n",
        "        self.model_name = model_name\n",
        "        self.api_key = api_key\n",
        "\n",
        "    @abstractmethod\n",
        "    def call_model(self, prompt: str) -> object:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def extract_text(self, response: object) -> str:\n",
        "        pass"
      ],
      "metadata": {
        "id": "obudd66sAtk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OpenAIProvider(LLMProvider):\n",
        "    def __init__(self, model_name: str, api_key: str, client):\n",
        "        super().__init__(model_name, api_key, client)\n",
        "\n",
        "    def call_model(self, prompt: str) -> dict:\n",
        "        # You can now use self.model_name and self.api_key\n",
        "        response = client.chat.completions.create(\n",
        "        model=self.model_name,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.7)\n",
        "        return response\n",
        "\n",
        "    def extract_text(self, response: dict) -> str:\n",
        "        return response.choices[0].message.content.strip()"
      ],
      "metadata": {
        "id": "C3Xe1Rv4BbyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#OpenAI(api_key=openai.api_key))\n",
        "#client = OpenAI(api_key=openai.api_key)\n",
        "openAI_gpt_4_1 = OpenAIProvider(\"gpt-4.1-2025-04-14\", api_key_openAI, OpenAI(api_key=openai.api_key))"
      ],
      "metadata": {
        "id": "NKsQt3UjBrlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = openAI_gpt_4_1.call_model(\"could you provide me a summary of the following text: \" + test1)"
      ],
      "metadata": {
        "id": "I66y6EoyBsby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_answwer = openAI_gpt_4_1.extract_text(response)"
      ],
      "metadata": {
        "id": "LEPFeOQVCHBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*OUTAKES*"
      ],
      "metadata": {
        "id": "YuBRs2BsC9fT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code example: OpenAI\n",
        "\n",
        "# Make sure your API key is set in your environment or replace below\n",
        "openai.api_key = api_key_openAI\n",
        "\n",
        "\n",
        "client = OpenAI(api_key=openai.api_key)  # or set your key directly\n",
        "\n",
        "def think_then_answer(question: str, model: str = \"gpt-4.1-2025-04-14\") -> dict:\n",
        "    \"\"\"\n",
        "    Executes a two-step 'think then answer' reasoning pattern.\n",
        "\n",
        "    Parameters:\n",
        "        question (str): The user question.\n",
        "        model (str): The OpenAI model ID (default uses GPT-4 Turbo).\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with 'thoughts' and 'final_answer'.\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Model thinks\n",
        "    think_prompt = f\"\"\"You're a thoughtful assistant. Before answering the user's question, write out your reasoning step by step.\n",
        "Question: {question}\n",
        "Your internal thoughts:\"\"\"\n",
        "\n",
        "    thoughts_response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": think_prompt}],\n",
        "        temperature=0.7\n",
        "    )\n",
        "    thoughts = thoughts_response.choices[0].message.content.strip()\n",
        "\n",
        "    # Step 2: Model answers using its own thoughts\n",
        "    final_prompt = f\"\"\"Based on your reasoning below, provide a clear and final answer to the user's question.\n",
        "Reasoning: {thoughts}\n",
        "Final Answer:\"\"\"\n",
        "\n",
        "    final_response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": final_prompt}],\n",
        "        temperature=0.7\n",
        "    )\n",
        "    final_answer = final_response.choices[0].message.content.strip()\n",
        "\n",
        "    return {\n",
        "        \"thoughts\": thoughts,\n",
        "        \"final_answer\": final_answer\n",
        "    }"
      ],
      "metadata": {
        "id": "J4zxkyuHDC87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#code example: Anthropic\n",
        "\n",
        "\n",
        "client = anthropic.Anthropic(api_key=api_key_anthropic)\n",
        "\n",
        "def think_then_answer_anthropic(question: str, model: str = \"claude-3-7-sonnet-20250219\") -> dict:\n",
        "    \"\"\"\n",
        "    Executes a two-step 'think then answer' reasoning pattern.\n",
        "\n",
        "    Parameters:\n",
        "        question (str): The user question.\n",
        "        model (str): The OpenAI model ID (default uses GPT-4 Turbo).\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with 'thoughts' and 'final_answer'.\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Model thinks\n",
        "    think_prompt = f\"\"\"You're a thoughtful assistant. Before answering the user's question, write out your reasoning step by step.\n",
        "Question: {question}\n",
        "Your internal thoughts:\"\"\"\n",
        "\n",
        "    thoughts_response = client.messages.create(\n",
        "        model=model,\n",
        "        max_tokens=4096,\n",
        "        messages=[{\"role\": \"user\", \"content\": think_prompt}]\n",
        "    )\n",
        "\n",
        "    thoughts = thoughts_response.content\n",
        "\n",
        "    # Step 2: Model answers using its own thoughts\n",
        "    final_prompt = f\"\"\"Based on your reasoning below, provide a clear and final answer to the user's question.\n",
        "Reasoning: {thoughts}\n",
        "Final Answer:\"\"\"\n",
        "\n",
        "    final_response = client.messages.create(\n",
        "        model=model,\n",
        "        max_tokens=4096,\n",
        "        messages=[{\"role\": \"user\", \"content\": final_prompt}],\n",
        "        temperature=0.7\n",
        "    )\n",
        "    final_answer = final_response.content\n",
        "\n",
        "    return {\n",
        "        \"thoughts\": thoughts,\n",
        "        \"final_answer\": final_answer\n",
        "    }"
      ],
      "metadata": {
        "id": "VH3FAfdkD4Mn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"could you provide me a summary of the following text: \" + test1\n",
        "answer = think_then_answer_anthropic(question)"
      ],
      "metadata": {
        "id": "B6_U4vibEG-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Thinking Tool Gemini\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Configure the API key\n",
        "genai.configure(api_key=api_key_gemini)\n",
        "\n",
        "# Initialize the Gemini 2.0 Flash model\n",
        "model_gemini = genai.GenerativeModel('gemini-2.0-flash')\n",
        "\n",
        "generation_config = {\n",
        "    \"temperature\": 0.7,  # Controls the randomness of the output (0.0 - 1.0)\n",
        "    \"max_output_tokens\": 256,  # Limits the maximum number of tokens in the generated response\n",
        "    # You can also include other parameters here like top_p, top_k, stop_sequences, etc.\n",
        "}\n",
        "\n",
        "\n",
        "def think_then_answer_gemini(question: str, model=model_gemini) -> dict:\n",
        "    \"\"\"\n",
        "    Executes a two-step 'think then answer' reasoning pattern.\n",
        "\n",
        "    Parameters:\n",
        "        question (str): The user question.\n",
        "        model (str): The OpenAI model ID (default uses GPT-4 Turbo).\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with 'thoughts' and 'final_answer'.\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Model thinks\n",
        "    think_prompt = f\"\"\"You're a thoughtful assistant. Before answering the user's question, write out your reasoning step by step.\n",
        "Question: {question}\n",
        "Your internal thoughts:\"\"\"\n",
        "\n",
        "    thoughts_response = model.generate_content(\n",
        "        contents=think_prompt,\n",
        "        generation_config=generation_config\n",
        "        )\n",
        "    thoughts = thoughts_response.text\n",
        "\n",
        "    # Step 2: Model answers using its own thoughts\n",
        "    final_prompt = f\"\"\"Based on your reasoning below, provide a clear and final answer to the user's question.\n",
        "Reasoning: {thoughts}\n",
        "Final Answer:\"\"\"\n",
        "\n",
        "    final_response = model.generate_content(\n",
        "        contents=final_prompt,\n",
        "        generation_config=generation_config\n",
        "    )\n",
        "\n",
        "    final_answer = final_response.text\n",
        "\n",
        "    return {\n",
        "        \"thoughts\": thoughts,\n",
        "        \"final_answer\": final_answer\n",
        "    }"
      ],
      "metadata": {
        "id": "NReJg-0kEo5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"could you provide me a summary of the following text: \" + test1\n",
        "answer = think_then_answer_gemini(question)"
      ],
      "metadata": {
        "id": "UNViyHyUE0of"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}