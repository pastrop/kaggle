{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNU1fmp4fkxgz5fc9/taJL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pastrop/kaggle/blob/master/Data_Analyst.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install anthropic\n",
        "!pip install sentence-transformers\n",
        "!pip install pandas\n",
        "!pip install yake"
      ],
      "metadata": {
        "id": "rVMAhiK3kUfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import anthropic\n",
        "import os\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import logging\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "import logging\n",
        "import traceback"
      ],
      "metadata": {
        "id": "yr1qrObCkLnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfRnTARQf6bn"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('Agent_Anth')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "fZIXWRTSxf-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Service Classes\n",
        "class CSVReader:\n",
        "    \"\"\"Tool to read CSV files into pandas dataframes.\"\"\"\n",
        "\n",
        "    def read_csv(self, file_path: str) -> Tuple[pd.DataFrame, str]:\n",
        "        \"\"\"\n",
        "        Read a CSV file into a pandas dataframe.\n",
        "\n",
        "        Args:\n",
        "            file_path: Path to the CSV file\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (dataframe, message)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            df = pd.read_csv(file_path)\n",
        "            columns_info = \", \".join([f\"{col} ({df[col].dtype})\" for col in df.columns])\n",
        "            message = f\"Successfully loaded CSV with {len(df)} rows and {len(df.columns)} columns: {columns_info}\"\n",
        "            logger.info(message)\n",
        "            return df, message\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Error reading CSV file: {str(e)}\"\n",
        "            logger.error(error_msg)\n",
        "            return pd.DataFrame(), error_msg\n",
        "\n",
        "class QueryAnalyzer:\n",
        "    \"\"\"Tool to analyze queries and determine information needs.\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: Optional[str] = None):\n",
        "        \"\"\"\n",
        "        Initialize with Anthropic API key.\n",
        "\n",
        "        Args:\n",
        "            api_key: Anthropic API key (will use environment variable if None)\n",
        "        \"\"\"\n",
        "        self.api_key = api_key or os.environ.get(\"ANTHROPIC_API_KEY\")\n",
        "        if not self.api_key:\n",
        "            logger.warning(\"No Anthropic API key provided. Set ANTHROPIC_API_KEY environment variable.\")\n",
        "\n",
        "        self.client = anthropic.Anthropic(api_key=self.api_key)\n",
        "\n",
        "    def analyze_query(self, query: str, columns: List[str]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analyze the query to determine information needs.\n",
        "\n",
        "        Args:\n",
        "            query: The user's query\n",
        "            columns: List of available columns in the dataframe\n",
        "\n",
        "        Returns:\n",
        "            Dict with analysis results\n",
        "        \"\"\"\n",
        "        try:\n",
        "            prompt = f\"\"\"\n",
        "            <columns>\n",
        "            {', '.join(columns)}\n",
        "            </columns>\n",
        "\n",
        "            You are an AI assistant that analyzes queries about a dataset. Based on the user query, determine:\n",
        "            1. Which columns from the dataset are needed to answer the query\n",
        "            2. What type of analysis is required (filtering, aggregation, etc.)\n",
        "            3. Whether any specific values or conditions are mentioned\n",
        "\n",
        "            User query: {query}\n",
        "\n",
        "            Respond in JSON format like this:\n",
        "            {{\n",
        "                \"needed_columns\": [\"column1\", \"column2\"],\n",
        "                \"analysis_type\": \"one of: filtering, aggregation, sorting, comparison, general_info, semantic_search\",\n",
        "                \"filter_conditions\": {{\"column_name\": \"filter_value\"}},\n",
        "                \"aggregation_function\": \"one of: count, sum, average, min, max, none\",\n",
        "                \"sort_by\": \"column_name or null\",\n",
        "                \"sort_order\": \"ascending or descending or null\",\n",
        "                \"requires_text_search\": true/false,\n",
        "                \"search_term\": \"term to search for in text or null\"\n",
        "                \"query\": \"original query\"\n",
        "            }}\n",
        "\n",
        "            Make sure all column names exactly match the provided list. If a column is not mentioned or needed, don't include it.\n",
        "            \"\"\"\n",
        "\n",
        "            response = self.client.messages.create(\n",
        "                model=\"claude-3-haiku-20240307\",\n",
        "                max_tokens=1000,\n",
        "                messages=[\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            analysis_text = response.content[0].text\n",
        "            print(f'query_analyzer printout: response:{analysis_text}')\n",
        "\n",
        "            # Extract JSON from response\n",
        "            import json\n",
        "            import re\n",
        "\n",
        "            json_match = re.search(r'{[\\s\\S]+}', analysis_text)\n",
        "            if json_match:\n",
        "                analysis = json.loads(json_match.group(0))\n",
        "                logger.info(f\"Query analysis completed: {str(analysis)}\")\n",
        "                return analysis\n",
        "            else:\n",
        "                logger.error(\"Failed to extract JSON from Claude's response\")\n",
        "                return {\n",
        "                    \"needed_columns\": columns,\n",
        "                    \"analysis_type\": \"general_info\",\n",
        "                    \"requires_text_search\": False\n",
        "                }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error analyzing query: {str(e)}\")\n",
        "            return {\n",
        "                \"needed_columns\": columns,\n",
        "                \"analysis_type\": \"general_info\",\n",
        "                \"requires_text_search\": False\n",
        "            }\n",
        "\n",
        "class ColumnSelector:\n",
        "    \"\"\"Tool to determine which columns are needed for a query.\"\"\"\n",
        "\n",
        "    def select_columns(self, df: pd.DataFrame, analysis: Dict[str, Any]) -> List[str]:\n",
        "        \"\"\"\n",
        "        Select columns needed to answer the query.\n",
        "\n",
        "        Args:\n",
        "            df: The dataframe\n",
        "            analysis: Query analysis results\n",
        "\n",
        "        Returns:\n",
        "            List of column names to use\n",
        "        \"\"\"\n",
        "        all_columns = df.columns.tolist()\n",
        "\n",
        "        # Start with columns specified in the analysis\n",
        "        needed_columns = analysis.get(\"needed_columns\", [])\n",
        "\n",
        "        # Always include text column if text search is required\n",
        "        if analysis.get(\"requires_text_search\", False) and \"text\" in all_columns:\n",
        "            if \"text\" not in needed_columns:\n",
        "                needed_columns.append(\"text\")\n",
        "\n",
        "        # Add filter columns if not already included\n",
        "        filter_conditions = analysis.get(\"filter_conditions\", {})\n",
        "        for col in filter_conditions.keys():\n",
        "            if col in all_columns and col not in needed_columns:\n",
        "                needed_columns.append(col)\n",
        "\n",
        "        # Add sort column if not already included\n",
        "        sort_by = analysis.get(\"sort_by\")\n",
        "        if sort_by and sort_by in all_columns and sort_by not in needed_columns:\n",
        "            needed_columns.append(sort_by)\n",
        "\n",
        "        # If no columns were determined, return all columns\n",
        "        if not needed_columns:\n",
        "            logger.warning(\"No specific columns determined, using all columns\")\n",
        "            needed_columns = all_columns\n",
        "\n",
        "        logger.info(f\"Selected columns: {', '.join(needed_columns)}\")\n",
        "        print(f\"Column Selector Printout: Selected columns: {', '.join(needed_columns)}\")\n",
        "        return needed_columns\n",
        "\n",
        "class DataExtractor:\n",
        "    \"\"\"Tool to extract relevant data from the dataframe.\"\"\"\n",
        "\n",
        "    def extract_data(self, df: pd.DataFrame, analysis: Dict[str, Any], selected_columns: List[str]) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Extract relevant data based on query analysis.\n",
        "\n",
        "        Args:\n",
        "            df: The dataframe\n",
        "            analysis: Query analysis results\n",
        "            selected_columns: Columns to include\n",
        "\n",
        "        Returns:\n",
        "            Filtered dataframe\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Start with selected columns\n",
        "            result_df = df[selected_columns].copy()\n",
        "\n",
        "            # Apply filtering if specified\n",
        "            filter_conditions = analysis.get(\"filter_conditions\", {})\n",
        "            if filter_conditions and analysis.get(\"analysis_type\") in [\"filtering\", \"comparison\"]:\n",
        "                for col, value in filter_conditions.items():\n",
        "                    if col in df.columns:\n",
        "                        # Handle different filter types\n",
        "                        if isinstance(value, dict):\n",
        "                            # Range or comparison filter\n",
        "                            if \"min\" in value and \"max\" in value:\n",
        "                                result_df = result_df[(result_df[col] >= value[\"min\"]) &\n",
        "                                                     (result_df[col] <= value[\"max\"])]\n",
        "                            elif \"min\" in value:\n",
        "                                result_df = result_df[result_df[col] >= value[\"min\"]]\n",
        "                            elif \"max\" in value:\n",
        "                                result_df = result_df[result_df[col] <= value[\"max\"]]\n",
        "                            elif \"not_equal\" in value:\n",
        "                                result_df = result_df[result_df[col] != value[\"not_equal\"]]\n",
        "                        elif isinstance(value, list):\n",
        "                            # List of values\n",
        "                            result_df = result_df[result_df[col].isin(value)]\n",
        "                        else:\n",
        "                            # Simple equality\n",
        "                            result_df = result_df[result_df[col] == value]\n",
        "\n",
        "            # Apply sorting if specified\n",
        "            sort_by = analysis.get(\"sort_by\")\n",
        "            sort_order = analysis.get(\"sort_order\", \"ascending\")\n",
        "            if sort_by and sort_by in result_df.columns:\n",
        "                ascending = sort_order.lower() != \"descending\"\n",
        "                result_df = result_df.sort_values(by=sort_by, ascending=ascending)\n",
        "\n",
        "            # Apply aggregation if specified\n",
        "            agg_function = analysis.get(\"aggregation_function\")\n",
        "            if agg_function and agg_function != \"none\" and analysis.get(\"analysis_type\") == \"aggregation\":\n",
        "                # Determine which column to aggregate\n",
        "                agg_col = None\n",
        "                for col in result_df.columns:\n",
        "                    if col != \"text\" and pd.api.types.is_numeric_dtype(result_df[col]):\n",
        "                        agg_col = col\n",
        "                        break\n",
        "\n",
        "                if agg_col:\n",
        "                    if agg_function == \"count\":\n",
        "                        result_df = result_df.groupby(selected_columns[0])[agg_col].count().reset_index()\n",
        "                    elif agg_function == \"sum\":\n",
        "                        result_df = result_df.groupby(selected_columns[0])[agg_col].sum().reset_index()\n",
        "                    elif agg_function == \"average\":\n",
        "                        result_df = result_df.groupby(selected_columns[0])[agg_col].mean().reset_index()\n",
        "                    elif agg_function == \"min\":\n",
        "                        result_df = result_df.groupby(selected_columns[0])[agg_col].min().reset_index()\n",
        "                    elif agg_function == \"max\":\n",
        "                        result_df = result_df.groupby(selected_columns[0])[agg_col].max().reset_index()\n",
        "\n",
        "            logger.info(f\"Extracted {len(result_df)} rows of data\")\n",
        "            print(f\"Data Extractor Printout: {result_df}\")\n",
        "            return result_df\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error extracting data: {str(e)}\")\n",
        "            # Return original data with selected columns\n",
        "            return df[selected_columns].copy()\n",
        "\n",
        "class TextEmbedder:\n",
        "    \"\"\"Tool to generate and search text embeddings.\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
        "        \"\"\"\n",
        "        Initialize with embedding model.\n",
        "\n",
        "        Args:\n",
        "            model_name: Name of the embedding model\n",
        "        \"\"\"\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        logger.info(f\"Initialized embedding model: {model_name}\")\n",
        "\n",
        "    def search_similar_texts(self, df: pd.DataFrame, query: str, text_column: str = \"text\",\n",
        "                            top_k: int = 5) -> pd.DataFrame:\n",
        "       \"\"\"\n",
        "        Find texts most similar to the query.\n",
        "\n",
        "        Args:\n",
        "            df: Dataframe with text column\n",
        "            query: Search query\n",
        "            text_column: Column containing text\n",
        "            top_k: Number of results to return\n",
        "\n",
        "        Returns:\n",
        "            Dataframe with most similar texts\n",
        "        \"\"\"\n",
        "        print('I am inside searching simular texts')\n",
        "        if text_column not in df.columns:\n",
        "            logger.error(f\"Text column '{text_column}' not found in dataframe\")\n",
        "            return df\n",
        "\n",
        "        try:\n",
        "            # Generate embeddings\n",
        "            print('trying to generate embeddings')\n",
        "            texts = df[text_column].fillna(\"\").tolist()\n",
        "            print(f'text_embeddings printout: {texts}')\n",
        "            text_embeddings = self.model.encode(texts)\n",
        "            print('got the text embeddings!')\n",
        "            print(f'input query printout: {query} of type {type(query)}')\n",
        "            query_embedding = self.model.encode(query)\n",
        "            print('got the query embeddings!')\n",
        "\n",
        "            # Calculate similarities\n",
        "            similarities = cosine_similarity(\n",
        "                query_embedding.reshape(1, -1),\n",
        "                text_embeddings\n",
        "            )[0]\n",
        "\n",
        "            # Add similarity scores to dataframe\n",
        "            result_df = df.copy()\n",
        "            result_df[\"similarity_score\"] = similarities\n",
        "\n",
        "            # Sort by similarity and take top_k\n",
        "            result_df = result_df.sort_values(\"similarity_score\", ascending=False).head(top_k)\n",
        "\n",
        "            logger.info(f\"Found {len(result_df)} similar texts\")\n",
        "            print(f\"search_similar_texts printout: {result_df}\")\n",
        "            return result_df\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error searching similar texts: {str(e)}\")\n",
        "            print(traceback.format_exc())  # Print full traceback\n",
        "            return df\n",
        "\n",
        "class AnswerGenerator:\n",
        "    \"\"\"Tool to generate answers using Claude 3.7.\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: Optional[str] = None):\n",
        "        \"\"\"\n",
        "        Initialize with Anthropic API key.\n",
        "\n",
        "        Args:\n",
        "            api_key: Anthropic API key (will use environment variable if None)\n",
        "        \"\"\"\n",
        "        self.api_key = api_key or os.environ.get(\"ANTHROPIC_API_KEY\")\n",
        "        if not self.api_key:\n",
        "            logger.warning(\"No Anthropic API key provided. Set ANTHROPIC_API_KEY environment variable.\")\n",
        "\n",
        "        self.client = anthropic.Anthropic(api_key=self.api_key)\n",
        "\n",
        "    def generate_answer(self, query: str, data_df: pd.DataFrame, analysis: Dict[str, Any]) -> str:\n",
        "        \"\"\"\n",
        "        Generate an answer using Claude 3.7.\n",
        "\n",
        "        Args:\n",
        "            query: User query\n",
        "            data_df: Dataframe with relevant data\n",
        "            analysis: Query analysis results\n",
        "\n",
        "        Returns:\n",
        "            Generated answer\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Convert dataframe to string representation\n",
        "            data_str = data_df.to_string(index=False) if not data_df.empty else \"No data found\"\n",
        "\n",
        "            # Create prompt for Claude\n",
        "            prompt = f\"\"\"\n",
        "            <data>\n",
        "            {data_str}\n",
        "            </data>\n",
        "\n",
        "            <query_analysis>\n",
        "            {str(analysis)}\n",
        "            </query_analysis>\n",
        "\n",
        "            User query: {query}\n",
        "\n",
        "            Based on the provided data and analysis of the query, please provide a comprehensive answer to the user's question.\n",
        "            Include specific details from the data where appropriate. If the data doesn't contain information needed to answer the query,\n",
        "            state that clearly.\n",
        "\n",
        "            Answer the query directly and concisely. If appropriate, include any relevant statistics from the data.\n",
        "            \"\"\"\n",
        "\n",
        "            response = self.client.messages.create(\n",
        "                model=\"claude-3-haiku-20240307\",\n",
        "                max_tokens=2000,\n",
        "                messages=[\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            answer = response.content[0].text\n",
        "            logger.info(f\"Generated answer of length {len(answer)}\")\n",
        "            return answer\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generating answer: {str(e)}\")\n",
        "            return f\"I encountered an error while generating the answer: {str(e)}\""
      ],
      "metadata": {
        "id": "VPfobfxxj4x5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#logger = logging.getLogger(__name__)\n",
        "\n",
        "class CSVAgent:\n",
        "    \"\"\"Agent that analyzes CSV data to answer queries.\"\"\"\n",
        "\n",
        "    def __init__(self, csv_path, api_key=api_key):\n",
        "        \"\"\"\n",
        "        Initialize the agent.\n",
        "\n",
        "        Args:\n",
        "            csv_path: Path to the CSV file\n",
        "            api_key: Anthropic API key (optional)\n",
        "        \"\"\"\n",
        "        self.csv_path = csv_path\n",
        "        self.api_key = api_key\n",
        "\n",
        "        #Refactoring Prep:\n",
        "        #self.agent_client = anthropic.Anthropic(api_key=self.api_key)\n",
        "        #self.embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "        # Initialize tools\n",
        "        self.csv_reader = CSVReader()\n",
        "        self.query_analyzer = QueryAnalyzer(api_key=self.api_key)\n",
        "        self.column_selector = ColumnSelector()\n",
        "        self.data_extractor = DataExtractor()\n",
        "        self.text_embedder = TextEmbedder()\n",
        "        self.answer_generator = AnswerGenerator(api_key=self.api_key)\n",
        "\n",
        "        # Load CSV data\n",
        "        self.df, load_message = self.csv_reader.read_csv(csv_path)\n",
        "        logger.info(load_message)\n",
        "\n",
        "        # Store column information\n",
        "        self.columns = list(self.df.columns) if not self.df.empty else []\n",
        "\n",
        "    def process_query(self, query):\n",
        "        \"\"\"\n",
        "        Process a user query and generate an answer.\n",
        "\n",
        "        Args:\n",
        "            query: User query string\n",
        "\n",
        "        Returns:\n",
        "            dict: Response containing answer and processing details\n",
        "        \"\"\"\n",
        "        logger.info(f\"Processing query: {query}\")\n",
        "\n",
        "        if self.df.empty:\n",
        "            return {\n",
        "                \"answer\": \"Unable to analyze the CSV file. Please check the file path and format.\",\n",
        "                \"success\": False\n",
        "            }\n",
        "\n",
        "        try:\n",
        "            # Step 1: Analyze the query\n",
        "            analysis = self.query_analyzer.analyze_query(query, self.columns)\n",
        "\n",
        "            # Step 2: Select relevant columns\n",
        "            selected_columns = self.column_selector.select_columns(self.df, analysis)\n",
        "\n",
        "            # Step 3: Extract relevant data\n",
        "            extracted_data = self.data_extractor.extract_data(self.df, analysis, selected_columns)\n",
        "\n",
        "            # Step 4: Apply text search if needed\n",
        "            if analysis.get(\"requires_text_search\", False) and \"text\" in self.columns:\n",
        "                if analysis.get(\"search_term\") != None:\n",
        "                  search_term = analysis.get(\"search_term\")\n",
        "                else:\n",
        "                  search_term = analysis.get(\"query\")\n",
        "                extracted_data = self.text_embedder.search_similar_texts(\n",
        "                    extracted_data,\n",
        "                    search_term,\n",
        "                    text_column=\"text\",\n",
        "                    top_k=10\n",
        "                )\n",
        "\n",
        "            # Step 5: Generate answer\n",
        "            answer = self.answer_generator.generate_answer(query, extracted_data, analysis)\n",
        "\n",
        "            return {\n",
        "                \"answer\": answer,\n",
        "                \"columns_analyzed\": selected_columns,\n",
        "                \"rows_analyzed\": len(extracted_data),\n",
        "                \"analysis_type\": analysis.get(\"analysis_type\", \"unknown\"),\n",
        "                \"success\": True\n",
        "            }\n",
        "            '''\n",
        "            return{\n",
        "                \"success\":True\n",
        "            }\n",
        "            '''\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing query: {str(e)}\")\n",
        "            return {\n",
        "                \"answer\": f\"An error occurred while processing your query: {str(e)}\",\n",
        "                \"success\": False\n",
        "            }"
      ],
      "metadata": {
        "id": "K95z0uZElINi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_logging():\n",
        "    \"\"\"Set up logging configuration.\"\"\"\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "        handlers=[\n",
        "            logging.FileHandler(\"csv_agent.log\"),\n",
        "            logging.StreamHandler()\n",
        "        ]\n",
        "    )\n",
        "\n",
        "setup_logging()"
      ],
      "metadata": {
        "id": "9gb5jqXeljnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Initialize the agent\n",
        "agent = CSVAgent('alpha_test.csv')"
      ],
      "metadata": {
        "id": "1yUJkufXRekW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "queries examples:\n",
        "\n",
        "\" Please summarise texts about the Caldera Ginger Beer with the number_apperance 4\""
      ],
      "metadata": {
        "id": "K4fAjxz9JXjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \" Please find all beers that have the number_apperance 2.5 and return their names and the summary of texts for these beers\"\n",
        "\n",
        "result = agent.process_query(query)\n",
        "\n",
        "print(f\"\\nAnswer: {result['answer']}\\n\")\n",
        "if result['success']:\n",
        "    print(f\"Analysis type: {result['analysis_type']}\")\n",
        "    print(f\"Columns analyzed: {', '.join(result['columns_analyzed'])}\")\n",
        "    print(f\"Rows analyzed: {result['rows_analyzed']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPdMw_10RW5d",
        "outputId": "752a553c-8a16-4b73-c0ed-dfdd3a3cead1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query_analyzer printout: response:{\n",
            "    \"needed_columns\": [\"string_name\", \"text\"],\n",
            "    \"analysis_type\": \"filtering\",\n",
            "    \"filter_conditions\": {\"number_appearance\": 2.5},\n",
            "    \"aggregation_function\": \"none\",\n",
            "    \"sort_by\": null,\n",
            "    \"sort_order\": null,\n",
            "    \"requires_text_search\": false,\n",
            "    \"search_term\": null,\n",
            "    \"query\": \"Please find all beers that have the number_apperance 2.5 and return their names and the summary of texts for these beers\"\n",
            "}\n",
            "Column Selector Printout: Selected columns: string_name, text, number_appearance\n",
            "Data Extractor Printout:                    string_name  \\\n",
            "0                 Sausa Weizen   \n",
            "14       Caldera Oatmeal Stout   \n",
            "15       Caldera Oatmeal Stout   \n",
            "76            Caldera Pale Ale   \n",
            "190           Caldera Pale Ale   \n",
            "200           Caldera Pale Ale   \n",
            "257           Vas Deferens Ale   \n",
            "272  Old Growth Imperial Stout   \n",
            "\n",
            "                                                  text  number_appearance  \n",
            "0    A lot of foam. But a lot.\\tIn the smell some b...                2.5  \n",
            "14   Brown in color, somewhere between a porter and...                2.5  \n",
            "15   Caldera presents yet another circumstance wher...                2.5  \n",
            "76   Pours a crisp clear pale gold color with a sma...                2.5  \n",
            "190  Golden amber with a big foamy head that just f...                2.5  \n",
            "200  Poured into 12oz straight glass. Poured a clou...                2.5  \n",
            "257  Chilled bottle into a glass. A generous gift f...                2.5  \n",
            "272  Got this one from the Nashvillian, cheers John...                2.5  \n",
            "\n",
            "Answer: Based on the provided data and query analysis, the following beers have a number_appearance of 2.5:\n",
            "\n",
            "1. Sausa Weizen\n",
            "2. Caldera Oatmeal Stout\n",
            "3. Caldera Pale Ale (multiple entries)\n",
            "4. Vas Deferens Ale\n",
            "5. Old Growth Imperial Stout\n",
            "\n",
            "The summary of the text for these beers is as follows:\n",
            "\n",
            "Sausa Weizen:\n",
            "\"A lot of foam. But a lot.\\tIn the smell some banana, and then lactic and tart. Not a good start.\\tQuite dark orange in color, with a lively carbonation (now visible, under the foam).\\tAgain tending to lactic sourness.\\tSame for the taste. With some yeast and banana.\"\n",
            "\n",
            "Caldera Oatmeal Stout:\n",
            "\"Brown in color, somewhere between a porter and a brown ale. Lacking in aroma, but no off stuff. \\t\\tSame with the taste, lacking flavor, complexity, just went with smoothness. No off flavors though, so I can't say this is bad, just unadventurous, especially for Caldera, whom I think is generally underrated. You really have to search to pull anything out of this in terms of the usual chocolate/coffee flavors, really, the only thing I can tell is that the oats did their job, because this is smooth and unoffensive.\\t\\tOther than that, extremely pedestrian.\"\n",
            "\n",
            "Caldera Pale Ale (multiple entries):\n",
            "\"Pours a crisp clear pale gold color with a small head that dissipates rather quickly.\\t\\tHas a bitter hoppy pine and citrus smell. Very pungent for a APA.\\t\\tBitter bite hits right away and fades to a smooth caramel malty biscuity flavor. Not very complex, but good tastes none the less.\\t\\tA very light smooth beer. Very sessionable could def hammer a few of these back.\"\n",
            "\"Golden amber with a big foamy head that just floats on top. Crisp pale malt and floral hop aromas. Flavors of light floral hops with a good malty reflection that combine to make a honey-like finish with a light hop linger. Medium carbonation with a non descript mouthfeel.\\t\\tIt might not look like much but it tastes great although there's also not much to talk about in the mouthfeel. I really liked the honey I tasted in the finish.\"\n",
            "\"Poured into 12oz straight glass. Poured a cloudy yellow, almost the color of pineapple juice, with barely a thin cap of white head that had no retention and minimal lacing.\\t\\tThe aroma was mostly malt forward, a pleasant change from Pales that are basically \"IPA light\", with just enough hops to keep the scent balanced. The flavor was balanced as well, although I thought a little on the bland side. None of the flavors really jumped out at me.\\t\\tThe body was typical for the style but lost some points for being overly sticky. Drinkability was OK, I'll drink it again, but this beer is not on the level of Mirror Pond or Manny's.\\t\\tOverall, a decent enough brew, but nothing special.\"\n",
            "\n",
            "Vas Deferens Ale:\n",
            "\"Chilled bottle into a glass. A generous gift from ramnuts. Thanks, Frank! \\t\\tShared with alfrantzell and chswimmer. \\t\\tA: Pours a clear maroon body with a light tan head. The bubbles fill the tulip but quickly collapse into nothing. \\t\\tS: Not much aroma at all. Some nuts and orange, perhaps. \\t\\tT: This beer initially tasted like sushi. Ted narrowed it down to the seaweed wrap used in rolls, but I am convinced I picked up the soy / cardboard signs of oxidation, as well. As it warmed, those flavors receded a bit, producing flavors of orange, nuts, coffee, and some kind of spice. Somewhat disjointed. \\t\\tM: Oddly thin, with a watery flavor. \\t\\tD: Not enjoyable...\"\n",
            "\n",
            "Old Growth Imperial Stout:\n",
            "\"Got this one from the Nashvillian, cheers John!\\t\\tPours ebony with less than a pinky of mocha colored head.Near zero head retention & lacing\\t\\tS: Chocolate, dark fruit, vanilla, & a touch of Bourbon once warm\\t\\tT: Follows the nose, plus some herbal & woody hop notes, charred grain & dryness up front. More charred grain & Baker's chocolate as this warms. Finishes with a nice chocolate sweetness, herbal hops, a bit vanilla & more charred grain\\t\\tMF: Chewy, oily, subtle carbonation\\t\\tA nice Imp Stout, didn't quite come together for perfection, but a solid offering\"\n",
            "\n",
            "Analysis type: filtering\n",
            "Columns analyzed: string_name, text, number_appearance\n",
            "Rows analyzed: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Refactoring"
      ],
      "metadata": {
        "id": "uCBo5T8lynvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Service Classes\n",
        "class CSVReader:\n",
        "    \"\"\"Tool to read CSV files into pandas dataframes.\"\"\"\n",
        "\n",
        "    def read_csv(self, file_path: str) -> Tuple[pd.DataFrame, str]:\n",
        "        \"\"\"\n",
        "        Read a CSV file into a pandas dataframe.\n",
        "\n",
        "        Args:\n",
        "            file_path: Path to the CSV file\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (dataframe, message)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            df = pd.read_csv(file_path)\n",
        "            columns_info = \", \".join([f\"{col} ({df[col].dtype})\" for col in df.columns])\n",
        "            message = f\"Successfully loaded CSV with {len(df)} rows and {len(df.columns)} columns: {columns_info}\"\n",
        "            logger.info(message)\n",
        "            return df, message\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Error reading CSV file: {str(e)}\"\n",
        "            logger.error(error_msg)\n",
        "            return pd.DataFrame(), error_msg\n",
        "\n",
        "class Workflow:\n",
        "    \"\"\"\n",
        "    Tools to analyze queries and determine information needs;\n",
        "          to determine which columns are needed for a query;\n",
        "          to extract relevant data from the dataframe;\n",
        "          to generate and search text embeddings;\n",
        "          to generate answers using Claude 3.7.\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: Optional[str] = None, model_name: str = \"all-MiniLM-L6-v2\"):\n",
        "        \"\"\"\n",
        "        Initialize with Anthropic API key.\n",
        "\n",
        "        Args:\n",
        "            api_key: Anthropic API key (will use environment variable if None)\n",
        "\n",
        "        Initialize the embedding model.\n",
        "\n",
        "        Args:\n",
        "            model_name: Name of the embedding model\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        self.api_key = api_key or os.environ.get(\"ANTHROPIC_API_KEY\")\n",
        "\n",
        "        #if not self.api_key:\n",
        "            #logger.warning(\"No Anthropic API key provided. Set ANTHROPIC_API_KEY environment variable.\")\n",
        "\n",
        "        self.client = anthropic.Anthropic(api_key=self.api_key)\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "\n",
        "\n",
        "    def analyze_query(query: str, columns: List[str], api_key = api_key) -> Dict[str, Any]:\n",
        "            \"\"\"\n",
        "            Analyze the query to determine information needs.\n",
        "\n",
        "            Args:\n",
        "                query: The user's query\n",
        "                columns: List of available columns in the dataframe\n",
        "\n",
        "            Returns:\n",
        "                Dict with analysis results\n",
        "            \"\"\"\n",
        "            try:\n",
        "                prompt = f\"\"\"\n",
        "                <columns>\n",
        "                {', '.join(columns)}\n",
        "                </columns>\n",
        "\n",
        "                You are an AI assistant that analyzes queries about a dataset. Based on the user query, determine:\n",
        "                1. Which columns from the dataset are needed to answer the query\n",
        "                2. What type of analysis is required (filtering, aggregation, etc.)\n",
        "                3. Whether any specific values or conditions are mentioned\n",
        "\n",
        "                User query: {query}\n",
        "\n",
        "                Respond in JSON format like this:\n",
        "                {{\n",
        "                    \"needed_columns\": [\"column1\", \"column2\"],\n",
        "                    \"analysis_type\": \"one of: filtering, aggregation, sorting, comparison, general_info, semantic_search\",\n",
        "                    \"filter_conditions\": {{\"column_name\": \"filter_value\"}},\n",
        "                    \"aggregation_function\": \"one of: count, sum, average, min, max, none\",\n",
        "                    \"sort_by\": \"column_name or null\",\n",
        "                    \"sort_order\": \"ascending or descending or null\",\n",
        "                    \"requires_text_search\": true/false,\n",
        "                    \"search_term\": \"term to search for in text or null\"\n",
        "                    \"query\": \"original query\"\n",
        "                }}\n",
        "\n",
        "                Make sure all column names exactly match the provided list. If a column is not mentioned or needed, don't include it.\n",
        "                \"\"\"\n",
        "                client = anthropic.Anthropic(api_key)\n",
        "                response = self.client.messages.create(\n",
        "                    model=\"claude-3-haiku-20240307\",\n",
        "                    max_tokens=1000,\n",
        "                    messages=[\n",
        "                        {\"role\": \"user\", \"content\": prompt}\n",
        "                    ]\n",
        "                )\n",
        "\n",
        "                analysis_text = response.content[0].text\n",
        "                print(f'query_analyzer printout: response:{analysis_text}')\n",
        "\n",
        "                # Extract JSON from response\n",
        "                import json\n",
        "                import re\n",
        "\n",
        "                json_match = re.search(r'{[\\s\\S]+}', analysis_text)\n",
        "                if json_match:\n",
        "                    analysis = json.loads(json_match.group(0))\n",
        "                    logger.info(f\"Query analysis completed: {str(analysis)}\")\n",
        "                    return analysis\n",
        "                else:\n",
        "                    logger.error(\"Failed to extract JSON from Claude's response\")\n",
        "                    return {\n",
        "                        \"needed_columns\": columns,\n",
        "                        \"analysis_type\": \"general_info\",\n",
        "                        \"requires_text_search\": False\n",
        "                    }\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error analyzing query: {str(e)}\")\n",
        "                return {\n",
        "                    \"needed_columns\": columns,\n",
        "                    \"analysis_type\": \"general_info\",\n",
        "                    \"requires_text_search\": False\n",
        "                }\n",
        "\n",
        "    def select_columns(df: pd.DataFrame, analysis: Dict[str, Any]) -> List[str]:\n",
        "            \"\"\"\n",
        "            Select columns needed to answer the query.\n",
        "\n",
        "            Args:\n",
        "                df: The dataframe\n",
        "                analysis: Query analysis results\n",
        "\n",
        "            Returns:\n",
        "                List of column names to use\n",
        "            \"\"\"\n",
        "            all_columns = df.columns.tolist()\n",
        "\n",
        "            # Start with columns specified in the analysis\n",
        "            needed_columns = analysis.get(\"needed_columns\", [])\n",
        "\n",
        "            # Always include text column if text search is required\n",
        "            if analysis.get(\"requires_text_search\", False) and \"text\" in all_columns:\n",
        "                if \"text\" not in needed_columns:\n",
        "                    needed_columns.append(\"text\")\n",
        "\n",
        "            # Add filter columns if not already included\n",
        "            filter_conditions = analysis.get(\"filter_conditions\", {})\n",
        "            for col in filter_conditions.keys():\n",
        "                if col in all_columns and col not in needed_columns:\n",
        "                    needed_columns.append(col)\n",
        "\n",
        "            # Add sort column if not already included\n",
        "            sort_by = analysis.get(\"sort_by\")\n",
        "            if sort_by and sort_by in all_columns and sort_by not in needed_columns:\n",
        "                needed_columns.append(sort_by)\n",
        "\n",
        "            # If no columns were determined, return all columns\n",
        "            if not needed_columns:\n",
        "                logger.warning(\"No specific columns determined, using all columns\")\n",
        "                needed_columns = all_columns\n",
        "\n",
        "            logger.info(f\"Selected columns: {', '.join(needed_columns)}\")\n",
        "            print(f\"Column Selector Printout: Selected columns: {', '.join(needed_columns)}\")\n",
        "            return needed_columns\n",
        "\n",
        "    def extract_data(df: pd.DataFrame, analysis: Dict[str, Any], selected_columns: List[str]) -> pd.DataFrame:\n",
        "            \"\"\"\n",
        "            Extract relevant data based on query analysis.\n",
        "\n",
        "            Args:\n",
        "                df: The dataframe\n",
        "                analysis: Query analysis results\n",
        "                selected_columns: Columns to include\n",
        "\n",
        "            Returns:\n",
        "                Filtered dataframe\n",
        "            \"\"\"\n",
        "            try:\n",
        "                # Start with selected columns\n",
        "                result_df = df[selected_columns].copy()\n",
        "\n",
        "                # Apply filtering if specified\n",
        "                filter_conditions = analysis.get(\"filter_conditions\", {})\n",
        "                if filter_conditions and analysis.get(\"analysis_type\") in [\"filtering\", \"comparison\"]:\n",
        "                    for col, value in filter_conditions.items():\n",
        "                        if col in df.columns:\n",
        "                            # Handle different filter types\n",
        "                            if isinstance(value, dict):\n",
        "                                # Range or comparison filter\n",
        "                                if \"min\" in value and \"max\" in value:\n",
        "                                    result_df = result_df[(result_df[col] >= value[\"min\"]) &\n",
        "                                                        (result_df[col] <= value[\"max\"])]\n",
        "                                elif \"min\" in value:\n",
        "                                    result_df = result_df[result_df[col] >= value[\"min\"]]\n",
        "                                elif \"max\" in value:\n",
        "                                    result_df = result_df[result_df[col] <= value[\"max\"]]\n",
        "                                elif \"not_equal\" in value:\n",
        "                                    result_df = result_df[result_df[col] != value[\"not_equal\"]]\n",
        "                            elif isinstance(value, list):\n",
        "                                # List of values\n",
        "                                result_df = result_df[result_df[col].isin(value)]\n",
        "                            else:\n",
        "                                # Simple equality\n",
        "                                result_df = result_df[result_df[col] == value]\n",
        "\n",
        "                # Apply sorting if specified\n",
        "                sort_by = analysis.get(\"sort_by\")\n",
        "                sort_order = analysis.get(\"sort_order\", \"ascending\")\n",
        "                if sort_by and sort_by in result_df.columns:\n",
        "                    ascending = sort_order.lower() != \"descending\"\n",
        "                    result_df = result_df.sort_values(by=sort_by, ascending=ascending)\n",
        "\n",
        "                # Apply aggregation if specified\n",
        "                agg_function = analysis.get(\"aggregation_function\")\n",
        "                if agg_function and agg_function != \"none\" and analysis.get(\"analysis_type\") == \"aggregation\":\n",
        "                    # Determine which column to aggregate\n",
        "                    agg_col = None\n",
        "                    for col in result_df.columns:\n",
        "                        if col != \"text\" and pd.api.types.is_numeric_dtype(result_df[col]):\n",
        "                            agg_col = col\n",
        "                            break\n",
        "\n",
        "                    if agg_col:\n",
        "                        if agg_function == \"count\":\n",
        "                            result_df = result_df.groupby(selected_columns[0])[agg_col].count().reset_index()\n",
        "                        elif agg_function == \"sum\":\n",
        "                            result_df = result_df.groupby(selected_columns[0])[agg_col].sum().reset_index()\n",
        "                        elif agg_function == \"average\":\n",
        "                            result_df = result_df.groupby(selected_columns[0])[agg_col].mean().reset_index()\n",
        "                        elif agg_function == \"min\":\n",
        "                            result_df = result_df.groupby(selected_columns[0])[agg_col].min().reset_index()\n",
        "                        elif agg_function == \"max\":\n",
        "                            result_df = result_df.groupby(selected_columns[0])[agg_col].max().reset_index()\n",
        "\n",
        "                logger.info(f\"Extracted {len(result_df)} rows of data\")\n",
        "                print(f\"Data Extractor Printout: {result_df}\")\n",
        "                return result_df\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error extracting data: {str(e)}\")\n",
        "                # Return original data with selected columns\n",
        "                return df[selected_columns].copy()\n",
        "\n",
        "\n",
        "\n",
        "    def search_similar_texts(df: pd.DataFrame, query: str, model: SentenceTransformer,\n",
        "                            text_column: str = \"text\", top_k: int = 5) -> pd.DataFrame:\n",
        "          \"\"\"\n",
        "            Find texts most similar to the query.\n",
        "\n",
        "            Args:\n",
        "                df: Dataframe with text column\n",
        "                query: Search query\n",
        "                text_column: Column containing text\n",
        "                top_k: Number of results to return\n",
        "\n",
        "            Returns:\n",
        "                Dataframe with most similar texts\n",
        "            \"\"\"\n",
        "            print('I am inside searching simular texts')\n",
        "            if text_column not in df.columns:\n",
        "                logger.error(f\"Text column '{text_column}' not found in dataframe\")\n",
        "                return df\n",
        "\n",
        "            try:\n",
        "                # Generate embeddings\n",
        "                print('trying to generate embeddings')\n",
        "                texts = df[text_column].fillna(\"\").tolist()\n",
        "                print(f'text_embeddings printout: {texts}')\n",
        "                text_embeddings = self.model.encode(texts)\n",
        "                print('got the text embeddings!')\n",
        "                print(f'input query printout: {query} of type {type(query)}')\n",
        "                query_embedding = self.model.encode(query)\n",
        "                print('got the query embeddings!')\n",
        "\n",
        "                # Calculate similarities\n",
        "                similarities = cosine_similarity(\n",
        "                    query_embedding.reshape(1, -1),\n",
        "                    text_embeddings\n",
        "                )[0]\n",
        "\n",
        "                # Add similarity scores to dataframe\n",
        "                result_df = df.copy()\n",
        "                result_df[\"similarity_score\"] = similarities\n",
        "\n",
        "                # Sort by similarity and take top_k\n",
        "                result_df = result_df.sort_values(\"similarity_score\", ascending=False).head(top_k)\n",
        "\n",
        "                logger.info(f\"Found {len(result_df)} similar texts\")\n",
        "                print(f\"search_similar_texts printout: {result_df}\")\n",
        "                return result_df\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error searching similar texts: {str(e)}\")\n",
        "                print(traceback.format_exc())  # Print full traceback\n",
        "                return df\n",
        "\n",
        "    def generate_answer(query: str, data_df: pd.DataFrame, analysis: Dict[str, Any],\n",
        "                      client: anthropic.Anthropic) -> str:\n",
        "            \"\"\"\n",
        "            Generate an answer using Claude 3.7.\n",
        "\n",
        "            Args:\n",
        "                query: User query\n",
        "                data_df: Dataframe with relevant data\n",
        "                analysis: Query analysis results\n",
        "\n",
        "            Returns:\n",
        "                Generated answer\n",
        "            \"\"\"\n",
        "            try:\n",
        "                # Convert dataframe to string representation\n",
        "                data_str = data_df.to_string(index=False) if not data_df.empty else \"No data found\"\n",
        "\n",
        "                # Create prompt for Claude\n",
        "                prompt = f\"\"\"\n",
        "                <data>\n",
        "                {data_str}\n",
        "                </data>\n",
        "\n",
        "                <query_analysis>\n",
        "                {str(analysis)}\n",
        "                </query_analysis>\n",
        "\n",
        "                User query: {query}\n",
        "\n",
        "                Based on the provided data and analysis of the query, please provide a comprehensive answer to the user's question.\n",
        "                Include specific details from the data where appropriate. If the data doesn't contain information needed to answer the query,\n",
        "                state that clearly.\n",
        "\n",
        "                Answer the query directly and concisely. If appropriate, include any relevant statistics from the data.\n",
        "                \"\"\"\n",
        "\n",
        "                response = self.client.messages.create(\n",
        "                    model=\"claude-3-haiku-20240307\",\n",
        "                    max_tokens=2000,\n",
        "                    messages=[\n",
        "                        {\"role\": \"user\", \"content\": prompt}\n",
        "                    ]\n",
        "                )\n",
        "\n",
        "                answer = response.content[0].text\n",
        "                logger.info(f\"Generated answer of length {len(answer)}\")\n",
        "                return answer\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error generating answer: {str(e)}\")\n",
        "                return f\"I encountered an error while generating the answer: {str(e)}\""
      ],
      "metadata": {
        "id": "Yj59PNl219vJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Potential new tools:\n",
        "# 1. Concept Extraction\n",
        "import yake\n",
        "# YAKE Config\n",
        "kw_extractor = yake.KeywordExtractor()\n",
        "language = 'en'\n",
        "max_ngram_size = 2\n",
        "deduplication_threshold = 0.9\n",
        "numOfKeywords = 50\n",
        "#get the document corpus (assumes that the text is in the \"text\" column):\n",
        "\n",
        "def text_input(file = 'alpha_test.csv'):\n",
        "  df = pd.read_csv(file)\n",
        "  df_clean = df[df['text'].apply(lambda x: isinstance(x, str))]\n",
        "  texts = [item.replace(\"\\t\", \" \") for item in df_clean['text']]\n",
        "  return texts\n",
        "\n",
        "\n",
        "#Keyword for the corpus a.k.a Global Concepts\n",
        "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
        "keywords = custom_kw_extractor.extract_keywords(corpus)\n",
        "#select a number of keywords to work with\n",
        "def keywords_number(n = len(keywords), input=keywords):\n",
        "  return [item[0] for item in input[:n]]"
      ],
      "metadata": {
        "id": "iYGTizz7SWZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#logger = logging.getLogger(__name__)\n",
        "\n",
        "class CSVAgent_Rfct:\n",
        "    \"\"\"Agent that analyzes CSV data to answer queries.\"\"\"\n",
        "\n",
        "    def __init__(self, csv_path, api_key=api_key):\n",
        "        \"\"\"\n",
        "        Initialize the agent.\n",
        "\n",
        "        Args:\n",
        "            csv_path: Path to the CSV file\n",
        "            api_key: Anthropic API key (optional)\n",
        "        \"\"\"\n",
        "        self.csv_path = csv_path\n",
        "        self.api_key = api_key\n",
        "\n",
        "        #Refactoring Prep:\n",
        "        self.workflow = Workflow(api_key=self.api_key)\n",
        "\n",
        "        # Initialize tools\n",
        "        self.csv_reader = CSVReader()\n",
        "        self.query_analyzer = QueryAnalyzer(api_key=self.api_key)\n",
        "        self.column_selector = ColumnSelector()\n",
        "        self.data_extractor = DataExtractor()\n",
        "        self.text_embedder = TextEmbedder()\n",
        "        self.answer_generator = AnswerGenerator(api_key=self.api_key)\n",
        "\n",
        "\n",
        "        #Recatoring Prep (new class functions)\n",
        "\n",
        "\n",
        "\n",
        "        # Load CSV data\n",
        "        self.df, load_message = self.csv_reader.read_csv(csv_path)\n",
        "        logger.info(load_message)\n",
        "\n",
        "        # Store column information\n",
        "        self.columns = list(self.df.columns) if not self.df.empty else []\n",
        "\n",
        "    def process_query(self, query):\n",
        "        \"\"\"\n",
        "        Process a user query and generate an answer.\n",
        "\n",
        "        Args:\n",
        "            query: User query string\n",
        "\n",
        "        Returns:\n",
        "            dict: Response containing answer and processing details\n",
        "        \"\"\"\n",
        "        logger.info(f\"Processing query: {query}\")\n",
        "\n",
        "        if self.df.empty:\n",
        "            return {\n",
        "                \"answer\": \"Unable to analyze the CSV file. Please check the file path and format.\",\n",
        "                \"success\": False\n",
        "            }\n",
        "\n",
        "        try:\n",
        "            # Step 1: Analyze the query\n",
        "            analysis = self.workflow.analyze_query(query, self.columns)\n",
        "\n",
        "            # Step 2: Select relevant columns\n",
        "            selected_columns = self.workflow.select_columns(self.df, analysis)\n",
        "\n",
        "            # Step 3: Extract relevant data\n",
        "            extracted_data = self.workflow.extract_data(self.df, analysis, selected_columns)\n",
        "\n",
        "            # Step 4: Apply text search if needed\n",
        "            if analysis.get(\"requires_text_search\", False) and \"text\" in self.columns:\n",
        "                if analysis.get(\"search_term\") != None:\n",
        "                  search_term = analysis.get(\"search_term\")\n",
        "                else:\n",
        "                  search_term = analysis.get(\"query\")\n",
        "                extracted_data = self.workflow.search_similar_texts(\n",
        "                    extracted_data,\n",
        "                    search_term,\n",
        "                    text_column=\"text\",\n",
        "                    top_k=10\n",
        "                )\n",
        "\n",
        "            # Step 5: Generate answer\n",
        "            answer = self.workflow.generate_answer(query, extracted_data, analysis)\n",
        "\n",
        "            return {\n",
        "                \"answer\": answer,\n",
        "                \"columns_analyzed\": selected_columns,\n",
        "                \"rows_analyzed\": len(extracted_data),\n",
        "                \"analysis_type\": analysis.get(\"analysis_type\", \"unknown\"),\n",
        "                \"success\": True\n",
        "            }\n",
        "            '''\n",
        "            return{\n",
        "                \"success\":True\n",
        "            }\n",
        "            '''\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing query: {str(e)}\")\n",
        "            return {\n",
        "                \"answer\": f\"An error occurred while processing your query: {str(e)}\",\n",
        "                \"success\": False\n",
        "            }"
      ],
      "metadata": {
        "id": "Pa6Y2yrbfaWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Initialize the agent\n",
        "agent = CSVAgent_Rfct('alpha_test.csv')"
      ],
      "metadata": {
        "id": "_cTcOBusfyot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Expermienting with \"Thinking\" Tools"
      ],
      "metadata": {
        "id": "klyY4aUH8_Uq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install anthropic"
      ],
      "metadata": {
        "id": "3kC7J8FxqLL6"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "D142_FN9vx4q"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "import anthropic"
      ],
      "metadata": {
        "id": "GsasZb5mkN0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "api_key_openAI = userdata.get('OpenAI')\n",
        "api_key_anthropic = userdata.get('Antropic')\n",
        "api_key_gemini = userdata.get('google')"
      ],
      "metadata": {
        "id": "utn8DLiHF8TC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset to be used:\n",
        "df = pd.read_csv('Mejuri_texts.csv')\n",
        "#Text cleanup\n",
        "def text_input(file = 'alpha_test.csv'):\n",
        "  df = pd.read_csv(file)\n",
        "  df_clean = df[df['Text'].apply(lambda x: isinstance(x, str))]\n",
        "  texts = [item.replace(\"\\t\", \" \") for item in df_clean['Text']]\n",
        "\n",
        "  return texts"
      ],
      "metadata": {
        "id": "KFjMlgPP9HmW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts_cleaned = text_input('Mejuri_texts.csv')\n",
        "corpus = ' '.join(texts_cleaned)"
      ],
      "metadata": {
        "id": "KtoF7Nk09Vh5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test1 = ' '.join(corpus.split()[:200000])"
      ],
      "metadata": {
        "id": "xVGWKyY1kVk9"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Thinking Tool Gemini\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Configure the API key\n",
        "genai.configure(api_key=api_key_gemini)\n",
        "\n",
        "# Initialize the Gemini 2.0 Flash model\n",
        "model_gemini = genai.GenerativeModel('gemini-2.0-flash')\n",
        "\n",
        "generation_config = {\n",
        "    \"temperature\": 0.7,  # Controls the randomness of the output (0.0 - 1.0)\n",
        "    \"max_output_tokens\": 256,  # Limits the maximum number of tokens in the generated response\n",
        "    # You can also include other parameters here like top_p, top_k, stop_sequences, etc.\n",
        "}\n",
        "\n",
        "\n",
        "def think_then_answer_gemini(question: str, model=model_gemini) -> dict:\n",
        "    \"\"\"\n",
        "    Executes a two-step 'think then answer' reasoning pattern.\n",
        "\n",
        "    Parameters:\n",
        "        question (str): The user question.\n",
        "        model (str): The OpenAI model ID (default uses GPT-4 Turbo).\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with 'thoughts' and 'final_answer'.\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Model thinks\n",
        "    think_prompt = f\"\"\"You're a thoughtful assistant. Before answering the user's question, write out your reasoning step by step.\n",
        "Question: {question}\n",
        "Your internal thoughts:\"\"\"\n",
        "\n",
        "    thoughts_response = model.generate_content(\n",
        "        contents=think_prompt,\n",
        "        generation_config=generation_config\n",
        "        )\n",
        "    thoughts = thoughts_response.text\n",
        "\n",
        "    # Step 2: Model answers using its own thoughts\n",
        "    final_prompt = f\"\"\"Based on your reasoning below, provide a clear and final answer to the user's question.\n",
        "Reasoning: {thoughts}\n",
        "Final Answer:\"\"\"\n",
        "\n",
        "    final_response = model.generate_content(\n",
        "        contents=final_prompt,\n",
        "        generation_config=generation_config\n",
        "    )\n",
        "\n",
        "    final_answer = final_response.text\n",
        "\n",
        "    return {\n",
        "        \"thoughts\": thoughts,\n",
        "        \"final_answer\": final_answer\n",
        "    }"
      ],
      "metadata": {
        "id": "kuMr5Ih9diRP"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"could you provide me a summary of the following text: \" + test1\n",
        "answer = think_then_answer_gemini(question)"
      ],
      "metadata": {
        "id": "raqBnPTfvOGc"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOhoeyc5wM0E",
        "outputId": "10787323-541c-44f5-b9b4-a7b600696c50"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'thoughts': \"Okay, I need to summarize a lot of customer feedback about Mejuri stores. Here's how I'll approach it:\\n\\n1.  **Identify Key Themes:** I'll look for recurring topics in the feedback, both positive and negative. This includes aspects like customer service, product availability, store environment, pricing, and the piercing experience.\\n2.  **Quantify (roughly):** I'll note which themes are most frequent to understand what customers are talking about the most.\\n3.  **Summarize Positives:** Condense the positive feedback into a few concise sentences.\\n4.  **Summarize Negatives:** Condense the negative feedback and areas for improvement into a few concise sentences.\\n5.  **Present a Balanced Overview:** Make sure the summary reflects both the strengths and weaknesses mentioned in the text.\\n\\nHere's the summary:\\n\\nMejuri customers generally praise the store's aesthetic, the quality and design of the jewelry, and the helpfulness and friendliness of many staff members, highlighting specific individuals like certain stylists and piercers. They appreciate being able to try on jewelry, especially earrings, and find the pricing reasonable.\\n\\nHowever, several recurring issues emerge: a lack of inventory and size availability in stores\",\n",
              " 'final_answer': 'However, several recurring issues emerge: a lack of inventory and size availability in stores.\\n'}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#summarization\n",
        "question = \"could you provide me a summary of the following text: \" + corpus\n",
        "summary_response = model_gemini.generate_content(\n",
        "    contents=question,\n",
        "    generation_config=generation_config\n",
        ")\n",
        "\n",
        "summary = summary_response.text\n"
      ],
      "metadata": {
        "id": "j_E1zxffx-Bd"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "aSPRCElTykmg",
        "outputId": "f1300a52-2ba5-4679-8da0-9f32da385ff4"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Okay, here's a summary of the provided text focusing on key themes and points:\\n\\n**Overall Sentiment:**\\n\\nThe reviews are largely positive, praising the quality of Mejuri jewelry, the store's aesthetic, and the friendliness and helpfulness of the staff. However, there are also recurring criticisms regarding inventory issues (items out of stock), inconsistent customer service (some stylists are great, others are aloof or rude), and inefficiencies in the in-store checkout process. There are also some concerns about the limited selection of certain metals (white gold, silver), and the lack of clear pricing on displayed items.\\n\\n**Key Positives:**\\n\\n*   **High-Quality Jewelry:**  Many reviewers consistently praise the quality, beauty, and unique style of Mejuri's jewelry, especially the solid gold pieces and minimalist designs.\\n*   **Friendly and Knowledgeable Staff:** A significant number of reviewers highlight positive interactions with specific stylists who were helpful, patient, and provided excellent styling advice. Employees who do piercings are also highly commended. There are reoccurring names that are mentioned as being great to work with.\\n*   **Beautiful Store Aesthetic:** The store's design and ambiance are frequently described as clean, modern, chic, and welcoming.\\n*   **\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#code example: OpenAI\n",
        "\n",
        "# Make sure your API key is set in your environment or replace below\n",
        "openai.api_key = api_key_openAI\n",
        "\n",
        "\n",
        "client = OpenAI(api_key=openai.api_key)  # or set your key directly\n",
        "\n",
        "def think_then_answer(question: str, model: str = \"gpt-4.1-2025-04-14\") -> dict:\n",
        "    \"\"\"\n",
        "    Executes a two-step 'think then answer' reasoning pattern.\n",
        "\n",
        "    Parameters:\n",
        "        question (str): The user question.\n",
        "        model (str): The OpenAI model ID (default uses GPT-4 Turbo).\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with 'thoughts' and 'final_answer'.\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Model thinks\n",
        "    think_prompt = f\"\"\"You're a thoughtful assistant. Before answering the user's question, write out your reasoning step by step.\n",
        "Question: {question}\n",
        "Your internal thoughts:\"\"\"\n",
        "\n",
        "    thoughts_response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": think_prompt}],\n",
        "        temperature=0.7\n",
        "    )\n",
        "    thoughts = thoughts_response.choices[0].message.content.strip()\n",
        "\n",
        "    # Step 2: Model answers using its own thoughts\n",
        "    final_prompt = f\"\"\"Based on your reasoning below, provide a clear and final answer to the user's question.\n",
        "Reasoning: {thoughts}\n",
        "Final Answer:\"\"\"\n",
        "\n",
        "    final_response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": final_prompt}],\n",
        "        temperature=0.7\n",
        "    )\n",
        "    final_answer = final_response.choices[0].message.content.strip()\n",
        "\n",
        "    return {\n",
        "        \"thoughts\": thoughts,\n",
        "        \"final_answer\": final_answer\n",
        "    }"
      ],
      "metadata": {
        "id": "FDlIPjT6AxFX"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#code example: Anthropic\n",
        "\n",
        "\n",
        "client = anthropic.Anthropic(api_key=api_key_anthropic)\n",
        "\n",
        "def think_then_answer_anthropic(question: str, model: str = \"claude-3-7-sonnet-20250219\") -> dict:\n",
        "    \"\"\"\n",
        "    Executes a two-step 'think then answer' reasoning pattern.\n",
        "\n",
        "    Parameters:\n",
        "        question (str): The user question.\n",
        "        model (str): The OpenAI model ID (default uses GPT-4 Turbo).\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with 'thoughts' and 'final_answer'.\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Model thinks\n",
        "    think_prompt = f\"\"\"You're a thoughtful assistant. Before answering the user's question, write out your reasoning step by step.\n",
        "Question: {question}\n",
        "Your internal thoughts:\"\"\"\n",
        "\n",
        "    thoughts_response = client.messages.create(\n",
        "        model=model,\n",
        "        max_tokens=4096,\n",
        "        messages=[{\"role\": \"user\", \"content\": think_prompt}]\n",
        "    )\n",
        "\n",
        "    thoughts = thoughts_response.content\n",
        "\n",
        "    # Step 2: Model answers using its own thoughts\n",
        "    final_prompt = f\"\"\"Based on your reasoning below, provide a clear and final answer to the user's question.\n",
        "Reasoning: {thoughts}\n",
        "Final Answer:\"\"\"\n",
        "\n",
        "    final_response = client.messages.create(\n",
        "        model=model,\n",
        "        max_tokens=4096,\n",
        "        messages=[{\"role\": \"user\", \"content\": final_prompt}],\n",
        "        temperature=0.7\n",
        "    )\n",
        "    final_answer = final_response.content\n",
        "\n",
        "    return {\n",
        "        \"thoughts\": thoughts,\n",
        "        \"final_answer\": final_answer\n",
        "    }"
      ],
      "metadata": {
        "id": "-uxt1FAJqs7z"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"could you provide me a summary of the following text: \" + test1"
      ],
      "metadata": {
        "id": "TbIL9RVPmad6"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer = think_then_answer_anthropic(question)"
      ],
      "metadata": {
        "id": "Cnbqg4dGnTuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(answer['final_answer'][0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsH7SrCht6GW",
        "outputId": "779cada4-9005-435f-dbf8-cdabeff6301d"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the user's detailed feedback about their experiences shopping at Mejuri stores, the key themes and takeaways I would summarize are:\n",
            "\n",
            "Staff Responsiveness and Customer Service:\n",
            "The user had mixed experiences with Mejuri's store staff. While some employees were described as very helpful and friendly, the user also encountered less attentive or responsive staff at times. Maintaining a high level of customer service across all store locations appears to be an area for potential improvement.\n",
            "\n",
            "Product Selection and Inventory Availability:\n",
            "The user commented on the availability and selection of Mejuri's products, noting both positive experiences finding the items they were looking for, as well as instances where certain products were out of stock or unavailable. Ensuring consistent product inventory and selection could help enhance the overall shopping experience.\n",
            "\n",
            "Ease of Browsing and Checkout:\n",
            "The user provided feedback on the in-store browsing and checkout processes. While the checkout experience was generally smooth, the user suggested that having more detailed pricing information available in the stores could improve the browsing and decision-making process.\n",
            "\n",
            "Store Environment and Aesthetics:\n",
            "The user commented on the overall atmosphere and layout of the Mejuri stores, describing a pleasant and visually appealing shopping environment. Maintaining this high-quality store experience appears to be a strength of the brand.\n",
            "\n",
            "In summary, the user's feedback offers Mejuri valuable insights to consider as they look to enhance the in-store shopping experience for customers. By addressing areas like staff training, inventory management, and store design, Mejuri can work to consistently deliver the positive experiences the user described in their most favorable store visits.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Thinking tools code refactoring**"
      ],
      "metadata": {
        "id": "RSFtJdkXLbxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class LLMProvider(ABC):\n",
        "    def __init__(self, model_name: str, api_key: str, client = None):\n",
        "        self.client = client\n",
        "        self.model_name = model_name\n",
        "        self.api_key = api_key\n",
        "\n",
        "    @abstractmethod\n",
        "    def call_model(self, prompt: str) -> object:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def extract_text(self, response: object) -> str:\n",
        "        pass"
      ],
      "metadata": {
        "id": "CXv0sYYRLZx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OpenAIProvider(LLMProvider):\n",
        "    def __init__(self, model_name: str, api_key: str):\n",
        "        super().__init__(client, model_name, api_key, client)\n",
        "\n",
        "    def call_model(self, prompt: str) -> dict:\n",
        "        # You can now use self.model_name and self.api_key\n",
        "        response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.7)\n",
        "        return response\n",
        "\n",
        "    def extract_text(self, response: dict) -> str:\n",
        "        return response.choices[0].message.content.strip()"
      ],
      "metadata": {
        "id": "AsTmxsQfL0D4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openAI_gpt_4_1 = OpenAIProvider(\"gpt-4.1-2025-04-14\", api_key_openAI, OpenAI(api_key=openai.api_key))"
      ],
      "metadata": {
        "id": "W4tC58q-ZO50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Raptor Implementation"
      ],
      "metadata": {
        "id": "jU63_qreQs_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install llama-index-packs-raptor\n",
        "!pip install llama-index"
      ],
      "metadata": {
        "id": "RN1lDeympfFf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from llama_index.packs.raptor import RaptorPack\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core import Document"
      ],
      "metadata": {
        "id": "vbmfUIUo9L8w"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "IJLQq-dHCZ1j"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_list = texts_cleaned[:400]\n",
        "documents = [Document(text=t) for t in text_list]"
      ],
      "metadata": {
        "id": "u4tLGrUMFOXB"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key_openAI  # Replace with your actual key"
      ],
      "metadata": {
        "id": "7yvrjG3mGl3v"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raptor_pack = RaptorPack(\n",
        "    documents,\n",
        "    embed_model=OpenAIEmbedding(\n",
        "        model=\"text-embedding-3-small\"\n",
        "    ),  # used for embedding clusters\n",
        "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1),  # used for generating summaries\n",
        "    #vector_store=vector_store,  # used for storage\n",
        "    similarity_top_k=2,  # top k for each layer, or overall top-k for collapsed\n",
        "    mode=\"collapsed\",  # sets default mode\n",
        "    transformations=[\n",
        "        SentenceSplitter(chunk_size=400, chunk_overlap=50)\n",
        "    ],  # transformations applied for ingestion\n",
        ")"
      ],
      "metadata": {
        "id": "Q_7NkUi6ulzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nodes = raptor_pack.run(\"What customers think about Mejuri?\", mode=\"collapsed\")\n",
        "print(len(nodes))\n",
        "nodes[0].text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "QAufXLqYK8ab",
        "outputId": "5cef693e-46c0-4be6-eb96-79655ca96983"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Customers have shared their experiences with Mejuri, a jewelry brand, expressing satisfaction with the products and customer service. Many customers appreciate the helpful and kind staff, with some even sharing personal stories and connections with the stylists. However, there have been instances where loyal customers felt uncomfortable due to perceived profiling by security. While some customers praise Mejuri for its inclusivity and accessibility, others have expressed frustration over being asked to wait outside the store, feeling it goes against the brand's image of being relatable and unpretentious. Despite these issues, overall, customers seem to value their interactions with Mejuri and the quality of the jewelry offered.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models Connectivity Tests"
      ],
      "metadata": {
        "id": "FtcJ-rPoOFG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Anthropic client\n",
        "client = anthropic.Anthropic(api_key=api_key_anthropic)\n",
        "\n",
        "try:\n",
        "    # Make a simple API request\n",
        "    response = client.messages.create(\n",
        "        model=\"claude-3-haiku-20240307\",\n",
        "        max_tokens=50,\n",
        "        messages=[{\"role\": \"user\", \"content\": \"Hello! Can you confirm you're working?\"}]\n",
        "    )\n",
        "\n",
        "    # Print the response\n",
        "    print(\"Claude's Response:\", response.content)\n",
        "\n",
        "except anthropic.APIStatusError as e:\n",
        "    print(f\"API returned an error: {e}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "e0NCjdsuOMKJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38ea94a9-e0e4-4bcc-f969-16ade17ec78d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Claude's Response: [TextBlock(citations=None, text=\"Yes, I'm working and ready to assist you! How can I help you today?\", type='text')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gemini Model**"
      ],
      "metadata": {
        "id": "LnL7wf6aZhej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "# Replace with your actual Gemini API key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_API_KEY\"\n",
        "\n",
        "# Initialize the Gemini client\n",
        "client = genai.Client()\n",
        "\n",
        "# Specify the Gemini 2.0 Flash model\n",
        "model = client.models.get(\"gemini-2.0-flash\")\n",
        "\n",
        "# Create a prompt\n",
        "prompt = \"How does AI work?\"\n",
        "\n",
        "# Generate content using the model\n",
        "response = model.generate_content(prompt)\n",
        "\n",
        "# Print the response\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "8eHjLrt4Znpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install --upgrade google-generativeai"
      ],
      "metadata": {
        "id": "ANiG3DOMo8d-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "# Configure the API key\n",
        "genai.configure(api_key=api_key_gemini) # Replace \"YOUR_API_KEY\" with your actual API key\n",
        "\n",
        "# Initialize the Gemini 2.0 Flash model\n",
        "model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "\n",
        "generation_config = {\n",
        "    \"temperature\": 0.7,  # Controls the randomness of the output (0.0 - 1.0)\n",
        "    \"max_output_tokens\": 256,  # Limits the maximum number of tokens in the generated response\n",
        "    # You can also include other parameters here like top_p, top_k, stop_sequences, etc.\n",
        "}\n",
        "\n",
        "\n",
        "# Prepare the prompt for the model\n",
        "prompt = \"Write a short poem about the ocean.\"\n",
        "\n",
        "# Generate content using the model\n",
        "response = model.generate_content(\n",
        "    contents=prompt,\n",
        "    #safety_settings=safety_settings,\n",
        "    generation_config=generation_config\n",
        ")\n",
        "\n",
        "# Print the generated text\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "1FOvyJH3Z_BW",
        "outputId": "8c259e3b-84fb-46e4-ec38-604c6fcb783a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blue giant, breathing deep,\n",
            "Secrets held in currents sleep.\n",
            "Salty kiss upon the shore,\n",
            "Waves that crash and ever roar.\n",
            "\n",
            "Sunlight dances on the foam,\n",
            "A wild and watery home.\n",
            "Mysteries within its heart,\n",
            "A world forever set apart.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}