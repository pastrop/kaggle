{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCf8fQrfBxQGRu+63UP2VT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pastrop/kaggle/blob/master/GRPO_Study_Ntbk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRPO Study Notebook"
      ],
      "metadata": {
        "id": "HOHFwsLQ2koP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate Mulptiple responses from the model for the reward model training"
      ],
      "metadata": {
        "id": "wbrNI1Mu28mW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptbLfhlE2dsM"
      },
      "outputs": [],
      "source": [
        "# Load GPT model & tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Define a question (prompt)\n",
        "prompt = \"What are the benefits of exercise?\"\n",
        "\n",
        "# Tokenize the input\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# Generate multiple responses using sampling\n",
        "num_responses = 5  # Generate 5 different completions\n",
        "responses = model.generate(\n",
        "    input_ids,\n",
        "    max_length=50,\n",
        "    do_sample=True,  # Enables sampling instead of greedy decoding\n",
        "    top_k=50,  # Consider top 50 tokens at each step\n",
        "    top_p=0.9,  # Nucleus sampling: keeps top tokens contributing to 90% probability\n",
        "    temperature=0.7,  # Controls randomness (lower = more deterministic)\n",
        "    num_return_sequences=num_responses  # Generates multiple responses\n",
        ")\n",
        "\n",
        "# Decode and print responses\n",
        "decoded_responses = [tokenizer.decode(output, skip_special_tokens=True) for output in responses]\n",
        "for i, response in enumerate(decoded_responses):\n",
        "    print(f\"Response {i+1}: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example: Using a Reward Model to Score Responses\n",
        "Assume we have a pretrained reward model that scores responses from 0 to 1, where:\n",
        "\n",
        "1.0 = Excellent\n",
        "0.0 = Poor"
      ],
      "metadata": {
        "id": "GP6_8i_W3PPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RewardModel(nn.Module):\n",
        "    \"\"\"Simple reward model that scores responses.\"\"\"\n",
        "    def __init__(self, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1),  # Output: a single score per response\n",
        "            nn.Sigmoid()  # Normalize score between 0 and 1\n",
        "        )\n",
        "\n",
        "    def forward(self, response_embedding):\n",
        "        return self.fc(response_embedding)\n",
        "\n",
        "# Instantiate reward model\n",
        "reward_model = RewardModel(embedding_dim=768)  # Assuming GPT's embeddings\n",
        "\n",
        "# Convert responses to embeddings\n",
        "response_inputs = tokenizer(decoded_responses, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "response_embeddings = model.transformer.wte(response_inputs[\"input_ids\"]).mean(dim=1)  # Averaging token embeddings\n",
        "\n",
        "# Score each response\n",
        "reward_scores = reward_model(response_embeddings).squeeze()\n",
        "\n",
        "# Print scores\n",
        "for i, (response, score) in enumerate(zip(decoded_responses, reward_scores.tolist())):\n",
        "    print(f\"Response {i+1}: {response} | Score: {score:.3f}\")\n"
      ],
      "metadata": {
        "id": "dzNTOCh-3YnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Policy\n",
        "Train the Policy with GRPO\n",
        "Train the policy network to predict higher scores for better responses.\n",
        "Update it using a reinforcement learning algorithm like GRPO.\n",
        "Example: Training the Policy Network"
      ],
      "metadata": {
        "id": "YWi9qxQf4NC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy_optimizer = optim.Adam(policy_net.parameters(), lr=3e-4)\n",
        "\n",
        "for batch in training_data:  # Assume training_data is a dataset of responses + rewards\n",
        "    response_embeddings = model.transformer.wte(batch[\"response_ids\"])\n",
        "\n",
        "    predicted_scores = policy_net(response_embeddings.mean(dim=1))\n",
        "    advantage = batch[\"true_rewards\"] - predicted_scores.detach()  # Compute advantage\n",
        "\n",
        "    policy_loss = -torch.min(\n",
        "        predicted_scores * advantage,\n",
        "        torch.clamp(predicted_scores, 0.8, 1.2) * advantage\n",
        "    ).mean()\n",
        "\n",
        "    policy_optimizer.zero_grad()\n",
        "    policy_loss.backward()\n",
        "    policy_optimizer.step()\n"
      ],
      "metadata": {
        "id": "NG7gOqnA4P3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the Policy for Action Selection\n",
        "Once the policy is trained, it can be used to select actions in the environment.\n",
        "\n",
        "Example: Action Selection in a Trained Policy"
      ],
      "metadata": {
        "id": "0XWAXA034vj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def select_action(policy_net, state):\n",
        "    \"\"\"Selects an action based on the trained policy.\"\"\"\n",
        "    with torch.no_grad():  # No gradients needed for inference\n",
        "        action_probs = policy_net(state)\n",
        "        action = torch.multinomial(action_probs, 1)  # Sample from the policy distribution\n",
        "    return action.item()\n",
        "\n",
        "# Example usage:\n",
        "state = torch.rand((1, 4))  # Example state (assuming 4D input)\n",
        "action = select_action(policy_net, state)\n",
        "print(f\"Selected Action: {action}\")\n"
      ],
      "metadata": {
        "id": "XmvP6zM-4ydV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning an LLM with a Policy Network\n",
        "Below is a simplified PyTorch example that fine-tunes an LLM (e.g., GPT) using a trained policy."
      ],
      "metadata": {
        "id": "Gnvcqff14elR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load the LLM (e.g., GPT-2 for simplicity)\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Define a simple policy network that scores responses\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)  # Output: scalar score for each response\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Instantiate the policy network\n",
        "policy_net = PolicyNetwork(input_dim=768)  # GPT's embedding size\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Simulated training batch\n",
        "texts = [\"What is the capital of France?\", \"How does photosynthesis work?\"]\n",
        "inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# Step 1: Generate responses from the LLM\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(inputs[\"input_ids\"], max_length=50)\n",
        "\n",
        "# Step 2: Get response embeddings (used as input to policy network)\n",
        "response_embeddings = model.transformer.wte(outputs)  # Word token embeddings\n",
        "response_scores = policy_net(response_embeddings.mean(dim=1))  # Score each response\n",
        "\n",
        "# Step 3: Compute policy loss (GRPO-style clipped loss)\n",
        "baseline = response_scores.mean()  # Baseline for advantage calculation\n",
        "advantage = response_scores - baseline  # Compute advantage function\n",
        "policy_loss = -torch.min(response_scores * advantage, torch.clamp(response_scores, 0.8, 1.2) * advantage).mean()\n",
        "\n",
        "# Step 4: Backpropagate the loss to fine-tune the LLM\n",
        "optimizer.zero_grad()\n",
        "policy_loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "print(f\"Policy Loss: {policy_loss.item()}\")\n"
      ],
      "metadata": {
        "id": "3_AJNkb-4gwW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}