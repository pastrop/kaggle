{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/R9aMxCRyUFL9OyZfDqPB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pastrop/kaggle/blob/master/GRPO_Study_Ntbk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRPO Study Notebook"
      ],
      "metadata": {
        "id": "HOHFwsLQ2koP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is notebook is created purely as an example.  Don't expect a production quality code here.  This is a byproduct of me reading the Deepseek paper and understanding how the GPRO agorithm works.  I looked at other people's code for it and found it a bit vague  \n",
        "\n",
        "**Setup**\n",
        "1.   LLM is used to create training examples for training the Policy (I am using GPT2 and facebook's opt-1.3b just bcs it is easy without GPUs)\n",
        "2.   There is a pretrained Reward Model.  I Am using a fully trained sentiment classfier bcs I need smth quick and easy.\n",
        "3. The policy is a trivial 2 layer feed-forward net\n",
        "\n"
      ],
      "metadata": {
        "id": "rDiuw2Sizddk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install torch transformers"
      ],
      "metadata": {
        "id": "AHgvlWhmTJuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning LLM with the GRPO"
      ],
      "metadata": {
        "id": "LV7twYkUJe3x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "generating training examples using LLM"
      ],
      "metadata": {
        "id": "8-cj1WxfHBlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import torch.nn as nn # Import the torch.nn module and alias it as nn\n",
        "import torch.optim as optim # Import the torch.optim module and alias it as optim\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "AzBSza1nG5U-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using a trained sentiment model as a Reward Model**"
      ],
      "metadata": {
        "id": "9f7gYWNvThy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate responses function for Policy Network training\n",
        "def generate_responses(prompt, num_responses=5):\n",
        "    # Load GPT model & tokenizer\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n",
        "\n",
        "    # Set pad token explicitly\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Tokenize the input with padding and attention mask\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)# max_length is specific to facebook/opt-1.3b\n",
        "    # Generate multiple responses using sampling\n",
        "    responses = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_length=50,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        top_p=0.9,\n",
        "        temperature=0.7,\n",
        "        num_return_sequences=num_responses,\n",
        "        pad_token_id=tokenizer.pad_token_id  # Ensures padding works correctly\n",
        "    )\n",
        "\n",
        "    # Decode responses (remove prompt from output)\n",
        "    decoded_responses = [\n",
        "        tokenizer.decode(output[len(inputs[\"input_ids\"][0]):], skip_special_tokens=True).strip()\n",
        "        for output in responses\n",
        "    ]\n",
        "\n",
        "    return decoded_responses"
      ],
      "metadata": {
        "id": "34uQ_6D44365"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test for generate_reponses()\n",
        "prompts = [\"What people like or dislike about working out?\",\n",
        "           \"What people like or dislike about deserts?\",\n",
        "           \"Whatpeople like or dislike about traveling?\",\n",
        "           \"What people like or dislike about New York City?\",\n",
        "           \"What people like or dislike about living in France?\"]\n",
        "responses={}\n",
        "for prompt in prompts:\n",
        "    responses[prompt] = generate_responses(prompt)"
      ],
      "metadata": {
        "id": "qVVeUe8jqocm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reward Model**"
      ],
      "metadata": {
        "id": "u0j2lz6RBTpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Load sentiment classifier\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "reward_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "reward_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def get_reward_scores(responses):\n",
        "    \"\"\"Scores responses using a sentiment classifier.\"\"\"\n",
        "    inputs = reward_tokenizer(responses, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    with torch.no_grad():  # No gradient calculation needed\n",
        "        outputs = reward_model(**inputs)\n",
        "    scores = torch.softmax(outputs.logits, dim=1)[:, 1]  # Probability of \"positive\" class\n",
        "    return scores.detach()  # Detach to prevent computation graph tracking"
      ],
      "metadata": {
        "id": "8U-D6-JxTgBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test Run for the Reward Model (This is a test cell just to make sure get_reward_scores works properly)\n",
        "'''\n",
        "#Example responses\n",
        "responses = [\n",
        "    \"I love exercising, it makes me feel amazing!\",\n",
        "    \"Exercise is okay, but it's tiring.\",\n",
        "    \"I hate exercising, it's the worst!\"\n",
        "]\n",
        "'''\n",
        "test_responses = responses[\"What people like or dislike about living in France?\"]\n",
        "# Get reward scores\n",
        "reward_scores = get_reward_scores(test_responses)\n",
        "\n",
        "# Print scores\n",
        "for i, (test_response, score) in enumerate(zip(test_responses, reward_scores)):\n",
        "    print(f\"Response {i+1}: {test_response}\\nScore: {score.item():.3f}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0fb9VQETpoZ",
        "outputId": "97882220-045e-45e4-c857-1362affe6f78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response 1: I don't know, I don't have any French friends and I don't go there that often.\n",
            "Score: 0.011\n",
            "\n",
            "Response 2: I don't live in France but I have family there and I have heard many different things. One of the main things I've heard is that the French are a very laid back people.\n",
            "Score: 0.999\n",
            "\n",
            "Response 3: Iâ€™ve been living in France for a little over a year now. I like it a lot. I think the French people are friendly, helpful, and patient. I like\n",
            "Score: 1.000\n",
            "\n",
            "Response 4: You might be surprised to learn that France is a popular destination for people from all over the world. There are many reasons why people from all over the world love France, but Iï¿½\n",
            "Score: 1.000\n",
            "\n",
            "Response 5: I like living in France. I think the people are friendly and the French language is fun to learn.   I dislike the weather. The heat and humidity are so bad. I just\n",
            "Score: 0.004\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "training the target model"
      ],
      "metadata": {
        "id": "XUjSTRAN09sk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load GPT-2 model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Set padding token and enforce left-padding\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"  # Ensures padding is on the left for GPT-2\n",
        "\n",
        "# Optimizer for fine-tuning GPT-2\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Simulated training batch\n",
        "texts = [\"Do you like living in Paris?\", \"What are advantages and disadvantages of the paleo diet?\"]\n",
        "inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# Ensure device compatibility (if using GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
        "\n",
        "# Step 1: Generate responses WITH GRADIENT TRACKING\n",
        "outputs = model.generate(\n",
        "    inputs[\"input_ids\"],\n",
        "    attention_mask=inputs[\"attention_mask\"],  # ðŸ”¥ Fix missing attention mask\n",
        "    max_length=50,\n",
        "    return_dict_in_generate=True,\n",
        "    output_hidden_states=True  # âœ… Ensures we get hidden states\n",
        ")\n",
        "\n",
        "\n",
        "# Step 2: Compute response scores using the reward network\n",
        "response_scores = get_reward_scores(outputs)\n",
        "\n",
        "# Step 3: Compute policy loss (GRPO-style clipped loss)\n",
        "baseline = response_scores.mean()\n",
        "advantage = response_scores - baseline\n",
        "policy_loss = -torch.min(response_scores * advantage, torch.clamp(response_scores, 0.8, 1.2) * advantage).mean()\n",
        "\n",
        "# ðŸ”¥ Step 4: Backpropagate loss INTO GPT-2 parameters ðŸ”¥\n",
        "optimizer.zero_grad()\n",
        "policy_loss.backward()  # âœ… This now updates GPT-2's parameters\n",
        "optimizer.step()\n",
        "\n",
        "print(f\"Policy Loss: {policy_loss.item()}\")"
      ],
      "metadata": {
        "id": "3cNicYVp06dR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Added KL-term"
      ],
      "metadata": {
        "id": "yCyd6DtpOudt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Assume models and tokenizer are initialized\n",
        "# target_model: trainable GPT-2 model\n",
        "# ref_model: frozen GPT-2 model (base)\n",
        "# reward_model: function taking (query, response) -> reward (float or torch.Tensor)\n",
        "# tokenizer: GPT2Tokenizer\n",
        "# device: torch.device(\"cuda\" or \"cpu\")\n",
        "\n",
        "target_model.to(device)\n",
        "ref_model.to(device)\n",
        "ref_model.eval()\n",
        "\n",
        "optimizer = Adam(target_model.parameters(), lr=5e-6)\n",
        "kl_beta = 0.1  # KL divergence penalty coefficient\n",
        "num_epochs = 3\n",
        "batch_size = 4\n",
        "num_samples_per_query = 5\n",
        "\n",
        "def generate_response(model, prompt, max_length=50, num_samples=1):\n",
        "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(device)\n",
        "    responses = []\n",
        "    for _ in range(num_samples):\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            do_sample=True,\n",
        "            max_length=max_length,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            top_k=50,\n",
        "            top_p=0.95\n",
        "        )\n",
        "        responses.append(output[0])\n",
        "    return responses\n",
        "\n",
        "def compute_log_probs(model, input_ids):\n",
        "    # Get log-probs for next-token prediction\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, labels=input_ids)\n",
        "        log_probs = -outputs.loss * input_ids.size(1)  # Approximate total log-prob\n",
        "    return log_probs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for query_batch in dataloader:  # assumes `dataloader` yields batches of queries\n",
        "        optimizer.zero_grad()\n",
        "        batch_loss = 0.0\n",
        "\n",
        "        for query in query_batch:\n",
        "            prompt_text = query[\"text\"]\n",
        "            responses = generate_response(target_model, prompt_text, num_samples=num_samples_per_query)\n",
        "\n",
        "            rewards = []\n",
        "            log_probs_target = []\n",
        "            log_probs_ref = []\n",
        "\n",
        "            for response in responses:\n",
        "                prompt_response_ids = response.unsqueeze(0)  # (1, seq_len)\n",
        "\n",
        "                # Reward\n",
        "                decoded_response = tokenizer.decode(response, skip_special_tokens=True)\n",
        "                reward = reward_model(prompt_text, decoded_response)\n",
        "                rewards.append(reward)\n",
        "\n",
        "                # Log-probs from target and reference\n",
        "                log_prob_target = compute_log_probs(target_model, prompt_response_ids)\n",
        "                log_prob_ref = compute_log_probs(ref_model, prompt_response_ids)\n",
        "\n",
        "                log_probs_target.append(log_prob_target)\n",
        "                log_probs_ref.append(log_prob_ref)\n",
        "\n",
        "            # Convert to tensors\n",
        "            rewards = torch.tensor(rewards, dtype=torch.float32, device=device)\n",
        "            log_probs_target = torch.stack(log_probs_target)\n",
        "            log_probs_ref = torch.stack(log_probs_ref)\n",
        "\n",
        "            # Compute KL divergence\n",
        "            kl_divs = log_probs_target - log_probs_ref  # this is actually reverse KL for tokens\n",
        "\n",
        "            # GRPO Loss: Negative reward-weighted log-probs + KL penalty\n",
        "            # Use softmax baseline normalization for variance reduction (optional)\n",
        "            rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-8)\n",
        "\n",
        "            loss = -torch.mean(rewards * log_probs_target) + kl_beta * torch.mean(kl_divs)\n",
        "            batch_loss += loss\n",
        "\n",
        "        batch_loss /= batch_size\n",
        "        batch_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Loss: {batch_loss.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "ltEhSw4G06VO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "npauCN7G06LF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qQg6gTgC05yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments"
      ],
      "metadata": {
        "id": "yuo9D0Th1ks-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "training policy network with KL term"
      ],
      "metadata": {
        "id": "0_WCInxBYh2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load GPT model & tokenizer\n",
        "gpt_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "gpt_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "gpt_tokenizer.pad_token = gpt_tokenizer.eos_token # Use eos_token as pad_token for gpt2\n",
        "\n",
        "'''\n",
        "Logits Reshaping to avoid the dimension mismatch error:\n",
        "\t1.\tReshape new_logits:Inside the compute_kl_divergence function, the new_logits tensor is reshaped using unsqueeze(-1)\n",
        "    and repeat to match the dimensions of the original_logits. This ensures that both tensors have compatible shapes for calculating the\n",
        "    KL divergence.\n",
        "\t2.\tApply softmax after reshaping:Â The softmax function is applied toÂ new_logitsÂ afterÂ it has been reshaped t\n",
        "    o match the dimensions ofÂ original_logits. This ensures that the KL divergence is calculated between probability distributions\n",
        "    with the correct shape.\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "def compute_kl_divergence(original_logits, new_logits):\n",
        "    \"\"\"Compute KL divergence between the original GPT-2 outputs and the policy network outputs.\"\"\"\n",
        "    # Reshape new_logits to match original_logits\n",
        "    new_logits = new_logits.unsqueeze(-1).repeat(1, original_logits.shape[1])\n",
        "\n",
        "    original_probs = F.softmax(original_logits, dim=-1)\n",
        "    new_probs = F.softmax(new_logits, dim=-1)  # Apply softmax to new_logits after reshaping\n",
        "    kl_div = F.kl_div(new_probs.log(), original_probs, reduction=\"batchmean\")  # KL(P || Q)\n",
        "    return kl_div\n",
        "\n",
        "def train_policy_network_with_kl(responses, reward_scores, beta=0.01):\n",
        "    \"\"\"Updates the policy network using GRPO-style policy gradients with KL divergence.\"\"\"\n",
        "\n",
        "    # Get response embeddings from GPT-2\n",
        "    response_inputs = gpt_tokenizer(responses, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    # Ensure all token IDs are within the model's vocabulary\n",
        "    response_inputs[\"input_ids\"] = response_inputs[\"input_ids\"].clip(0, gpt_model.config.vocab_size -1)\n",
        "    response_embeddings = gpt_model.transformer.wte(response_inputs[\"input_ids\"]).mean(dim=1)  # Mean token embedding\n",
        "\n",
        "    # Get logits from the original GPT-2 model (before policy updates)\n",
        "    with torch.no_grad():\n",
        "        original_logits = gpt_model(response_inputs[\"input_ids\"]).logits.mean(dim=1)\n",
        "\n",
        "    # Get predicted scores from the policy network\n",
        "    predicted_scores = policy_net(response_embeddings).squeeze()\n",
        "\n",
        "    # Compute Advantage (A = reward - baseline)\n",
        "    baseline = reward_scores.mean()\n",
        "    advantage = reward_scores - baseline\n",
        "\n",
        "    # Compute policy loss (GRPO-style clipped loss)\n",
        "    clip_ratio = 0.2\n",
        "    policy_loss = -torch.min(\n",
        "        predicted_scores * advantage,\n",
        "        torch.clamp(predicted_scores, 1 - clip_ratio, 1 + clip_ratio) * advantage\n",
        "    ).mean()\n",
        "\n",
        "    # Compute KL divergence loss\n",
        "    new_logits = policy_net(response_embeddings)  # Policy's logits\n",
        "    kl_loss = compute_kl_divergence(original_logits, new_logits) # Calculate KL divergence\n",
        "\n",
        "    # Final loss with KL penalty\n",
        "    total_loss = policy_loss + beta * kl_loss\n",
        "\n",
        "    # Backpropagation\n",
        "    policy_optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    policy_optimizer.step()\n",
        "\n",
        "    return total_loss.item(), kl_loss.item()"
      ],
      "metadata": {
        "id": "7bAoOltuI95w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop with KL regularization\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    training_responses = responses[prompts[epoch]]\n",
        "    reward_scores = get_reward_scores(training_responses)\n",
        "    loss, kl = train_policy_network_with_kl(training_responses, reward_scores)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {loss:.4f}, KL Divergence: {kl:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hbB0Z86Ybth",
        "outputId": "eafeb668-8982-4f33-bc20-0c168cc42f49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 - Loss: 0.0732, KL Divergence: 4.6478\n",
            "Epoch 2/5 - Loss: 0.1697, KL Divergence: 4.5876\n",
            "Epoch 3/5 - Loss: 0.1655, KL Divergence: 4.6667\n",
            "Epoch 4/5 - Loss: 0.1732, KL Divergence: 4.2839\n",
            "Epoch 5/5 - Loss: 0.2287, KL Divergence: 4.7516\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the target model using the Policy Network"
      ],
      "metadata": {
        "id": "VnsaoHaRklJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load GPT-2 model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Set padding token and enforce left-padding\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"  # Ensures padding is on the left for GPT-2\n",
        "\n",
        "# Optimizer for fine-tuning GPT-2\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Simulated training batch\n",
        "texts = [\"Do you like living in Paris?\", \"What are advantages and disadvantages of the paleo diet?\"]\n",
        "inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# Ensure device compatibility (if using GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
        "\n",
        "# Step 1: Generate responses WITH GRADIENT TRACKING\n",
        "outputs = model.generate(\n",
        "    inputs[\"input_ids\"],\n",
        "    attention_mask=inputs[\"attention_mask\"],  # ðŸ”¥ Fix missing attention mask\n",
        "    max_length=50,\n",
        "    return_dict_in_generate=True,\n",
        "    output_hidden_states=True  # âœ… Ensures we get hidden states\n",
        ")\n",
        "\n",
        "# Extract hidden states from outputs\n",
        "hidden_states = outputs.hidden_states  # This is a tuple of layers' hidden states\n",
        "last_layer_hidden_states = hidden_states[-1]  # Get last layer's hidden states\n",
        "\n",
        "# Get the hidden states of the final generated tokens\n",
        "last_hidden_state = last_layer_hidden_states[-1]  # ðŸ”¥ Extract tensor, not tuple\n",
        "response_embeddings = last_hidden_state.mean(dim=1)  # Mean over sequence tokens\n",
        "\n",
        "# Step 3: Define a simple policy network (if not already defined)\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)  # Output: scalar score for each response\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "policy_net = PolicyNetwork(input_dim=768).to(device)  # Ensure it's on the right device\n",
        "\n",
        "# Step 4: Compute response scores using the policy network\n",
        "response_scores = policy_net(response_embeddings)\n",
        "\n",
        "# Step 5: Compute policy loss (GRPO-style clipped loss)\n",
        "baseline = response_scores.mean()\n",
        "advantage = response_scores - baseline\n",
        "policy_loss = -torch.min(response_scores * advantage, torch.clamp(response_scores, 0.8, 1.2) * advantage).mean()\n",
        "\n",
        "# ðŸ”¥ Step 6: Backpropagate loss INTO GPT-2 parameters ðŸ”¥\n",
        "optimizer.zero_grad()\n",
        "policy_loss.backward()  # âœ… This now updates GPT-2's parameters\n",
        "optimizer.step()\n",
        "\n",
        "print(f\"Policy Loss: {policy_loss.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBjCFtO-iUzu",
        "outputId": "cfde5ded-b371-4425-9167-c1df145fa262"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy Loss: 0.2255459576845169\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " (this is a untrained neural net used as an example, it will give random rewards) to Score Responses"
      ],
      "metadata": {
        "id": "Tuu6uv9hVFHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RewardModel(nn.Module):\n",
        "    \"\"\"Simple reward model that scores responses.\"\"\"\n",
        "    def __init__(self, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1),  # Output: a single score per response\n",
        "            nn.Sigmoid()  # Normalize score between 0 and 1\n",
        "        )\n",
        "\n",
        "    def forward(self, response_embedding):\n",
        "        return self.fc(response_embedding)\n",
        "\n",
        "# Instantiate reward model\n",
        "reward_model = RewardModel(embedding_dim=768)  # Assuming GPT's embeddings\n",
        "\n",
        "# Convert responses to embeddings\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "response_inputs = tokenizer(decoded_responses, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "response_embeddings = model.transformer.wte(response_inputs[\"input_ids\"]).mean(dim=1)  # Averaging token embeddings\n",
        "\n",
        "# Score each response\n",
        "reward_scores = reward_model(response_embeddings).squeeze()\n",
        "\n",
        "# Print scores\n",
        "for i, (response, score) in enumerate(zip(decoded_responses, reward_scores.tolist())):\n",
        "    print(f\"Response {i+1}: {response} | Score: {score:.3f}\")"
      ],
      "metadata": {
        "id": "tz2EIrFyUevQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "toy example of the policy network"
      ],
      "metadata": {
        "id": "8q0NU1UdCyK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(state_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, action_dim),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        return self.fc(state)\n",
        "\n",
        "def compute_advantage(rewards, values, gamma=0.99):\n",
        "    \"\"\"Computes advantage estimates using reward and value function.\"\"\"\n",
        "    returns = []\n",
        "    advs = []\n",
        "    G = 0\n",
        "    for t in reversed(range(len(rewards))):\n",
        "        G = rewards[t] + gamma * G  # Compute return\n",
        "        returns.insert(0, G)\n",
        "        advs.insert(0, G - values[t])  # Advantage = Return - Value Estimate\n",
        "    return torch.tensor(advs, dtype=torch.float32)\n",
        "\n",
        "# Hyperparameters\n",
        "epsilon = 0.2\n",
        "gamma = 0.99\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Sample data (dummy example)\n",
        "state_dim = 4\n",
        "action_dim = 2\n",
        "states = torch.rand((5, state_dim))  # 5 sample states\n",
        "actions = torch.tensor([0, 1, 0, 1, 0])  # Actions taken\n",
        "old_probs = torch.tensor([0.4, 0.6, 0.5, 0.7, 0.5])  # Old policy probabilities\n",
        "rewards = [1, 0, 1, 1, 0]  # Rewards received\n",
        "values = [0.5, 0.4, 0.6, 0.7, 0.3]  # Value estimates\n",
        "\n",
        "# Initialize network and optimizer\n",
        "policy_net = PolicyNetwork(state_dim, action_dim)\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
        "\n",
        "# Compute advantage\n",
        "advantages = compute_advantage(rewards, values, gamma)\n",
        "\n",
        "# Compute new policy probabilities\n",
        "new_probs = policy_net(states).gather(1, actions.view(-1, 1)).squeeze()\n",
        "\n",
        "# Compute probability ratio\n",
        "ratios = new_probs / old_probs\n",
        "\n",
        "# Compute clipped and unclipped loss\n",
        "clipped_ratios = torch.clamp(ratios, 1 - epsilon, 1 + epsilon)\n",
        "loss = -torch.min(ratios * advantages, clipped_ratios * advantages).mean()\n",
        "\n",
        "# Perform gradient ascent\n",
        "optimizer.zero_grad()\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "print(f\"Policy Loss: {loss.item()}\")\n"
      ],
      "metadata": {
        "id": "y_3-VodsCgh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the Policy with GRPO\n",
        "Train the policy network to predict higher scores for better responses.\n",
        "Update it using a reinforcement learning algorithm like GRPO.\n",
        "Example: Training the Policy Network"
      ],
      "metadata": {
        "id": "PFMwNE-hCuD8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the Policy for Action Selection\n",
        "Once the policy is trained, it can be used to select actions in the environment.\n",
        "\n",
        "Example: Action Selection in a Trained Policy"
      ],
      "metadata": {
        "id": "0XWAXA034vj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def select_action(policy_net, state):\n",
        "    \"\"\"Selects an action based on the trained policy.\"\"\"\n",
        "    with torch.no_grad():  # No gradients needed for inference\n",
        "        action_probs = policy_net(state)\n",
        "        action = torch.multinomial(action_probs, 1)  # Sample from the policy distribution\n",
        "    return action.item()\n",
        "\n",
        "# Example usage:\n",
        "state = torch.rand((1, 4))  # Example state (assuming 4D input)\n",
        "action = select_action(policy_net, state)\n",
        "print(f\"Selected Action: {action}\")\n"
      ],
      "metadata": {
        "id": "XmvP6zM-4ydV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Simulated training data (for visualization purposes)\n",
        "num_epochs = 50\n",
        "loss_values = []\n",
        "kl_values = []\n",
        "grad_norms = []\n",
        "\n",
        "# Dummy policy network (small for visualization purposes)\n",
        "policy_net = nn.Sequential(\n",
        "    nn.Linear(768, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64, 1)\n",
        ")\n",
        "policy_optimizer = optim.Adam(policy_net.parameters(), lr=1e-5)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Simulated loss and KL divergence values\n",
        "    policy_loss = torch.rand(1).item() * 2  # Randomized for visualization\n",
        "    kl_loss = torch.rand(1).item() * 0.2\n",
        "    total_loss = policy_loss + kl_loss\n",
        "\n",
        "    # Simulate gradient computation\n",
        "    policy_optimizer.zero_grad()\n",
        "    total_loss_tensor = torch.tensor(total_loss, requires_grad=True)\n",
        "    total_loss_tensor.backward()\n",
        "\n",
        "    # Compute gradient norm\n",
        "    total_norm = 0\n",
        "    for param in policy_net.parameters():\n",
        "        if param.grad is not None:\n",
        "            total_norm += param.grad.norm().item()\n",
        "    grad_norms.append(total_norm)\n",
        "\n",
        "    # Apply gradient step\n",
        "    policy_optimizer.step()\n",
        "\n",
        "    # Store values for plotting\n",
        "    loss_values.append(total_loss)\n",
        "    kl_values.append(kl_loss)\n",
        "\n",
        "# Plot the results\n",
        "fig, ax1 = plt.subplots()\n",
        "ax1.set_xlabel(\"Epochs\")\n",
        "ax1.set_ylabel(\"Loss / KL Divergence\", color=\"tab:blue\")\n",
        "ax1.plot(loss_values, label=\"Total Loss\", color=\"tab:red\")\n",
        "ax1.plot(kl_values, label=\"KL Divergence\", color=\"tab:blue\", linestyle=\"dashed\")\n",
        "ax1.legend(loc=\"upper right\")\n",
        "ax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.set_ylabel(\"Gradient Norms\", color=\"tab:green\")\n",
        "ax2.plot(grad_norms, label=\"Gradient Norms\", color=\"tab:green\", linestyle=\"dotted\")\n",
        "ax2.tick_params(axis=\"y\", labelcolor=\"tab:green\")\n",
        "ax2.legend(loc=\"lower right\")\n",
        "\n",
        "plt.title(\"Policy Training: Loss, KL Divergence & Gradient Norms\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fPVBdnLCFL0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test of using GPT2 to generate multiple responses based on the promt"
      ],
      "metadata": {
        "id": "d96X6YKryb0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load GPT model & tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Define a question (prompt)\n",
        "prompt = \"What people like or dislike about working out?\"\n",
        "\n",
        "# Tokenize the input\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# Generate multiple responses using sampling\n",
        "num_responses = 5  # Generate 5 different completions\n",
        "responses = model.generate(\n",
        "    input_ids,\n",
        "    max_length=50,\n",
        "    do_sample=True,  # Enables sampling instead of greedy decoding\n",
        "    top_k=50,  # Consider top 50 tokens at each step\n",
        "    top_p=0.9,  # Nucleus sampling: keeps top tokens contributing to 90% probability\n",
        "    temperature=0.7,  # Controls randomness (lower = more deterministic)\n",
        "    num_return_sequences=num_responses  # Generates multiple responses\n",
        ")\n",
        "\n",
        "# Decode and print responses\n",
        "#decoded_responses = [tokenizer.decode(output, skip_special_tokens=True) for output in responses]\n",
        "#Decode $ remove the prompt &print\n",
        "decoded_responses = [\n",
        "    tokenizer.decode(output[len(input_ids[0]):], skip_special_tokens=True).strip()\n",
        "    for output in responses\n",
        "]\n",
        "for i, response in enumerate(decoded_responses):\n",
        "    print(f\"Response {i+1}: {response}\")"
      ],
      "metadata": {
        "id": "PygDNDnAyeAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "example of not working code\n",
        "\n",
        "ðŸ”´ Problem in the Code\n",
        "The policy_loss.backward() only computes gradients for the policy network because response_scores = policy_net(response_embeddings.mean(dim=1)) does not involve GPT-2 parameters.\n",
        "\n",
        "torch.no_grad() disabled gradient tracking for GPT-2â€™s outputs.\n",
        "response_embeddings = model.transformer.wte(outputs): Here, we used frozen embeddings, which do not track gradients.\n",
        "Since these embeddings were detached from GPT-2, policy_loss.backward() did not propagate gradients into GPT-2â€”only into the policy network."
      ],
      "metadata": {
        "id": "BvlrCQ5g9wjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load the LLM (e.g., GPT-2 for simplicity)\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Define a simple policy network that scores responses\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)  # Output: scalar score for each response\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Instantiate the policy network\n",
        "policy_net = PolicyNetwork(input_dim=768)  # GPT's embedding size\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Simulated training batch\n",
        "texts = [\"What is the capital of France?\", \"How does photosynthesis work?\"]\n",
        "inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# Step 1: Generate responses from the LLM\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(inputs[\"input_ids\"], max_length=50)\n",
        "\n",
        "# Step 2: Get response embeddings (used as input to policy network)\n",
        "response_embeddings = model.transformer.wte(outputs)  # Word token embeddings\n",
        "response_scores = policy_net(response_embeddings.mean(dim=1))  # Score each response\n",
        "\n",
        "# Step 3: Compute policy loss (GRPO-style clipped loss)\n",
        "baseline = response_scores.mean()  # Baseline for advantage calculation\n",
        "advantage = response_scores - baseline  # Compute advantage function\n",
        "policy_loss = -torch.min(response_scores * advantage, torch.clamp(response_scores, 0.8, 1.2) * advantage).mean()\n",
        "\n",
        "# Step 4: Backpropagate the loss to fine-tune the LLM\n",
        "optimizer.zero_grad()\n",
        "policy_loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "print(f\"Policy Loss: {policy_loss.item()}\")"
      ],
      "metadata": {
        "id": "s7hGPXgU90eS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test code to delete:\n",
        "\n",
        "# Define a simple policy network\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)  # Output: scalar score for each response\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Instantiate the policy network (Assumed Pretrained)\n",
        "policy_net = PolicyNetwork(input_dim=768)  # GPT-2 embedding size\n",
        "\n",
        "# Optimizer for fine-tuning GPT-2\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5)"
      ],
      "metadata": {
        "id": "bRjMIKQMDfSQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}