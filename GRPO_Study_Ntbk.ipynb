{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKuocMqYyIWOQhQqjAqreS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pastrop/kaggle/blob/master/GRPO_Study_Ntbk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRPO Study Notebook"
      ],
      "metadata": {
        "id": "HOHFwsLQ2koP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install torch transformers"
      ],
      "metadata": {
        "id": "AHgvlWhmTJuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning LLM with the GRPO"
      ],
      "metadata": {
        "id": "LV7twYkUJe3x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "generating training examples using LLM"
      ],
      "metadata": {
        "id": "8-cj1WxfHBlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Load GPT model & tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Define a question (prompt)\n",
        "prompt = \"What are the benefits of exercise?\"\n",
        "\n",
        "# Tokenize the input\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# Generate multiple responses using sampling\n",
        "num_responses = 5  # Generate 5 different completions\n",
        "responses = model.generate(\n",
        "    input_ids,\n",
        "    max_length=50,\n",
        "    do_sample=True,  # Enables sampling instead of greedy decoding\n",
        "    top_k=50,  # Consider top 50 tokens at each step\n",
        "    top_p=0.9,  # Nucleus sampling: keeps top tokens contributing to 90% probability\n",
        "    temperature=0.7,  # Controls randomness (lower = more deterministic)\n",
        "    num_return_sequences=num_responses  # Generates multiple responses\n",
        ")\n",
        "\n",
        "# Decode and print responses\n",
        "decoded_responses = [tokenizer.decode(output, skip_special_tokens=True) for output in responses]\n",
        "for i, response in enumerate(decoded_responses):\n",
        "    print(f\"Response {i+1}: {response}\")\n"
      ],
      "metadata": {
        "id": "03cr53Z2G_A2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using a Reward Model (this is a untrained neural net used as an example, it will give random rewards) to Score Responses\n",
        "It scores responses from 0 to 1, where:\n",
        "\n",
        "1.0 = Excellent\n",
        "0.0 = Poor"
      ],
      "metadata": {
        "id": "wi3KFnl-ILA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RewardModel(nn.Module):\n",
        "    \"\"\"Simple reward model that scores responses.\"\"\"\n",
        "    def __init__(self, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1),  # Output: a single score per response\n",
        "            nn.Sigmoid()  # Normalize score between 0 and 1\n",
        "        )\n",
        "\n",
        "    def forward(self, response_embedding):\n",
        "        return self.fc(response_embedding)\n",
        "\n",
        "# Instantiate reward model\n",
        "reward_model = RewardModel(embedding_dim=768)  # Assuming GPT's embeddings\n",
        "\n",
        "# Convert responses to embeddings\n",
        "response_inputs = tokenizer(decoded_responses, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "response_embeddings = model.transformer.wte(response_inputs[\"input_ids\"]).mean(dim=1)  # Averaging token embeddings\n",
        "\n",
        "# Score each response\n",
        "reward_scores = reward_model(response_embeddings).squeeze()\n",
        "\n",
        "# Print scores\n",
        "for i, (response, score) in enumerate(zip(decoded_responses, reward_scores.tolist())):\n",
        "    print(f\"Response {i+1}: {response} | Score: {score:.3f}\")\n"
      ],
      "metadata": {
        "id": "74it5QxOI5GW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "using a trained sentiment model as a reward model"
      ],
      "metadata": {
        "id": "9f7gYWNvThy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Load sentiment classifier\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "reward_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "reward_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def get_reward_scores(responses):\n",
        "    \"\"\"Scores responses using a sentiment classifier.\"\"\"\n",
        "    inputs = reward_tokenizer(responses, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    with torch.no_grad():  # No gradient calculation needed\n",
        "        outputs = reward_model(**inputs)\n",
        "    scores = torch.softmax(outputs.logits, dim=1)[:, 1]  # Probability of \"positive\" class\n",
        "    return scores.detach()  # Detach to prevent computation graph tracking\n",
        "\n",
        "# Example responses\n",
        "responses = [\n",
        "    \"I love exercising, it makes me feel amazing!\",\n",
        "    \"Exercise is okay, but it's tiring.\",\n",
        "    \"I hate exercising, it's the worst!\"\n",
        "]\n",
        "\n",
        "# Get reward scores\n",
        "reward_scores = get_reward_scores(responses)\n",
        "\n",
        "# Print scores\n",
        "for i, (response, score) in enumerate(zip(responses, reward_scores)):\n",
        "    print(f\"Response {i+1}: {response}\\nScore: {score.item():.3f}\\n\")\n"
      ],
      "metadata": {
        "id": "T0fb9VQETpoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a Policy Networks"
      ],
      "metadata": {
        "id": "gVXkCdVPKTgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple policy network that scores responses\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)  # Output: scalar score for each response\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Instantiate the policy network\n",
        "policy_net = PolicyNetwork(input_dim=768)  # GPT's embedding size\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5)"
      ],
      "metadata": {
        "id": "0Je3gpQ2KYU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the policy network"
      ],
      "metadata": {
        "id": "3TyGxBg_HdnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy_optimizer = optim.Adam(policy_net.parameters(), lr=3e-4)\n",
        "\n",
        "for batch in training_data:  # Assume training_data is a dataset of responses + rewards\n",
        "    response_embeddings = model.transformer.wte(batch[\"response_ids\"])\n",
        "\n",
        "    predicted_scores = policy_net(response_embeddings.mean(dim=1))\n",
        "    advantage = batch[\"true_rewards\"] - predicted_scores.detach()  # Compute advantage\n",
        "\n",
        "    policy_loss = -torch.min(\n",
        "        predicted_scores * advantage,\n",
        "        torch.clamp(predicted_scores, 0.8, 1.2) * advantage\n",
        "    ).mean()\n",
        "\n",
        "    policy_optimizer.zero_grad()\n",
        "    policy_loss.backward()\n",
        "    policy_optimizer.step()\n"
      ],
      "metadata": {
        "id": "PfTyw1voHdPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "training policy network with KL term"
      ],
      "metadata": {
        "id": "0_WCInxBYh2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def compute_kl_divergence(original_logits, new_logits):\n",
        "    \"\"\"Compute KL divergence between the original GPT-2 outputs and the policy network outputs.\"\"\"\n",
        "    original_probs = F.softmax(original_logits, dim=-1)\n",
        "    new_probs = F.softmax(new_logits, dim=-1)\n",
        "    kl_div = F.kl_div(new_probs.log(), original_probs, reduction=\"batchmean\")  # KL(P || Q)\n",
        "    return kl_div\n",
        "\n",
        "def train_policy_network_with_kl(responses, reward_scores, beta=0.01):\n",
        "    \"\"\"Updates the policy network using GRPO-style policy gradients with KL divergence.\"\"\"\n",
        "\n",
        "    # Get response embeddings from GPT-2\n",
        "    response_inputs = gpt_tokenizer(responses, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    response_embeddings = gpt_model.transformer.wte(response_inputs[\"input_ids\"]).mean(dim=1)  # Mean token embedding\n",
        "\n",
        "    # Get logits from the original GPT-2 model (before policy updates)\n",
        "    with torch.no_grad():\n",
        "        original_logits = gpt_model(response_inputs[\"input_ids\"]).logits.mean(dim=1)\n",
        "\n",
        "    # Get predicted scores from the policy network\n",
        "    predicted_scores = policy_net(response_embeddings).squeeze()\n",
        "\n",
        "    # Compute Advantage (A = reward - baseline)\n",
        "    baseline = reward_scores.mean()\n",
        "    advantage = reward_scores - baseline\n",
        "\n",
        "    # Compute policy loss (GRPO-style clipped loss)\n",
        "    clip_ratio = 0.2\n",
        "    policy_loss = -torch.min(\n",
        "        predicted_scores * advantage,\n",
        "        torch.clamp(predicted_scores, 1 - clip_ratio, 1 + clip_ratio) * advantage\n",
        "    ).mean()\n",
        "\n",
        "    # Compute KL divergence loss\n",
        "    new_logits = policy_net(response_embeddings)  # Policy's logits\n",
        "    kl_loss = compute_kl_divergence(original_logits, new_logits)\n",
        "\n",
        "    # Final loss with KL penalty\n",
        "    total_loss = policy_loss + beta * kl_loss\n",
        "\n",
        "    # Backpropagation\n",
        "    policy_optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    policy_optimizer.step()\n",
        "\n",
        "    return total_loss.item(), kl_loss.item()\n",
        "\n",
        "# Training loop with KL regularization\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    responses = generate_responses(prompt)\n",
        "    reward_scores = get_reward_scores(responses)\n",
        "    loss, kl = train_policy_network_with_kl(responses, reward_scores)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {loss:.4f}, KL Divergence: {kl:.4f}\")\n"
      ],
      "metadata": {
        "id": "8hbB0Z86Ybth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning an LLM with a Policy Network\n",
        "Below is a simplified PyTorch example that fine-tunes an LLM (e.g., GPT) using a trained policy."
      ],
      "metadata": {
        "id": "Gnvcqff14elR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load the LLM (e.g., GPT-2 for simplicity)\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Define a simple policy network that scores responses\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)  # Output: scalar score for each response\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Instantiate the policy network\n",
        "policy_net = PolicyNetwork(input_dim=768)  # GPT's embedding size\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Simulated training batch\n",
        "texts = [\"What is the capital of France?\", \"How does photosynthesis work?\"]\n",
        "inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# Step 1: Generate responses from the LLM\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(inputs[\"input_ids\"], max_length=50)\n",
        "\n",
        "# Step 2: Get response embeddings (used as input to policy network)\n",
        "response_embeddings = model.transformer.wte(outputs)  # Word token embeddings\n",
        "response_scores = policy_net(response_embeddings.mean(dim=1))  # Score each response\n",
        "\n",
        "# Step 3: Compute policy loss (GRPO-style clipped loss)\n",
        "baseline = response_scores.mean()  # Baseline for advantage calculation\n",
        "advantage = response_scores - baseline  # Compute advantage function\n",
        "policy_loss = -torch.min(response_scores * advantage, torch.clamp(response_scores, 0.8, 1.2) * advantage).mean()\n",
        "\n",
        "# Step 4: Backpropagate the loss to fine-tune the LLM\n",
        "optimizer.zero_grad()\n",
        "policy_loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "print(f\"Policy Loss: {policy_loss.item()}\")\n"
      ],
      "metadata": {
        "id": "3_AJNkb-4gwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Policy\n",
        "*random examples*"
      ],
      "metadata": {
        "id": "YWi9qxQf4NC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "toy example of the policy network"
      ],
      "metadata": {
        "id": "8q0NU1UdCyK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(state_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, action_dim),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        return self.fc(state)\n",
        "\n",
        "def compute_advantage(rewards, values, gamma=0.99):\n",
        "    \"\"\"Computes advantage estimates using reward and value function.\"\"\"\n",
        "    returns = []\n",
        "    advs = []\n",
        "    G = 0\n",
        "    for t in reversed(range(len(rewards))):\n",
        "        G = rewards[t] + gamma * G  # Compute return\n",
        "        returns.insert(0, G)\n",
        "        advs.insert(0, G - values[t])  # Advantage = Return - Value Estimate\n",
        "    return torch.tensor(advs, dtype=torch.float32)\n",
        "\n",
        "# Hyperparameters\n",
        "epsilon = 0.2\n",
        "gamma = 0.99\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Sample data (dummy example)\n",
        "state_dim = 4\n",
        "action_dim = 2\n",
        "states = torch.rand((5, state_dim))  # 5 sample states\n",
        "actions = torch.tensor([0, 1, 0, 1, 0])  # Actions taken\n",
        "old_probs = torch.tensor([0.4, 0.6, 0.5, 0.7, 0.5])  # Old policy probabilities\n",
        "rewards = [1, 0, 1, 1, 0]  # Rewards received\n",
        "values = [0.5, 0.4, 0.6, 0.7, 0.3]  # Value estimates\n",
        "\n",
        "# Initialize network and optimizer\n",
        "policy_net = PolicyNetwork(state_dim, action_dim)\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
        "\n",
        "# Compute advantage\n",
        "advantages = compute_advantage(rewards, values, gamma)\n",
        "\n",
        "# Compute new policy probabilities\n",
        "new_probs = policy_net(states).gather(1, actions.view(-1, 1)).squeeze()\n",
        "\n",
        "# Compute probability ratio\n",
        "ratios = new_probs / old_probs\n",
        "\n",
        "# Compute clipped and unclipped loss\n",
        "clipped_ratios = torch.clamp(ratios, 1 - epsilon, 1 + epsilon)\n",
        "loss = -torch.min(ratios * advantages, clipped_ratios * advantages).mean()\n",
        "\n",
        "# Perform gradient ascent\n",
        "optimizer.zero_grad()\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "print(f\"Policy Loss: {loss.item()}\")\n"
      ],
      "metadata": {
        "id": "y_3-VodsCgh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the Policy with GRPO\n",
        "Train the policy network to predict higher scores for better responses.\n",
        "Update it using a reinforcement learning algorithm like GRPO.\n",
        "Example: Training the Policy Network"
      ],
      "metadata": {
        "id": "PFMwNE-hCuD8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the Policy for Action Selection\n",
        "Once the policy is trained, it can be used to select actions in the environment.\n",
        "\n",
        "Example: Action Selection in a Trained Policy"
      ],
      "metadata": {
        "id": "0XWAXA034vj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def select_action(policy_net, state):\n",
        "    \"\"\"Selects an action based on the trained policy.\"\"\"\n",
        "    with torch.no_grad():  # No gradients needed for inference\n",
        "        action_probs = policy_net(state)\n",
        "        action = torch.multinomial(action_probs, 1)  # Sample from the policy distribution\n",
        "    return action.item()\n",
        "\n",
        "# Example usage:\n",
        "state = torch.rand((1, 4))  # Example state (assuming 4D input)\n",
        "action = select_action(policy_net, state)\n",
        "print(f\"Selected Action: {action}\")\n"
      ],
      "metadata": {
        "id": "XmvP6zM-4ydV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}