{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNx6EeNFpGk1UMEdpBZNDtf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pastrop/kaggle/blob/master/GRPO_Study_Ntbk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRPO Study Notebook"
      ],
      "metadata": {
        "id": "HOHFwsLQ2koP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install torch transformers"
      ],
      "metadata": {
        "id": "AHgvlWhmTJuG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning LLM with the GRPO"
      ],
      "metadata": {
        "id": "LV7twYkUJe3x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "generating training examples using LLM"
      ],
      "metadata": {
        "id": "8-cj1WxfHBlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import torch.nn as nn # Import the torch.nn module and alias it as nn\n",
        "import torch.optim as optim # Import the torch.optim module and alias it as optim\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "AzBSza1nG5U-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load GPT model & tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Define a question (prompt)\n",
        "prompt = \"What people like or dislike about working out?\"\n",
        "\n",
        "# Tokenize the input\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# Generate multiple responses using sampling\n",
        "num_responses = 5  # Generate 5 different completions\n",
        "responses = model.generate(\n",
        "    input_ids,\n",
        "    max_length=50,\n",
        "    do_sample=True,  # Enables sampling instead of greedy decoding\n",
        "    top_k=50,  # Consider top 50 tokens at each step\n",
        "    top_p=0.9,  # Nucleus sampling: keeps top tokens contributing to 90% probability\n",
        "    temperature=0.7,  # Controls randomness (lower = more deterministic)\n",
        "    num_return_sequences=num_responses  # Generates multiple responses\n",
        ")\n",
        "\n",
        "# Decode and print responses\n",
        "#decoded_responses = [tokenizer.decode(output, skip_special_tokens=True) for output in responses]\n",
        "#Decode $ remove the prompt &print\n",
        "decoded_responses = [\n",
        "    tokenizer.decode(output[len(input_ids[0]):], skip_special_tokens=True).strip()\n",
        "    for output in responses\n",
        "]\n",
        "for i, response in enumerate(decoded_responses):\n",
        "    print(f\"Response {i+1}: {response}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03cr53Z2G_A2",
        "outputId": "202ced75-e330-4865-c0b5-187a2479ae61"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response 1: Well, it's like the idea of being in the gym, or your girlfriend or boyfriend, or whatever. But if you're doing it that way, and you're having a great time and\n",
            "Response 2: People like to work out. You have to be very careful and you have to be very careful about what you say. You have to be very careful about what you say. And I think that\n",
            "Response 3: I don't think I have an answer for that.\n",
            "\n",
            "The last thing you want to do is to give up.\n",
            "\n",
            "It's a really good idea to do it for a few\n",
            "Response 4: Workout is a great way to get your body to feel good. I am not saying that it is not possible to get fit and get better, but it is very important to get your body\n",
            "Response 5: I think it's a really cool thing to do. I think people like to work out. I think people like to be physically active. I think people like to eat. I think people like\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using a trained sentiment model as a Reward Model**"
      ],
      "metadata": {
        "id": "9f7gYWNvThy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load sentiment classifier\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "reward_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "reward_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def get_reward_scores(responses):\n",
        "    \"\"\"Scores responses using a sentiment classifier.\"\"\"\n",
        "    inputs = reward_tokenizer(responses, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    with torch.no_grad():  # No gradient calculation needed\n",
        "        outputs = reward_model(**inputs)\n",
        "    scores = torch.softmax(outputs.logits, dim=1)[:, 1]  # Probability of \"positive\" class\n",
        "    return scores.detach()  # Detach to prevent computation graph tracking"
      ],
      "metadata": {
        "id": "8U-D6-JxTgBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test Run for the Reward Model (This is a test cell just to make sure get_reward_scores works properly)\n",
        "'''\n",
        "#Example responses\n",
        "responses = [\n",
        "    \"I love exercising, it makes me feel amazing!\",\n",
        "    \"Exercise is okay, but it's tiring.\",\n",
        "    \"I hate exercising, it's the worst!\"\n",
        "]\n",
        "'''\n",
        "# Get reward scores\n",
        "reward_scores = get_reward_scores(decoded_responses)\n",
        "\n",
        "# Print scores\n",
        "for i, (decoded_response, score) in enumerate(zip(decoded_responses, reward_scores)):\n",
        "    print(f\"Response {i+1}: {decoded_response}\\nScore: {score.item():.3f}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0fb9VQETpoZ",
        "outputId": "491b646c-25b9-4720-bfd9-b8627eb594b5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response 1: Well, it's like the idea of being in the gym, or your girlfriend or boyfriend, or whatever. But if you're doing it that way, and you're having a great time and\n",
            "Score: 1.000\n",
            "\n",
            "Response 2: People like to work out. You have to be very careful and you have to be very careful about what you say. You have to be very careful about what you say. And I think that\n",
            "Score: 0.035\n",
            "\n",
            "Response 3: I don't think I have an answer for that.\n",
            "\n",
            "The last thing you want to do is to give up.\n",
            "\n",
            "It's a really good idea to do it for a few\n",
            "Score: 0.868\n",
            "\n",
            "Response 4: Workout is a great way to get your body to feel good. I am not saying that it is not possible to get fit and get better, but it is very important to get your body\n",
            "Score: 0.999\n",
            "\n",
            "Response 5: I think it's a really cool thing to do. I think people like to work out. I think people like to be physically active. I think people like to eat. I think people like\n",
            "Score: 1.000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a Policy Networks"
      ],
      "metadata": {
        "id": "gVXkCdVPKTgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple policy network that scores responses\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)  # Output: scalar score for each response\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x).squeeze(-1)  # Ensure output is [batch_size] shape\n",
        "\n",
        "# Instantiate the policy network\n",
        "policy_net = PolicyNetwork(input_dim=768)  # GPT-2 embedding size\n",
        "policy_optimizer = optim.Adam(policy_net.parameters(), lr=1e-5)"
      ],
      "metadata": {
        "id": "PfTyw1voHdPv"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "training policy network with KL term"
      ],
      "metadata": {
        "id": "0_WCInxBYh2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_kl_divergence(original_logits, new_logits):\n",
        "    \"\"\"Compute KL divergence between the original GPT-2 outputs and the policy network outputs.\"\"\"\n",
        "    original_probs = F.softmax(original_logits, dim=-1)\n",
        "    new_probs = F.softmax(new_logits, dim=-1)\n",
        "    kl_div = F.kl_div(new_probs.log(), original_probs, reduction=\"batchmean\")  # KL(P || Q)\n",
        "    return kl_div\n",
        "\n",
        "def train_policy_network_with_kl(responses, reward_scores, beta=0.01):\n",
        "    \"\"\"Updates the policy network using GRPO-style policy gradients with KL divergence.\"\"\"\n",
        "\n",
        "    # Get response embeddings from GPT-2\n",
        "    response_inputs = gpt_tokenizer(responses, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    response_embeddings = gpt_model.transformer.wte(response_inputs[\"input_ids\"]).mean(dim=1)  # Mean token embedding\n",
        "\n",
        "    # Get logits from the original GPT-2 model (before policy updates)\n",
        "    with torch.no_grad():\n",
        "        original_logits = gpt_model(response_inputs[\"input_ids\"]).logits.mean(dim=1)\n",
        "\n",
        "    # Get predicted scores from the policy network\n",
        "    predicted_scores = policy_net(response_embeddings).squeeze()\n",
        "\n",
        "    # Compute Advantage (A = reward - baseline)\n",
        "    baseline = reward_scores.mean()\n",
        "    advantage = reward_scores - baseline\n",
        "\n",
        "    # Compute policy loss (GRPO-style clipped loss)\n",
        "    clip_ratio = 0.2\n",
        "    policy_loss = -torch.min(\n",
        "        predicted_scores * advantage,\n",
        "        torch.clamp(predicted_scores, 1 - clip_ratio, 1 + clip_ratio) * advantage\n",
        "    ).mean()\n",
        "\n",
        "    # Compute KL divergence loss\n",
        "    new_logits = policy_net(response_embeddings)  # Policy's logits\n",
        "    kl_loss = compute_kl_divergence(original_logits, new_logits)\n",
        "\n",
        "    # Final loss with KL penalty\n",
        "    total_loss = policy_loss + beta * kl_loss\n",
        "\n",
        "    # Backpropagation\n",
        "    policy_optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    policy_optimizer.step()\n",
        "\n",
        "    return total_loss.item(), kl_loss.item()\n",
        "\n",
        "# Training loop with KL regularization\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    responses = generate_responses(prompt)\n",
        "    reward_scores = get_reward_scores(responses)\n",
        "    loss, kl = train_policy_network_with_kl(responses, reward_scores)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {loss:.4f}, KL Divergence: {kl:.4f}\")\n"
      ],
      "metadata": {
        "id": "8hbB0Z86Ybth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning an LLM with a Policy Network\n",
        "Below is a simplified PyTorch example that fine-tunes an LLM (e.g., GPT) using a trained policy."
      ],
      "metadata": {
        "id": "Gnvcqff14elR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load the LLM (e.g., GPT-2 for simplicity)\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Define a simple policy network that scores responses\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)  # Output: scalar score for each response\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Instantiate the policy network\n",
        "policy_net = PolicyNetwork(input_dim=768)  # GPT's embedding size\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Simulated training batch\n",
        "texts = [\"What is the capital of France?\", \"How does photosynthesis work?\"]\n",
        "inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# Step 1: Generate responses from the LLM\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(inputs[\"input_ids\"], max_length=50)\n",
        "\n",
        "# Step 2: Get response embeddings (used as input to policy network)\n",
        "response_embeddings = model.transformer.wte(outputs)  # Word token embeddings\n",
        "response_scores = policy_net(response_embeddings.mean(dim=1))  # Score each response\n",
        "\n",
        "# Step 3: Compute policy loss (GRPO-style clipped loss)\n",
        "baseline = response_scores.mean()  # Baseline for advantage calculation\n",
        "advantage = response_scores - baseline  # Compute advantage function\n",
        "policy_loss = -torch.min(response_scores * advantage, torch.clamp(response_scores, 0.8, 1.2) * advantage).mean()\n",
        "\n",
        "# Step 4: Backpropagate the loss to fine-tune the LLM\n",
        "optimizer.zero_grad()\n",
        "policy_loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "print(f\"Policy Loss: {policy_loss.item()}\")\n"
      ],
      "metadata": {
        "id": "3_AJNkb-4gwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Policy & Reward Models\n",
        "*random examples*"
      ],
      "metadata": {
        "id": "YWi9qxQf4NC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " (this is a untrained neural net used as an example, it will give random rewards) to Score Responses"
      ],
      "metadata": {
        "id": "Tuu6uv9hVFHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RewardModel(nn.Module):\n",
        "    \"\"\"Simple reward model that scores responses.\"\"\"\n",
        "    def __init__(self, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1),  # Output: a single score per response\n",
        "            nn.Sigmoid()  # Normalize score between 0 and 1\n",
        "        )\n",
        "\n",
        "    def forward(self, response_embedding):\n",
        "        return self.fc(response_embedding)\n",
        "\n",
        "# Instantiate reward model\n",
        "reward_model = RewardModel(embedding_dim=768)  # Assuming GPT's embeddings\n",
        "\n",
        "# Convert responses to embeddings\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "response_inputs = tokenizer(decoded_responses, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "response_embeddings = model.transformer.wte(response_inputs[\"input_ids\"]).mean(dim=1)  # Averaging token embeddings\n",
        "\n",
        "# Score each response\n",
        "reward_scores = reward_model(response_embeddings).squeeze()\n",
        "\n",
        "# Print scores\n",
        "for i, (response, score) in enumerate(zip(decoded_responses, reward_scores.tolist())):\n",
        "    print(f\"Response {i+1}: {response} | Score: {score:.3f}\")"
      ],
      "metadata": {
        "id": "tz2EIrFyUevQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "toy example of the policy network"
      ],
      "metadata": {
        "id": "8q0NU1UdCyK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(state_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, action_dim),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        return self.fc(state)\n",
        "\n",
        "def compute_advantage(rewards, values, gamma=0.99):\n",
        "    \"\"\"Computes advantage estimates using reward and value function.\"\"\"\n",
        "    returns = []\n",
        "    advs = []\n",
        "    G = 0\n",
        "    for t in reversed(range(len(rewards))):\n",
        "        G = rewards[t] + gamma * G  # Compute return\n",
        "        returns.insert(0, G)\n",
        "        advs.insert(0, G - values[t])  # Advantage = Return - Value Estimate\n",
        "    return torch.tensor(advs, dtype=torch.float32)\n",
        "\n",
        "# Hyperparameters\n",
        "epsilon = 0.2\n",
        "gamma = 0.99\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Sample data (dummy example)\n",
        "state_dim = 4\n",
        "action_dim = 2\n",
        "states = torch.rand((5, state_dim))  # 5 sample states\n",
        "actions = torch.tensor([0, 1, 0, 1, 0])  # Actions taken\n",
        "old_probs = torch.tensor([0.4, 0.6, 0.5, 0.7, 0.5])  # Old policy probabilities\n",
        "rewards = [1, 0, 1, 1, 0]  # Rewards received\n",
        "values = [0.5, 0.4, 0.6, 0.7, 0.3]  # Value estimates\n",
        "\n",
        "# Initialize network and optimizer\n",
        "policy_net = PolicyNetwork(state_dim, action_dim)\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
        "\n",
        "# Compute advantage\n",
        "advantages = compute_advantage(rewards, values, gamma)\n",
        "\n",
        "# Compute new policy probabilities\n",
        "new_probs = policy_net(states).gather(1, actions.view(-1, 1)).squeeze()\n",
        "\n",
        "# Compute probability ratio\n",
        "ratios = new_probs / old_probs\n",
        "\n",
        "# Compute clipped and unclipped loss\n",
        "clipped_ratios = torch.clamp(ratios, 1 - epsilon, 1 + epsilon)\n",
        "loss = -torch.min(ratios * advantages, clipped_ratios * advantages).mean()\n",
        "\n",
        "# Perform gradient ascent\n",
        "optimizer.zero_grad()\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "print(f\"Policy Loss: {loss.item()}\")\n"
      ],
      "metadata": {
        "id": "y_3-VodsCgh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the Policy with GRPO\n",
        "Train the policy network to predict higher scores for better responses.\n",
        "Update it using a reinforcement learning algorithm like GRPO.\n",
        "Example: Training the Policy Network"
      ],
      "metadata": {
        "id": "PFMwNE-hCuD8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the Policy for Action Selection\n",
        "Once the policy is trained, it can be used to select actions in the environment.\n",
        "\n",
        "Example: Action Selection in a Trained Policy"
      ],
      "metadata": {
        "id": "0XWAXA034vj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def select_action(policy_net, state):\n",
        "    \"\"\"Selects an action based on the trained policy.\"\"\"\n",
        "    with torch.no_grad():  # No gradients needed for inference\n",
        "        action_probs = policy_net(state)\n",
        "        action = torch.multinomial(action_probs, 1)  # Sample from the policy distribution\n",
        "    return action.item()\n",
        "\n",
        "# Example usage:\n",
        "state = torch.rand((1, 4))  # Example state (assuming 4D input)\n",
        "action = select_action(policy_net, state)\n",
        "print(f\"Selected Action: {action}\")\n"
      ],
      "metadata": {
        "id": "XmvP6zM-4ydV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}