{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GAN_SimpleDNN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1KD3ZgLs80vY",
        "Mvjjan17qHjq"
      ],
      "include_colab_link": true
    },
    "coursera": {
      "schema_names": [
        "GANSC1-1A"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pastrop/kaggle/blob/master/GAN_SimpleDNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1czVdIlqnImH"
      },
      "source": [
        "# Sequence GAN Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfkorNJrnmNO"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from tqdm.auto import tqdm\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST # Training dataset\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "torch.manual_seed(0) # Set for testing purposes, please do not change!\n",
        "\n",
        "def show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28)):\n",
        "    '''\n",
        "    Function for visualizing images: Given a tensor of images, number of images, and\n",
        "    size per image, plots and prints the images in a uniform grid.\n",
        "    '''\n",
        "    image_unflat = image_tensor.detach().cpu().view(-1, *size)\n",
        "    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n",
        "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
        "    plt.show()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mvjjan17qHjq"
      },
      "source": [
        "#### MNIST Dataset\n",
        "The training images are from [MNIST](http://yann.lecun.com/exdb/mnist/). It contains 60,000 images of handwritten digits, from 0 to 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1A1M6kpnfxw"
      },
      "source": [
        "## Generator\n",
        "Each block includes a [linear transformation](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) to map to another shape, a [batch normalization](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html), and finally a non-linear activation function (using a [ReLU here](https://pytorch.org/docs/master/generated/torch.nn.ReLU.html)) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZbqdw21hK5i"
      },
      "source": [
        "def get_generator_block(input_dim, output_dim):\n",
        "    '''\n",
        "    Function for returning a block of the generator's neural network\n",
        "    given input and output dimensions.\n",
        "    Parameters:\n",
        "        input_dim: the dimension of the input vector, a scalar\n",
        "        output_dim: the dimension of the output vector, a scalar\n",
        "    Returns:\n",
        "        a generator neural network layer, with a linear transformation \n",
        "          followed by a batch normalization and then a relu activation\n",
        "    '''\n",
        "    return nn.Sequential(\n",
        "        nn.Linear(input_dim, output_dim),\n",
        "        nn.BatchNorm1d(output_dim),\n",
        "        nn.ReLU()\n",
        "    )"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEihsdhOrU0m"
      },
      "source": [
        "Generator class takes 3 values:\n",
        "\n",
        "*   The noise vector dimension\n",
        "*   The image dimension\n",
        "*   The initial hidden dimension\n",
        "\n",
        "Using these values, the generator will build a neural network with 5 layers/blocks. Beginning with the noise vector, the generator will apply non-linear transformations via the block function until the tensor is mapped to the size of the image to be outputted (the same size as the real images from MNIST). You will need to fill in the code for final layer since it is different than the others. The final layer does not need a normalization or activation function, but does need to be scaled with a [sigmoid function](https://pytorch.org/docs/master/generated/torch.nn.Sigmoid.html). \n",
        "\n",
        "Finally, you are given a forward pass function that takes in a noise vector and generates an image of the output dimension using your neural network.\n",
        "\n",
        "<details>\n",
        "\n",
        "<summary>\n",
        "<font size=\"3\" color=\"green\">\n",
        "<b><code><font size=\"4\">Generator</font></code></b>\n",
        "</font>\n",
        "</summary>\n",
        "\n",
        "1. The output size of the final linear transformation should be im_dim, but remember you need to scale the outputs between 0 and 1 using the sigmoid function.\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvO7h0LYnEJZ"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    '''\n",
        "    Generator Class\n",
        "    Values:\n",
        "        z_dim: the dimension of the noise vector, a scalar\n",
        "        im_dim: the dimension of the images, fitted for the dataset used, a scalar\n",
        "          (MNIST images are 28 x 28 = 784 so that is your default)\n",
        "        hidden_dim: the inner dimension, a scalar\n",
        "    '''\n",
        "    def __init__(self, z_dim=10, im_dim=784, hidden_dim=128):\n",
        "        super(Generator, self).__init__()\n",
        "        # Build the neural network\n",
        "        self.gen = nn.Sequential(\n",
        "            get_generator_block(z_dim, hidden_dim),\n",
        "            get_generator_block(hidden_dim, hidden_dim * 2),\n",
        "            get_generator_block(hidden_dim * 2, hidden_dim * 4),\n",
        "            get_generator_block(hidden_dim * 4, hidden_dim * 8),\n",
        "            nn.Linear(hidden_dim*8,im_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, noise):\n",
        "        '''\n",
        "        Function for completing a forward pass of the generator: Given a noise tensor, \n",
        "        returns generated images.\n",
        "        Parameters:\n",
        "            noise: a noise tensor with dimensions (n_samples, z_dim)\n",
        "        '''\n",
        "        return self.gen(noise)\n",
        "    \n",
        "    # Needed for testing\n",
        "    def get_gen(self):\n",
        "        '''\n",
        "        Returns:\n",
        "            the sequential model\n",
        "        '''\n",
        "        return self.gen"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FLX69EaqRjn"
      },
      "source": [
        "## Noise\n",
        "\n",
        "Note that whenever you create a new tensor using torch.ones, torch.zeros, or torch.randn, you either need to create it on the target device, e.g. `torch.ones(3, 3, device=device)`, or move it onto the target device using `torch.ones(3, 3).to(device)`. You do not need to do this if you're creating a tensor by manipulating another tensor or by using a variation that defaults the device to the input, such as `torch.ones_like`. In general, use `torch.ones_like` and `torch.zeros_like` instead of `torch.ones` or `torch.zeros` where possible.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8COwJ9PkqUyd"
      },
      "source": [
        "def get_noise(n_samples, z_dim, device='cpu'):\n",
        "    '''\n",
        "    Function for creating noise vectors: Given the dimensions (n_samples, z_dim),\n",
        "    creates a tensor of that shape filled with random numbers from the normal distribution.\n",
        "    Parameters:\n",
        "        n_samples: the number of samples to generate, a scalar\n",
        "        z_dim: the dimension of the noise vector, a scalar\n",
        "        device: the device type\n",
        "    '''\n",
        "    # NOTE: To use this on GPU with device='cuda', make sure to pass the device \n",
        "    return torch.randn(n_samples, z_dim, device = device)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9fScH98nkYH"
      },
      "source": [
        "## Discriminator\n",
        "*Note: You use leaky ReLUs to prevent the \"dying ReLU\" problem, which refers to the phenomenon where the parameters stop changing due to consistently negative values passed to a ReLU, which result in a zero gradient. You will learn more about this in the following lectures!* \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYi8YFcseYFK"
      },
      "source": [
        "def get_discriminator_block(input_dim, output_dim):\n",
        "    '''\n",
        "    Discriminator Block\n",
        "    Function for returning a neural network of the discriminator given input and output dimensions.\n",
        "    Parameters:\n",
        "        input_dim: the dimension of the input vector, a scalar\n",
        "        output_dim: the dimension of the output vector, a scalar\n",
        "    Returns:\n",
        "        a discriminator neural network layer, with a linear transformation \n",
        "          followed by an nn.LeakyReLU activation with negative slope of 0.2 \n",
        "          (https://pytorch.org/docs/master/generated/torch.nn.LeakyReLU.html)\n",
        "    '''\n",
        "    return nn.Sequential(\n",
        "        nn.Linear(input_dim, output_dim),\n",
        "        nn.LeakyReLU(0.2),        \n",
        "    )"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tco9ffKnaNS"
      },
      "source": [
        "The discriminator class holds 2 values:\n",
        "\n",
        "*   The image dimension\n",
        "*   The hidden dimension\n",
        "\n",
        "The discriminator will build a neural network with 4 layers. It will start with the image tensor and transform it until it returns a single number (1-dimension tensor) output. This output classifies whether an image is fake or real. Note that you do not need a sigmoid after the output layer since it is included in the loss function. Finally, to use your discrimator's neural network you are given a forward pass function that takes in an image tensor to be classified.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aA4AxGnmpuPq"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    '''\n",
        "    Discriminator Class\n",
        "    Values:\n",
        "        im_dim: the dimension of the images, fitted for the dataset used, a scalar\n",
        "            (MNIST images are 28x28 = 784 so that is your default)\n",
        "        hidden_dim: the inner dimension, a scalar\n",
        "    '''\n",
        "    def __init__(self, im_dim=784, hidden_dim=128):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.disc = nn.Sequential(\n",
        "            get_discriminator_block(im_dim, hidden_dim * 4),\n",
        "            get_discriminator_block(hidden_dim * 4, hidden_dim * 2),\n",
        "            get_discriminator_block(hidden_dim * 2, hidden_dim),\n",
        "\n",
        "            nn.Linear(hidden_dim,1)\n",
        "        )\n",
        "\n",
        "    def forward(self, image):\n",
        "        '''\n",
        "        Function for completing a forward pass of the discriminator: Given an image tensor, \n",
        "        returns a 1-dimension tensor representing fake/real.\n",
        "        Parameters:\n",
        "            image: a flattened image tensor with dimension (im_dim)\n",
        "        '''\n",
        "        return self.disc(image)\n",
        "    \n",
        "    # Needed for testing\n",
        "    def get_disc(self):\n",
        "        '''\n",
        "        Returns:\n",
        "            the sequential model\n",
        "        '''\n",
        "        return self.disc"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRk_8azSq3tF"
      },
      "source": [
        "## Training\n",
        "First, you will set your parameters:\n",
        "  *   criterion: the loss function\n",
        "  *   n_epochs: the number of times you iterate through the entire dataset when training\n",
        "  *   z_dim: the dimension of the noise vector\n",
        "  *   display_step: how often to display/visualize the images\n",
        "  *   batch_size: the number of images per forward/backward pass\n",
        "  *   lr: the learning rate\n",
        "  *   device: the device type, here using a GPU (which runs CUDA), not CPU\n",
        "\n",
        "Next, you will load the MNIST dataset as tensors using a dataloader.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFLQ039u-qdu"
      },
      "source": [
        "%%capture \n",
        "# Set your parameters\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "n_epochs = 3 #200\n",
        "z_dim = 64\n",
        "display_step = 500\n",
        "batch_size = 128\n",
        "lr = 0.00001\n",
        "\n",
        "# Load MNIST dataset as tensors\n",
        "dataloader = DataLoader(\n",
        "    MNIST('.', download=True, transform=transforms.ToTensor()),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True)\n",
        "\n",
        "device = 'cpu' #'cuda'"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24Var22i_Ccs"
      },
      "source": [
        "Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDFRZ8tg_Y57"
      },
      "source": [
        "gen = Generator(z_dim).to(device)\n",
        "gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)\n",
        "disc = Discriminator().to(device) \n",
        "disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iCTg3w4_Zw6"
      },
      "source": [
        "Before you train your GAN, you will need to create functions to calculate the discriminator's loss and the generator's loss. This is how the discriminator and generator will know how they are doing and improve themselves. Since the generator is needed when calculating the discriminator's loss, the call .detach() on the generator result ensures that only the discriminator is updated!\n",
        "\n",
        "loss function (`criterion`) is defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYzBtiYyz8IJ"
      },
      "source": [
        "def get_disc_loss(gen, disc, criterion, real, num_images, z_dim, device):\n",
        "    '''\n",
        "    Return the loss of the discriminator given inputs.\n",
        "    Parameters:\n",
        "        gen: the generator model, which returns an image given z-dimensional noise\n",
        "        disc: the discriminator model, which returns a single-dimensional prediction of real/fake\n",
        "        criterion: the loss function, which should be used to compare \n",
        "               the discriminator's predictions to the ground truth reality of the images \n",
        "               (e.g. fake = 0, real = 1)\n",
        "        real: a batch of real images\n",
        "        num_images: the number of images the generator should produce, \n",
        "                which is also the length of the real images\n",
        "        z_dim: the dimension of the noise vector, a scalar\n",
        "        device: the device type\n",
        "    Returns:\n",
        "        disc_loss: a torch scalar loss value for the current batch\n",
        "    '''\n",
        "    noise_vectors = get_noise(num_images, z_dim, device)\n",
        "    gen_out = gen(noise_vectors)\n",
        "    disc_output_fake = disc(gen_out.detach())\n",
        "    loss_fake = criterion(disc_output_fake,torch.zeros_like(disc_output_fake))\n",
        "    disc_output_real = disc(real)\n",
        "    loss_real = criterion(disc_output_real,torch.ones_like(disc_output_real))\n",
        "    disc_loss = (loss_fake+loss_real)/2\n",
        "    return disc_loss"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zV_8i6y30nTE"
      },
      "source": [
        "def get_gen_loss(gen, disc, criterion, num_images, z_dim, device):\n",
        "    '''\n",
        "    Return the loss of the generator given inputs.\n",
        "    Parameters:\n",
        "        gen: the generator model, which returns an image given z-dimensional noise\n",
        "        disc: the discriminator model, which returns a single-dimensional prediction of real/fake\n",
        "        criterion: the loss function, which should be used to compare \n",
        "               the discriminator's predictions to the ground truth reality of the images \n",
        "               (e.g. fake = 0, real = 1)\n",
        "        num_images: the number of images the generator should produce, \n",
        "                which is also the length of the real images\n",
        "        z_dim: the dimension of the noise vector, a scalar\n",
        "        device: the device type\n",
        "    Returns:\n",
        "        gen_loss: a torch scalar loss value for the current batch\n",
        "    '''\n",
        "    noise_vectors = get_noise(num_images, z_dim, device)\n",
        "    gen_out = gen(noise_vectors)\n",
        "    disc_output_fake = disc(gen_out)\n",
        "    gen_loss = criterion(disc_output_fake,torch.ones_like(disc_output_fake))\n",
        "    return gen_loss"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3RVHTAvTlod"
      },
      "source": [
        "You should roughly expect to see this progression. On a GPU, this should take about 15 seconds per 500 steps, on average, while on CPU it will take roughly 1.5 minutes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXptQZcwrBrq"
      },
      "source": [
        "cur_step = 0\n",
        "mean_generator_loss = 0\n",
        "mean_discriminator_loss = 0\n",
        "test_generator = True # Whether the generator should be tested\n",
        "gen_loss = False\n",
        "error = False\n",
        "for epoch in range(n_epochs):\n",
        "  \n",
        "    # Dataloader returns the batches\n",
        "    for real, _ in tqdm(dataloader):\n",
        "        cur_batch_size = len(real)\n",
        "\n",
        "        # Flatten the batch of real images from the dataset\n",
        "        real = real.view(cur_batch_size, -1).to(device)\n",
        "\n",
        "        ### Update discriminator ###\n",
        "        # Zero out the gradients before backpropagation\n",
        "        disc_opt.zero_grad()\n",
        "\n",
        "        # Calculate discriminator loss\n",
        "        disc_loss = get_disc_loss(gen, disc, criterion, real, cur_batch_size, z_dim, device)\n",
        "\n",
        "        # Update gradients\n",
        "        disc_loss.backward(retain_graph=True)\n",
        "\n",
        "        # Update optimizer\n",
        "        disc_opt.step()\n",
        "\n",
        "        # For testing purposes, to keep track of the generator weights\n",
        "        if test_generator:\n",
        "            old_generator_weights = gen.gen[0][0].weight.detach().clone()\n",
        "\n",
        "        ### Update generator ###\n",
        "        #     Hint: This code will look a lot like the discriminator updates!\n",
        "        #     These are the steps you will need to complete:\n",
        "        #       1) Zero out the gradients.\n",
        "        #       2) Calculate the generator loss, assigning it to gen_loss.\n",
        "        #       3) Backprop through the generator: update the gradients and optimizer.\n",
        "\n",
        "        gen_opt.zero_grad()\n",
        "        gen_loss = get_gen_loss(gen, disc, criterion, cur_batch_size, z_dim, device)\n",
        "        gen_loss.backward(retain_graph=True)\n",
        "        gen_opt.step()\n",
        "\n",
        "        # For testing purposes, to check that your code changes the generator weights\n",
        "        if test_generator:\n",
        "            try:\n",
        "                assert lr > 0.0000002 or (gen.gen[0][0].weight.grad.abs().max() < 0.0005 and epoch == 0)\n",
        "                assert torch.any(gen.gen[0][0].weight.detach().clone() != old_generator_weights)\n",
        "            except:\n",
        "                error = True\n",
        "                print(\"Runtime tests have failed\")\n",
        "\n",
        "        # Keep track of the average discriminator loss\n",
        "        mean_discriminator_loss += disc_loss.item() / display_step\n",
        "\n",
        "        # Keep track of the average generator loss\n",
        "        mean_generator_loss += gen_loss.item() / display_step\n",
        "\n",
        "        ### Visualization code ###\n",
        "        if cur_step % display_step == 0 and cur_step > 0:\n",
        "            print(f\"Epoch {epoch}, step {cur_step}: Generator loss: {mean_generator_loss}, discriminator loss: {mean_discriminator_loss}\")\n",
        "            fake_noise = get_noise(cur_batch_size, z_dim, device=device)\n",
        "            fake = gen(fake_noise)\n",
        "            show_tensor_images(fake)\n",
        "            show_tensor_images(real)\n",
        "            mean_generator_loss = 0\n",
        "            mean_discriminator_loss = 0\n",
        "        cur_step += 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIQntRxTBXKf"
      },
      "source": [
        "# Sandbox"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrlFnZf6U15O"
      },
      "source": [
        "class GAN_MLP():\n",
        "  '''\n",
        "  GAN based on Sequential Network Architecture\n",
        "  Values:\n",
        "  '''\n",
        "  def __init__(self,z_dim = 64, device = 'cpu'):\n",
        "    self.z_dim = z_dim\n",
        "    self.device = device\n",
        "    self.gen = Generator(z_dim).to(device)\n",
        "    self.gen_opt = torch.optim.Adam(self.gen.parameters(), lr=lr)\n",
        "    self.disc = Discriminator().to(device) \n",
        "    self.disc_opt = torch.optim.Adam(self.disc.parameters(), lr=lr)\n",
        "    self.criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "  def show_tensor_images(self, image_tensor, num_images=25, size=(1, 28, 28)):\n",
        "    '''\n",
        "    Function for visualizing images: Given a tensor of images, number of images, and\n",
        "    size per image, plots and prints the images in a uniform grid.\n",
        "    '''\n",
        "    image_unflat = image_tensor.detach().cpu().view(-1, *size)\n",
        "    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n",
        "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
        "    plt.show()\n",
        "\n",
        "  def get_noise(self, n_samples):\n",
        "    '''\n",
        "    Function for creating noise vectors: Given the dimensions (n_samples, z_dim),\n",
        "    creates a tensor of that shape filled with random numbers from the normal distribution.\n",
        "    Parameters:\n",
        "        n_samples: the number of samples to generate, a scalar\n",
        "        z_dim: the dimension of the noise vector, a scalar\n",
        "        device: the device type\n",
        "    '''\n",
        "    # NOTE: To use this on GPU with device='cuda', make sure to pass the device \n",
        "    return torch.randn(n_samples, self.z_dim, device = self.device)\n",
        "  \n",
        "  def get_gen_loss(self, n_samples):\n",
        "    '''\n",
        "    Return the loss of the generator given inputs.\n",
        "    Parameters:\n",
        "        gen: the generator model, which returns an image given z-dimensional noise\n",
        "        disc: the discriminator model, which returns a single-dimensional prediction of real/fake\n",
        "        criterion: the loss function, which should be used to compare \n",
        "              the discriminator's predictions to the ground truth reality of the images \n",
        "              (e.g. fake = 0, real = 1)\n",
        "        num_images: the number of images the generator should produce, \n",
        "                which is also the length of the real images\n",
        "        z_dim: the dimension of the noise vector, a scalar\n",
        "        device: the device type\n",
        "    Returns:\n",
        "        gen_loss: a torch scalar loss value for the current batch\n",
        "    '''\n",
        "    #test fixture\n",
        "    if n_samples == 0:\n",
        "      print('testing')\n",
        "      return\n",
        "    noise_vectors = self.get_noise(n_samples)\n",
        "    gen_out = self.gen(noise_vectors)\n",
        "    disc_output_fake = self.disc(gen_out)\n",
        "    gen_loss = self.criterion(disc_output_fake,torch.ones_like(disc_output_fake))\n",
        "    return gen_loss\n",
        "\n",
        "  def get_disc_loss(self, n_samples):\n",
        "    '''\n",
        "    Return the loss of the discriminator given inputs.\n",
        "    Parameters:\n",
        "        gen: the generator model, which returns an image given z-dimensional noise\n",
        "        disc: the discriminator model, which returns a single-dimensional prediction of real/fake\n",
        "        criterion: the loss function, which should be used to compare \n",
        "              the discriminator's predictions to the ground truth reality of the images \n",
        "              (e.g. fake = 0, real = 1)\n",
        "        real: a batch of real images\n",
        "        num_images: the number of images the generator should produce, \n",
        "                which is also the length of the real images\n",
        "        z_dim: the dimension of the noise vector, a scalar\n",
        "        device: the device type\n",
        "    Returns:\n",
        "        disc_loss: a torch scalar loss value for the current batch\n",
        "    '''\n",
        "    noise_vectors = self.get_noise(n_samples)\n",
        "    gen_out = self.gen(noise_vectors)\n",
        "    disc_output_fake = self.disc(gen_out.detach())\n",
        "    loss_fake = self.criterion(disc_output_fake,torch.zeros_like(disc_output_fake))\n",
        "    disc_output_real = self.disc(real)\n",
        "    loss_real = self.criterion(disc_output_real,torch.ones_like(disc_output_real))\n",
        "    disc_loss = (loss_fake+loss_real)/2\n",
        "    return disc_loss"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkbuOMIrf3s9"
      },
      "source": [
        "# GAN network initialization\n",
        "my_gan = GAN_MLP()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1Nu58iJ1Oo4"
      },
      "source": [
        "my_gan.gen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iScp15vYffDW"
      },
      "source": [
        "n_epochs = 3 #200\n",
        "display_step = 500\n",
        "lr = 0.00001\n",
        "\n",
        "cur_step = 0\n",
        "mean_generator_loss = 0\n",
        "mean_discriminator_loss = 0\n",
        "test_generator = False # Whether the generator should be tested\n",
        "gen_loss = False\n",
        "error = False\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  \n",
        "    # Dataloader returns the batches\n",
        "    for real, _ in tqdm(dataloader):\n",
        "        cur_batch_size = len(real)\n",
        "\n",
        "        # Flatten the batch of real images from the dataset\n",
        "        real = real.view(cur_batch_size, -1).to(device)\n",
        "\n",
        "        ### Update discriminator ###\n",
        "        # Zero out the gradients before backpropagation\n",
        "        my_gan.disc_opt.zero_grad()\n",
        "\n",
        "        # Calculate discriminator loss\n",
        "        disc_loss = my_gan.get_disc_loss(cur_batch_size)\n",
        "\n",
        "        # Update gradients\n",
        "        disc_loss.backward(retain_graph=True)\n",
        "\n",
        "        # Update optimizer\n",
        "        my_gan.disc_opt.step()\n",
        "\n",
        "        # For testing purposes, to keep track of the generator weights\n",
        "        if test_generator:\n",
        "            old_generator_weights = gen.gen[0][0].weight.detach().clone()\n",
        "\n",
        "        ### Update generator ###\n",
        "\n",
        "        my_gan.gen_opt.zero_grad()\n",
        "        gen_loss = my_gan.get_gen_loss(cur_batch_size)\n",
        "        gen_loss.backward(retain_graph=True)\n",
        "        my_gan.gen_opt.step()\n",
        "\n",
        "        # For testing purposes, to check that your code changes the generator weights\n",
        "        if test_generator:\n",
        "            try:\n",
        "                assert lr > 0.0000002 or (my_gan.gen.gen[0][0].weight.grad.abs().max() < 0.0005 and epoch == 0)\n",
        "                assert torch.any(my_gan.gen.gen[0][0].weight.detach().clone() != old_generator_weights)\n",
        "            except:\n",
        "                error = True\n",
        "                print(\"Runtime tests have failed\")\n",
        "\n",
        "        # Keep track of the average discriminator loss\n",
        "        mean_discriminator_loss += disc_loss.item() / display_step\n",
        "\n",
        "        # Keep track of the average generator loss\n",
        "        mean_generator_loss += gen_loss.item() / display_step\n",
        "\n",
        "        ### Visualization code ###\n",
        "        if cur_step % display_step == 0 and cur_step > 0:\n",
        "            print(f\"Epoch {epoch}, step {cur_step}: Generator loss: {mean_generator_loss}, discriminator loss: {mean_discriminator_loss}\")\n",
        "            fake_noise = my_gan.get_noise(cur_batch_size)\n",
        "            fake = my_gan.gen(fake_noise)\n",
        "            my_gan.show_tensor_images(fake)\n",
        "            my_gan.show_tensor_images(real)\n",
        "            mean_generator_loss = 0\n",
        "            mean_discriminator_loss = 0\n",
        "        cur_step += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv4U5SSQ170h"
      },
      "source": [
        "# TESTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSggwl1J-XBt"
      },
      "source": [
        "#TEST BLOCK - 1\n",
        "# Verify the generator block function\n",
        "def test_gen_block(in_features, out_features, num_test=1000):\n",
        "    block = get_generator_block(in_features, out_features)\n",
        "\n",
        "    # Check the three parts\n",
        "    assert len(block) == 3\n",
        "    assert type(block[0]) == nn.Linear\n",
        "    assert type(block[1]) == nn.BatchNorm1d\n",
        "    assert type(block[2]) == nn.ReLU\n",
        "    \n",
        "    # Check the output shape\n",
        "    test_input = torch.randn(num_test, in_features)\n",
        "    test_output = block(test_input)\n",
        "    assert tuple(test_output.shape) == (num_test, out_features)\n",
        "    assert test_output.std() > 0.55\n",
        "    assert test_output.std() < 0.65\n",
        "\n",
        "test_gen_block(25, 12)\n",
        "test_gen_block(15, 28)\n",
        "print(\"Success!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0UW5DetBIY8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17a619f4-081a-4c1a-936c-6bc030be0815"
      },
      "source": [
        "#TEST BLOCK - 2\n",
        "# Verify the generator class\n",
        "def test_generator(z_dim, im_dim, hidden_dim, num_test=10000):\n",
        "    gen = Generator(z_dim, im_dim, hidden_dim).get_gen()\n",
        "    \n",
        "    # Check there are six modules in the sequential part\n",
        "    assert len(gen) == 6\n",
        "    test_input = torch.randn(num_test, z_dim)\n",
        "    test_output = gen(test_input)\n",
        "\n",
        "    # Check that the output shape is correct\n",
        "    assert tuple(test_output.shape) == (num_test, im_dim)\n",
        "    assert test_output.max() < 1, \"Make sure to use a sigmoid\"\n",
        "    assert test_output.min() > 0, \"Make sure to use a sigmoid\"\n",
        "    assert test_output.std() > 0.05, \"Don't use batchnorm here\"\n",
        "    assert test_output.std() < 0.15, \"Don't use batchnorm here\"\n",
        "\n",
        "test_generator(5, 10, 20)\n",
        "test_generator(20, 8, 24)\n",
        "print(\"Success!\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgWdQK-7P3qN",
        "outputId": "062d079e-ed61-48af-a7da-1bc7285541b5"
      },
      "source": [
        "#TEST BLOCK - 3\n",
        "# Verify the noise vector function\n",
        "def test_get_noise(n_samples, z_dim, device='cpu'):\n",
        "    noise = get_noise(n_samples, z_dim, device)\n",
        "    \n",
        "    # Make sure a normal distribution was used\n",
        "    assert tuple(noise.shape) == (n_samples, z_dim)\n",
        "    assert torch.abs(noise.std() - torch.tensor(1.0)) < 0.01\n",
        "    assert str(noise.device).startswith(device)\n",
        "\n",
        "test_get_noise(1000, 100, 'cpu')\n",
        "if torch.cuda.is_available():\n",
        "    test_get_noise(1000, 32, 'cuda')\n",
        "print(\"Success!\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2r7Uh_GnCnAb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85f2f965-afd2-41bf-9dec-e4d15cd02b45"
      },
      "source": [
        "#TEST BLOCK - 4\n",
        "# Verify the discriminator block function\n",
        "def test_disc_block(in_features, out_features, num_test=10000):\n",
        "    block = get_discriminator_block(in_features, out_features)\n",
        "\n",
        "    # Check there are two parts\n",
        "    assert len(block) == 2\n",
        "    test_input = torch.randn(num_test, in_features)\n",
        "    test_output = block(test_input)\n",
        "\n",
        "    # Check that the shape is right\n",
        "    assert tuple(test_output.shape) == (num_test, out_features)\n",
        "    \n",
        "    # Check that the LeakyReLU slope is about 0.2\n",
        "    assert -test_output.min() / test_output.max() > 0.1\n",
        "    assert -test_output.min() / test_output.max() < 0.3\n",
        "    assert test_output.std() > 0.3\n",
        "    assert test_output.std() < 0.5\n",
        "\n",
        "test_disc_block(25, 12)\n",
        "test_disc_block(15, 28)\n",
        "print(\"Success!\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qncmSuZmMXBx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "909c3ac6-5824-4773-b807-95291745f982"
      },
      "source": [
        "#TEST BLOCK - 5\n",
        "# Verify the discriminator class\n",
        "def test_discriminator(z_dim, hidden_dim, num_test=100):\n",
        "    \n",
        "    disc = Discriminator(z_dim, hidden_dim).get_disc()\n",
        "\n",
        "    # Check there are three parts\n",
        "    assert len(disc) == 4\n",
        "\n",
        "    # Check the linear layer is correct\n",
        "    test_input = torch.randn(num_test, z_dim)\n",
        "    test_output = disc(test_input)\n",
        "    assert tuple(test_output.shape) == (num_test, 1)\n",
        "    \n",
        "    # Make sure there's no sigmoid\n",
        "    assert test_input.max() > 1\n",
        "    assert test_input.min() < -1\n",
        "\n",
        "test_discriminator(5, 10)\n",
        "test_discriminator(20, 8)\n",
        "print(\"Success!\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjyIY7XUEpbu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "100a1d75-2ad9-4df4-8bfc-d8f3337fa9f0"
      },
      "source": [
        "#TEST BLOCK - 6\n",
        "def test_disc_reasonable(num_images=10):\n",
        "    # Don't use explicit casts to cuda - use the device argument\n",
        "    import inspect, re\n",
        "    lines = inspect.getsource(get_disc_loss)\n",
        "    assert (re.search(r\"to\\(.cuda.\\)\", lines)) is None\n",
        "    assert (re.search(r\"\\.cuda\\(\\)\", lines)) is None\n",
        "    \n",
        "    z_dim = 64\n",
        "    gen = torch.zeros_like\n",
        "    disc = lambda x: x.mean(1)[:, None]\n",
        "    criterion = torch.mul # Multiply\n",
        "    real = torch.ones(num_images, z_dim)\n",
        "    disc_loss = get_disc_loss(gen, disc, criterion, real, num_images, z_dim, 'cpu')\n",
        "    assert torch.all(torch.abs(disc_loss.mean() - 0.5) < 1e-5)\n",
        "    \n",
        "    gen = torch.ones_like\n",
        "    criterion = torch.mul # Multiply\n",
        "    real = torch.zeros(num_images, z_dim)\n",
        "    assert torch.all(torch.abs(get_disc_loss(gen, disc, criterion, real, num_images, z_dim, 'cpu')) < 1e-5)\n",
        "    \n",
        "    gen = lambda x: torch.ones(num_images, 10)\n",
        "    disc = lambda x: x.mean(1)[:, None] + 10\n",
        "    criterion = torch.mul # Multiply\n",
        "    real = torch.zeros(num_images, 10)\n",
        "    assert torch.all(torch.abs(get_disc_loss(gen, disc, criterion, real, num_images, z_dim, 'cpu').mean() - 5) < 1e-5)\n",
        "\n",
        "    gen = torch.ones_like\n",
        "    disc = nn.Linear(64, 1, bias=False)\n",
        "    real = torch.ones(num_images, 64) * 0.5\n",
        "    disc.weight.data = torch.ones_like(disc.weight.data) * 0.5\n",
        "    disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)\n",
        "    criterion = lambda x, y: torch.sum(x) + torch.sum(y)\n",
        "    disc_loss = get_disc_loss(gen, disc, criterion, real, num_images, z_dim, 'cpu').mean()\n",
        "    disc_loss.backward()\n",
        "    assert torch.isclose(torch.abs(disc.weight.grad.mean() - 11.25), torch.tensor(3.75))\n",
        "    \n",
        "def test_disc_loss(max_tests = 10):\n",
        "    z_dim = 64\n",
        "    gen = Generator(z_dim).to(device)\n",
        "    gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)\n",
        "    disc = Discriminator().to(device) \n",
        "    disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)\n",
        "    num_steps = 0\n",
        "    for real, _ in dataloader:\n",
        "        cur_batch_size = len(real)\n",
        "        real = real.view(cur_batch_size, -1).to(device)\n",
        "\n",
        "        ### Update discriminator ###\n",
        "        # Zero out the gradient before backpropagation\n",
        "        disc_opt.zero_grad()\n",
        "\n",
        "        # Calculate discriminator loss\n",
        "        disc_loss = get_disc_loss(gen, disc, criterion, real, cur_batch_size, z_dim, device)\n",
        "        assert (disc_loss - 0.68).abs() < 0.05\n",
        "\n",
        "        # Update gradients\n",
        "        disc_loss.backward(retain_graph=True)\n",
        "\n",
        "        # Check that they detached correctly\n",
        "        assert gen.gen[0][0].weight.grad is None\n",
        "\n",
        "        # Update optimizer\n",
        "        old_weight = disc.disc[0][0].weight.data.clone()\n",
        "        disc_opt.step()\n",
        "        new_weight = disc.disc[0][0].weight.data\n",
        "        \n",
        "        # Check that some discriminator weights changed\n",
        "        assert not torch.all(torch.eq(old_weight, new_weight))\n",
        "        num_steps += 1\n",
        "        if num_steps >= max_tests:\n",
        "            break\n",
        "\n",
        "test_disc_reasonable()\n",
        "test_disc_loss()\n",
        "print(\"Success!\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFiwoJu-G0l9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fe90565-134a-46ec-a38c-1ff07fe374f9"
      },
      "source": [
        "#TEST BLOCK - 7\n",
        "def test_gen_reasonable(num_images=10):\n",
        "    # Don't use explicit casts to cuda - use the device argument\n",
        "    import inspect, re\n",
        "    lines = inspect.getsource(get_gen_loss)\n",
        "    assert (re.search(r\"to\\(.cuda.\\)\", lines)) is None\n",
        "    assert (re.search(r\"\\.cuda\\(\\)\", lines)) is None\n",
        "    \n",
        "    z_dim = 64\n",
        "    gen = torch.zeros_like\n",
        "    disc = nn.Identity()\n",
        "    criterion = torch.mul # Multiply\n",
        "    gen_loss_tensor = get_gen_loss(gen, disc, criterion, num_images, z_dim, 'cpu')\n",
        "    assert torch.all(torch.abs(gen_loss_tensor) < 1e-5)\n",
        "    #Verify shape. Related to gen_noise parametrization\n",
        "    assert tuple(gen_loss_tensor.shape) == (num_images, z_dim)\n",
        "\n",
        "    gen = torch.ones_like\n",
        "    disc = nn.Identity()\n",
        "    criterion = torch.mul # Multiply\n",
        "    real = torch.zeros(num_images, 1)\n",
        "    gen_loss_tensor = get_gen_loss(gen, disc, criterion, num_images, z_dim, 'cpu')\n",
        "    assert torch.all(torch.abs(gen_loss_tensor - 1) < 1e-5)\n",
        "    #Verify shape. Related to gen_noise parametrization\n",
        "    assert tuple(gen_loss_tensor.shape) == (num_images, z_dim)\n",
        "    \n",
        "\n",
        "def test_gen_loss(num_images):\n",
        "    z_dim = 64\n",
        "    gen = Generator(z_dim).to(device)\n",
        "    gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)\n",
        "    disc = Discriminator().to(device) \n",
        "    disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)\n",
        "    \n",
        "    gen_loss = get_gen_loss(gen, disc, criterion, num_images, z_dim, device)\n",
        "    \n",
        "    # Check that the loss is reasonable\n",
        "    assert (gen_loss - 0.7).abs() < 0.1\n",
        "    gen_loss.backward()\n",
        "    old_weight = gen.gen[0][0].weight.clone()\n",
        "    gen_opt.step()\n",
        "    new_weight = gen.gen[0][0].weight\n",
        "    assert not torch.all(torch.eq(old_weight, new_weight))\n",
        "\n",
        "\n",
        "test_gen_reasonable(10)\n",
        "test_gen_loss(18)\n",
        "print(\"Success!\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}